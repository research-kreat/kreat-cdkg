patent_id,summary_text,patent_type,patent_date,patent_title,wipo_kind,num_claims,assignee_names,disambig_assignee_organization,ipc,cpc_group,cpc_type,cpc_group_title,cpc_class_title,cpc_subclass_title,foreign_citation_count,local_citation_count,description_text,inventor_names,domains,sub_domains,abstract
12020719,"The present invention relates to the field of playing back audio/video signals.

BACKGROUND OF THE INVENTION

Audio/video decoders are known that comprise a demultiplexer having an input connected to a source of audio/video digital data, and two outputs connected respectively to an audio processor circuit and to a video processor circuit. The audio processor circuit is arranged to process the audio data and to transmit it in the form of audio signals to a sound playback member such as one or more loudspeakers connected to the decoder, and the video processor circuit is arranged to process the video data and to transmit it in the form of video signals to a display screen such as a TV set connected to the decoder.

It is essential for the transmission of the audio data to the sound playback member to be synchronized with the transmission of the video data to the TV set so that the sounds being played back correspond to the images being displayed.

For this purpose, and since the time required for the audio processing is not necessarily the same as the time required for the video processing, the audio and video processor circuits each include a respective buffer memory (commonly shortened to “buffer”) in which the audio and video data is stored prior to being transmitted. The stored data is associated with presentation timestamps (PTS), and the decoder includes a reference clock used for determining the instant at which the audio data and the video data having the same presentation timestamp is to be transmitted simultaneously respectively to the sound playback member and to the TV set.

So long as the decoder is connected to the sound playback member by a wired connection that is unlikely to suffer external disturbances, this generally presents no problem.

Nevertheless, it is increasingly common for the decoder to be connected to a wireless speaker via a radio link of Wi-Fi type. The audio processor circuit of the decoder is then arranged to supply audio data to a radio transmitter, and the speaker includes a radio receiver connected to a decoder circuit arranged to supply the sound playback member with the audio data that results from decoding the audio data transmitted by the audio processor circuit. However, it commonly happens that the radio connection is disturbed, e.g. by the structure of the building or by interfering electromagnetic radiation. Such disturbances give rise to delays in receiving, decoding, and supplying the audio data to the sound playback member. Since these disturbances and the delays to which they give rise vary depending on location and also over time, manufacturers provide a buffer of predetermined size in the speaker in order to accommodate latency in the playback of audio data, which latency is capable of compensating some predetermined maximum delay (where the size of the buffer is directly proportional to the duration of that delay) so as to conserve synchronization with the video data even in the event of disturbance. As a result, in certain locations and/or at a given instant in a particular location, the buffer will be too large, while in other locations and/or at some other given instant in the same location, the buffer will be too small and will not enable perfect synchronization to be maintained between the image and the sound. Also, since the latency in sound playback also requires the video data to be delayed in order to conserve the synchronization between the image and sound, it is not desirable to provide a buffer that is so large that it introduces a delay that is perceptible by the user.

OBJECT OF THE INVENTION

A particular object of the invention is to remedy these drawbacks, at least in part.

SUMMARY OF THE INVENTION

To this end, the invention provides a method of playing back audio/video signals by means of an audio/video system comprising a decoder appliance connected to a display screen and to at least one wireless speaker, the decoder appliance comprising a demultiplexer having an input connected to a source of audio/video data, a first output connected to a video processor circuit arranged to supply video data to the display screen, and a second output connected to an audio processor circuit arranged to supply encoded audio frames to a radio transmitter, the speaker including a radio receiver connected to a decoder circuit arranged to supply a sound playback member with decoded audio frames resulting from decoding the encoded audio frames, the speaker including a clock that is synchronized with a clock of the decoder and a buffer for storing audio frames so as to allow latency between receiving encoded audio frames and supplying decoded audio frames, the method comprising the steps of:determining the filling level of the buffer;comparing the filling level of the buffer with a target filling level for the buffer and deducing a filling difference therefrom; andincreasing or decreasing the number of audio frames that are stored as a function of the filling difference so as to adjust the latency.

Thus, it is possible to adjust latency depending on requirements, and more particularly depending on the disturbances that have been encountered in the past, so as to conserve synchronization between the images and the soundtrack.

Advantageously, the number of stored audio frames is decreased if the filling difference is not less than a predetermined percentage about the filling target, and preferably the predetermined percentage is equal to about 25%, for example.

In a particular implementation, the filling target is equal to the sum of a safety value plus a margin.

Under such circumstances, alternatively:the margin is equal to the minimum filling level over a predetermined past duration, e.g. of about 2 minutes;the margin is equal to one-tenth of the sum of the preceding margin plus nine times the present filling level.

Preferably, separately or in combination:the safety value corresponds to a filling level allowing latency in the range 70 milliseconds (ms) to 200 ms;the safety level is increased if the margin has been negative during a predetermined time lapse for a number of times that is greater than a threshold;the method comprises a first stage of adjusting filling strongly when the filling level is less than a first threshold value corresponding to a first percentage of the safety value, the first threshold value being equal, for example, to about 80% of the safety value and the latency being modified by a value less than or equal to 20 ms;the method includes a second stage of adjusting the filling strongly when the filling level is less than a second threshold value less than the first threshold value, the second threshold value being, for example, equal to about 60% of the safety value and the latency being modified by a value less than or equal to 40 ms.

According to a particular characteristic, the filling level of the buffer is determined each time an audio frame is extracted from the buffer or at regular intervals.

In a first particular mode of adjustment, the decoder and the speaker each include a respective clock with the clocks being synchronized, with the number of audio frames that are stored being increased by retarding the clocks by a predetermined duration and with the number of audio frames that are stored being decreased by advancing the clocks by a predetermined duration. This predetermined duration is preferably equal to a small value, e.g. less than or equal to 150 microseconds (μs), such as a value of 50 μs.

This duration may be predetermined in constant manner (the value of the adjustment remains constant during the utilization session) or in dynamic manner (the value of the adjustment varies as a function of the total amount of adjustment that is to be made).

In a second particular mode of adjustment, a presentation timestamp is associated with each image of video data and with each corresponding audio frame, with the number of stored audio frames being increased by adding a predetermined duration to the presentation timestamps, and with the number of stored audio frames being decreased by subtracting a predetermined duration from the presentation timestamps. This predetermined duration is preferably equal to a small value, e.g. less than or equal to 150 μs, such as by a value of 50 μs.

Advantageously, after decreasing latency, the method comprises a step of:eliminating samples from an audio frame before transmission to the sound playback member or re-sampling an audio frame;optionally, eliminating an image from the video data.

Advantageously, after increasing latency, the method includes a step of:adding samples to an audio frame before transmission to the sound playback member or re-sampling an audio frame or interpolating samples in an audio frame;optionally, inserting an image into the video data, e.g. by duplication or by interpolation.

The invention also provides:a system for performing the method;a program including instructions for performing the method;a data medium containing such a program.

Other characteristics and advantages of the invention appear on reading the following description of a particular and nonlimiting embodiment of the invention.

",utility,2024-06-25,"Method, a system, and a program for playing back audio/video signals with automatic adjustment of latency",B2,24.0,,SAGEMCOM BROADBAND SAS,"G10L 19/16, G10L 19/5, H04N 21/43, H04N 21/845, H04R 5/4","G10L19/167,H04N21/44004,G10L19/005,H04N21/4302,H04N21/4305,H04N21/4307,H04N21/4341,H04N21/43615,H04N21/4392,H04N21/8456,H04N21/8547,H04R5/04","inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional","Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis -Correction of errors induced by the transmission channel, if related to the coding algorithm, Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis -using predictive techniques-Vocoder architecture-Audio streaming, i.e. formatting and decoding of an encoded audio signal representation into a data stream for transmission or storage purposes, Selective content distribution, e.g. interactive television or video on demand [VOD] -Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof-Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware -Content synchronisation processes, e.g. decoder synchronisation, Selective content distribution, e.g. interactive television or video on demand [VOD] -Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof-Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware -Content synchronisation processes, e.g. decoder synchronisation-Synchronising client clock from received content stream, e.g. locking decoder clock with encoder clock, extraction of the PCR packets , Selective content distribution, e.g. interactive television or video on demand [VOD] -Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof-Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware -Content synchronisation processes, e.g. decoder synchronisation-Synchronising the rendering of multiple content streams or additional data on devices, e.g. synchronisation of audio on a mobile phone with the video output on the TV screen, Selective content distribution, e.g. interactive television or video on demand [VOD] -Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof-Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware -Disassembling of a multiplex stream, e.g. demultiplexing audio and video streams, extraction of additional data from a video stream; Remultiplexing of multiplex streams; Extraction or processing of SI; Disassembling of packetised elementary stream -Demultiplexing of audio and video streams, Selective content distribution, e.g. interactive television or video on demand [VOD] -Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof-Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware -Interfacing a local distribution network, e.g. communicating with another STB ; or one or more peripheral devices;  inside the home-Interfacing a Home Network, e.g. for connecting the client to a plurality of peripherals , Selective content distribution, e.g. interactive television or video on demand [VOD] -Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof-Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware -Processing of audio elementary streams-involving audio buffer management, Selective content distribution, e.g. interactive television or video on demand [VOD] -Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof-Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware -Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream or rendering scenes according to encoded video stream scene graphs-involving video buffer management, e.g. video decoder buffer or video display buffer, Selective content distribution, e.g. interactive television or video on demand [VOD] -Generation or processing of content or additional data by content creator independently of the distribution process; Content per se -Generation or processing of protective or descriptive data associated with content; Content structuring-Structuring of content, e.g. decomposing content into time segments-by decomposing the content in the time domain, e.g. in time segments, Selective content distribution, e.g. interactive television or video on demand [VOD] -Generation or processing of content or additional data by content creator independently of the distribution process; Content per se -Assembly of content; Generation of multimedia applications-Content authoring-involving timestamps for synchronizing content, Stereophonic arrangements -Circuit arrangements, ; e.g. for selective connection of amplifier inputs/outputs to loudspeakers, for loudspeaker detection, or for adaptation of settings to personal preferences or hearing impairments ","MUSICAL INSTRUMENTS; ACOUSTICS, ELECTRIC COMMUNICATION TECHNIQUE","SPEECH ANALYSIS TECHNIQUES OR SPEECH SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING TECHNIQUES; SPEECH OR AUDIO CODING OR DECODING, PICTORIAL COMMUNICATION, e.g. TELEVISION, LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS ",0.0,4.0,"DETAILED DESCRIPTION OF THE INVENTION

With reference toFIG.1, the audio/video system of the invention comprises a decoder appliance220connected to a display screen300and to a wireless speaker250.

The decoder appliance220possesses an electronic control unit that comprises a demultiplexer222having an input connected to a source200of audio/video digital data via a receive buffer221, a first output connected to a video processor circuit arranged to supply the display screen300with video data representative of images, and a second output connected to an audio processor circuit arranged to supply a radio transmitter with encoded audio frames.

The source200may be a digital data network (e.g. such as a satellite network, a cable TV network, an xDSL connection, the Internet, or a local computer network), or an internal source such as a hard disk or a removable memory medium such as a memory card.

The demultiplexer222is known per se and it is arranged to extract from the audio/video digital data both the audio data230and also the video data240for supplying respectively to the audio processor circuit and to the video processor circuit.

The video processor circuit comprises in succession a decoder241and a buffer242associated to the decoder241for storing the decoded video data. The video data are each representative of an image with which is associated a presentation time stamp (PTS). The buffer242is connected to the display screen300, specifically a TV set.

The audio processor circuit comprises in succession a decoder231, an encoder232, and a buffer233for storing re-encoded audio data. Each piece of re-encoded audio data is an audio trame associated with a presentation time stamp (PTS) corresponding to the PTS of the corresponding image with which the audio frame is to be presented simultaneously. The encoder232is connected to the radio transmitter234via the buffer233.

The speaker250comprises a radio receiver255connected to an electronic control unit that comprises a decoder circuit arranged to supply a sound playback member256, specifically a loudspeaker, with decoded audio frames obtained by decoding the re-encoded audio data as received by the radio receiver255.

The decoding circuit of the speaker250comprises both a decoder253having an input connected to the radio receiver255via a receive buffer252and an output connected to the audio playback member256via a further254for storing audio frames, and also an audio circuit (not shown) comprising in particular the digital to analog converter (DAC) and an amplifier. It can be understood that both the buffer252and the buffer254are capable of accommodating latency between receiving the re-encoded audio data and supplying the decoded audio data to the sound playback member256.

In practice, the electronic control unit of the decoder appliance220and the electronic control unit of the speaker250comprise respective processors and memories enabling them to execute computer programs having instructions arranged to perform the program of the invention. Thus, the demultiplexers, the decoders, and the encoders in this example are portions of computer programs. In variants, dedicated electronic circuits could be used for performing these functions.

In operation, the electronic control unit of the decoder appliance220receives audio/video data from the source200and places it in its receive buffer221. The audio data230is separated from the video data240by means of the demultiplexer222.

The video data is decoded by the decoder241and is placed in the buffer242.

The audio data is decoded by the decoder231, re-encoded by the encoder232into a format appropriate for transmission to the speaker250, and placed in the buffer233, for sending subsequently to the speaker250by the radio transmitter234.

The electronic control unit of the speaker250receives the re-encoded audio data via the radio receiver255and places it in its own receive buffer252. This audio data is then decoded by the decoder253and placed in the buffer254.

As mentioned above, each image of the video data is associated with a respective presentation time stamp (PTS). The audio data is processed in frames (each representing a succession of samples), and each audio frame is likewise associated with a PTS for enabling the audio frame to be made to correspond with the image with which it is to be played back simultaneously (by matching their PTSes).

It should be observed that the PTSes used in this example are not the PTSes coming from the incoming stream from the source200, which might or might not contain PTSes (referred to as “primary” PTSes), but rather PTSes that are generated by the electronic control unit of the decoder appliance220(referred to as “secondary” PTSes). In order to be able to use the PTSes, the electronic control unit of the decoder appliance220and the electronic control unit of the speaker250include respective clocks249and251. The clocks249and251are synchronized with each other, by means of network time protocol (NTP), precision time protocol (PTP), or other protocols. The clock249becomes the reference clock for playback, and thus acts as the program clock reference (PCR).

The secondary PTSes are calculated by the electronic control unit of the decoder appliance220by taking account of:the presentation order and duration for of each image and audio frame of the input stream (which stream might potentially itself include primary PTSes expressed on the basis of an external PCR transmitted in the stream200);the presentation offset associated with the buffers used, firstly between the decoder appliance220and the speaker250, and secondly within the speaker250.

The PTSes are expressed as a number of “ticks” of the PCR reference clock, usually with a resolution that is smaller (in general the PCR is a 27 megahertz (MHz) clock, while the resolution of the PTSes is 90 kilohertz (kHz)).

When the time comes to present the video data (i.e. when the presentation timestamp is reached), the video frames are sent to the TV set300via a video link.

When the time comes to present the audio data (i.e. when the presentation timestamp is reached), the audio samples are sent to the DAC of the speaker250for subsequent amplification and playback by the loudspeaker of the sound playback member256.

The buffers present in the decoder appliance220are managed in conventional manner.

At the speaker end, there are two possible strategies for insertion into the buffer memory:decoding the audio frames as quickly as possible by means of the decoder253, and placing each decoded frame in the buffer254prior to reaching the corresponding PTS; orkeeping the encoded frames in the buffer252, and causing the frames to be decoded by the decoder253immediately prior to the PTS, and then proceeding with sound playback.

These two possibilities do not have any consequence for the present invention, which may be implemented with either of them.

FIG.2shows the audio data stored in one of the buffers of the speaker250(which may be the buffer252or the buffer254), and assuming that each frame lasts for 20 ms:the frame N, of PTS that corresponds to the current PTS, is removed from the buffer in order to be played back by the playback member256;the frames N+1 to N+6 are in the buffer, waiting to be read (which will take place when their PTSes become equal to the current PTS);the frame N+7, which is being received by the speaker250, is about to enter the buffer.

At the moment corresponding toFIG.2, there are 120 ms of samples present in the buffer. This corresponds to the present margin of the buffer. So long as disturbances in transmission last for less than 120 ms, there is no audible repercussion.

The buffer thus allows for latency of 120 ms between a frame being received and the frame being played back.

When the buffer is completely empty without any new frames being available, the “margin” is negative, with its value then corresponding to the time during which it has not been possible to play back any audio data.

The method of the invention seeks to enable the latency to be adjusted, and it comprises the steps of:determining the filling level of the buffer;comparing the filling level of the buffer with a target filling level for the buffer and deducing a filling difference therefrom;increasing or decreasing the number of audio frames that are stored as a function of the filling difference so as to adjust the latency.

This method is described below in detail.

The target filling level, or “buffer target”, is equal to the sum of a safety value plus a margin.

The “margin” of the buffer corresponds to the estimated level of robustness. The instantaneous margin corresponds to the value of the buffer at a given instant. Nevertheless, in order to minimize any risk of interruption, it is advantageous to calculate the margin as a moving value (and not an instantaneous value), which margin value is a function of present and past instantaneous values.

There are numerous methods for estimating such a moving value. For example:the margin=the buffer minimum over the last two minutes;the margin is estimated regularly (e.g. once every 20 ms): the margin=(prev_margin+present_buffer*9)/10, where prev_margin is the previously estimated margin and present_buffer is the present filling level of the buffer. This method amounts to implementing a lowpass filter serving to characterize the “forgetting” of past data/estimates.

Naturally, it is possible to use other methods, or to combine a plurality of methods with one another, e.g. such as combining the two methods given above by way of example.

Prior to adjusting latency, it is appropriate to determine whether it is desirable to increase or to decrease the current latency.

To do this, the first step in this example is to determine a safety threshold below which it is desired not to go. By way of example, a value lying in the range of 70 ms to 200 ms is appropriate in many circumstances. This “safety” value is for compensating unexpected disturbances.

In an environment that has previously encountered numerous disturbances (in which negative margin values have often occurred), it may be decided to increase this safety value.

The buffer filling target is determined as follows:
buffer_target=safety+desired_margin.

With reference toFIG.3, the present filling level of the buffer (buffer_value) is calculated regularly, e.g. when removing the audio data of a frame for playback by the loudspeaker (401), or at regular intervals (e.g. once every 20 ms, which corresponds to the duration of one frame) if the buffer is empty (402).

The present filling level of the buffer is then compared (403) with the filling target of the buffer, using the following criteria:if buffer_value<buffer_target, then it is necessary to increase the quantity of frames in the buffer (410);if buffer_value>=(buffer_target+25%), then it is possible to decrease the quantity of frames in the buffer (412); orotherwise, no adjustment is made (411).

The percentage of 25% represents a damping value that can be adjusted as a function of the strategies used so as to avoid too many adjustments of latency. It is preferable to conserve a minimum value (not to reduce latency to 0%) in order to avoid unnecessary adjustments of the system.

It can be understood that increasing the quantity of frames in the buffer amounts to increasing latency, and that decreasing the quantity of frames in the buffer amounts to reducing latency.

In order to adjust latency, two approaches are possible:adjusting the clocks; oradjusting the PTSes.

There follows an explanation of adjusting the clocks.

In order to adjust latency so that it increases, the clocks249and251are retarded a little (e.g. by a duration less than or equal to 150 μs, such as 50 μs). Both clocks must be adjusted in identical manner in order to preserve audio/video synchronization.

In order to adjust latency so that it decreases, the clocks249and251are advanced.

In order to ensure both clocks are adjusted simultaneously, several approaches are possible:the electronic control unit of the speaker250adjusts its own playback clock (251) and sends an adjustment message to the electronic control unit of the decoder appliance220, which adjusts its clock249;the electronic control unit of the speaker250transmits adjustment information to the electronic control unit of the decoder appliance220(or else the appliance itself calculates the necessary adjustments, if it has access to the buffer values of the speaker), and the electronic control unit of the decoder appliance220adjusts its clock. Thereafter, the electronic control unit of the decoder appliance220sends a clock adjustment message to the electronic control unit of the speaker250, or else both clocks become synchronized without requiring an adjustment message if a protocol such as NTP or PTP is used. It should be observed that it is preferable for it to be the electronic control unit of the decoder appliance220that performs clock adjustment, since that makes it possible to manage situations in which a plurality of speakers250are connected to (and are thus servocontrolled on) the electronic control unit of the decoder appliance220.

The adjustment of the PTSes is explained below with reference toFIG.4. The idea is no longer to adjust the clock, but rather to adjust the PTSes of future audio and video data.

For example, instead of retarding the clock by 100 μs, it is possible to add 100 μs to the future PTSes of the audio and video data (450), which has the effect of increasing latency, and thus of increasing the filling level of the buffer. In contrast, reducing the PTSes (452) has the consequence of reducing latency, and thus of reducing the filling level of the buffer.

The PTSes can be adjusted either simultaneously by the electronic unit of the speaker250and by the electronic unit of the decoder appliance220, or else totally by the electronic unit of the decoder appliance of220. This second approach is simpler, since all of the adjustment is performed by a single piece of equipment. To do this, the electronic unit of the speaker250transmits information about the filling level of its buffer regularly to the electronic unit of the decoder appliance220, thus enabling the electronic unit of the decoder appliance of220to have all of the information necessary for enabling it to evaluate the adjustments that need to be made.

Regardless of whether it is the clocks that are adjusted or the PTSes, the effects on playback of the audio and video data are similar.

In the speaker250, adjusting the playback clock gives rise to adjustments in the audio data to be played back, with various technical possibilities, e.g. such as:in order to decrease latency:removing PCM samples; andre-sampling a frame;in order to increase latency:adding PCM samples (copies of preceding samples);interpolating PCM samples;re-sampling a frame.

In the decoder appliance of220, adjusting the clock or the PTSes has an impact on the moments at which images are presented, and in extreme cases (adjustment by more than the duration of an image) an image might be duplicated or eliminated.

It is preferable to make the adjustments progressively (e.g. by a unit duration that is less than or equal to 150 μs, such as 50 μs each time), so that the audio/video adjustments (which are likewise progressive) are perceptible to the user as little as possible.

For example, with 20 ms audio frames, there are 50 audio frames per second. With a maximum adjustment of 50 μs for each audio frame, the system of the invention can adjust latency by a maximum duration of 50*50 μs, i.e. 2.5 ms per second. This is not perceptible by the user.

The above-defined progressive adjustment enables the system to be adjusted smoothly, which is particularly effective when disturbances are more or less regular.

Nevertheless, for a situation in which disturbances occur massively or suddenly, it can happen that the above-described normal mode of adjustment is insufficient. Under such circumstances, the method of the invention advantageously provides an exceptional mode of adjustment, referred to as “panic” mode, as shown inFIG.5.

In order to implement this exceptional mode of adjustment, a “strong” adjustment threshold is determined beforehand for when the present filling level of the buffer (buffer_value) is much less than the safety value. For example, a threshold of 80% of the safety value may be selected, such that the “panic” mode is triggered when:
buffer_value<80%*safety.

In “panic” mode, latency adjustment is allowed to increase more strongly, e.g. by 10 ms or even 20 ms.

In a variant, it is also possible to adjust latency more strongly as the present filling level of the buffer drops further below the safety value. For example, a plurality of thresholds may be defined, namely a first threshold at 80% of the safety value, a second threshold at 60% of the safety value, and a third threshold at 40% of the safety value. Each threshold is associated with a respective adjustment level, such that:if (60%*safety)<buffer_value≤(80%*safety), then adjust latency by 10 ms;if (40%*safety)<buffer_value≤(60%*safety), then adjust latency by 20 ms;if buffer_value≤(40%*safety), adjust latency by 40 ms.

The strongest adjustments are more likely to be perceived by the user, but that is a lesser evil compared with a break in audio playback.

The latency adjustment values given above are given solely by way of example. Thus, the specified values of 10 ms, 20 ms, and 40 ms could be replaced by other values, e.g. such as 20 ms, 40 ms, and 100 ms, the essential point being that the first value is less than or equal to the second, which is itself less than or equal to the third.

On the same principle, the safety values (80%, 60%, and 40%) could be replaced by other values.

If the buffer becomes completely empty during reading after the preceding audio frame has been played back in full, then there will be a break in the sound. Under such circumstances, instead of adjusting progressively, it is advisable to accept that there has been a break, and to fill the buffer up to the target value before starting to read again. Specifically, if this is the situation, that means that progressive adjustments have not been sufficient for keeping audio data in the buffer, and it is probable that any attempt at adjusting progressively will end up with the buffer becoming completely empty repeatedly, giving rise to multiple breaks in playback. Under such circumstances, it can be preferable to have a single break of greater length prior to starting to read again with the buffer at an optimum filling level, rather than having a system oscillating between breaks and playback.

Naturally, other variants of this “panic” mode are possible. For example, in a simplified mode, once buffer_value<safety, the filling level of the buffer could be adjusted directly up to the target value buffer_target before starting to read again. “Panic” mode is considered as having priority over “normal” dynamic adjustment of latency:the present filling level of the buffer (buffer_value) is calculated regularly, e.g. when removing the audio data of a frame for playback on the loudspeaker (401), or at regular intervals (e.g. every 20 ms, which corresponds to the duration of one frame) if the buffer is empty (402);the filling level of the buffer is initially evaluated relative to the safety value (420);as a function of this evaluation, adjustment is triggered either in “panic” mode (425), or else in “normal” mode (430).

When starting playback (reading a recording on a hard disk or a video on demand, channel hopping, . . . ), it is appropriate to determine a desired initial latency, since, on starting reading, the system does not have an estimate of the present filling level (buffer_value).

Several approaches are possible, such as for example:using a starting latency that is determined as a function of the latencies encountered during earlier playback;using a large starting latency, and allowing the system to optimize on its own;using a small latency, and allowing the system optionally to adjust its latency, should that be necessary.

The third approach presents the advantage of starting playback with latency that is small, which is particularly advantageous when channel hopping repeatedly, so that the channels are played back as quickly as possible.

By way of example, it might be decided to start playback when the filling target (buffer_target value) is reached, or even before (e.g. 50 ms). Under such circumstances, after starting playback quickly, the system makes systematic adjustments to latency once a playback session is under way.

In a particular implementation, the decoder appliance is connected to a plurality of wireless speakers (specifically a number N of speakers).

Under such circumstances, the determined filling level is calculated from the filling levels of the buffers of at least a group of speakers from among said speakers. Thus, instead of taking account of the present filling level (buffer_value) of a single speaker, the system takes account of a global buffer value (global_buffer_value) that is calculated from the present filling level (buffer_value) of each of the speakers in the group under consideration.

In a first a variant, the determined filling level is calculated from the filling levels of the buffers of the N speakers.

Specifically, the global value is calculated as being the lowest filling level from among the present filling levels of the buffers of the N speakers that are paying back sound, i.e.:global_buffer_value=min(buffer_value1, . . . , buffer_valueN).

Thereafter, the system uses this global buffer value (instead of and replacing the individual value as defined in the above-described implementation) in order to calculate the necessary latency adjustment. Thus, the system adjusts itself in such a manner as to avoid any of the speakers presenting a break in audio.

In a second variant, the global value of the buffer is calculated while taking only a subset of the speakers into consideration, choosing to leave the potential for one or more speakers to suffer breaks in sound playback. By way of example, such a variant is advantageous in a system possessing synchronized speakers in a plurality of rooms (referred to as a “multi-room” audio system). Under such circumstances, the global buffer value could be calculated while taking into consideration only the speakers that are situated in the same room as the room in which the TV set is located.

The underlying logic is that it is difficult to accept breaks in sound from the speakers actively involved in the TV experience, while such breaks are more acceptable for speakers situated in other rooms. When the connections with the speakers in the other rooms are more disturbed, this makes it possible to avoid excessively increasing the latency of the main TV system.

In this second variant, two groups are defined: a group of main speakers that are situated in the proximity of the TV set, and that are needed for audio-video playback of the “home cinema” type, and a group of auxiliary speakers that are situated in one or more other rooms.

With the simplest strategy, the global filling level is calculated while making use of the buffer filling levels of the main speakers only.

The global value then corresponds to the lowest of the filling levels from among the present filling levels of the buffers of the main speakers.

Intermediate strategies are possible, giving precedence to the speakers that are said to be “necessary” (those that are situated in the same room as the screen), while still taking account of the “auxiliary” speakers, providing they do not penalize the system excessively.

For example, account may be taken of the auxiliary speakers only if their buffer values are close to the buffer values of the necessary speakers, with a tolerance threshold of 5% or 10%. Under such circumstances, the global buffer value is calculated as follows (with a selected tolerance threshold of 10%):
necessary_buffer value=min(necessary buffer_values)
global buffer_value=min(necessary_buffer_value,
max(necessary_buffer_value*0.9,min(auxiliary buffer values))).

Naturally, the invention is not limited to the embodiment described and covers any variant coming within the ambit of the invention as defined by the claims.

In particular, the structure of the system could be different from that described, and by way of example it could have some other number of wireless speakers.

The Wi-Fi transmitter could be incorporated in the decoder, or the decoder could be connected to a network of the local area network (LAN) type by a wired connection. The speaker could be connected to the LAN type network via a Wi-Fi connection using an access point external to the decoder.

The method may be applied to any one of the buffers of the speaker (e.g.252or254) or to a plurality of the buffers of the speaker.

More than two adjustment thresholds may be provided, or on the contrary there could be only one.

","Gabriel Bouvigne, Vincent SCHOTT, Frédéric SODI","Consumer Goods, Technology,Media, Digital Media, Entertainment, Telecommunications,Technology, Media, Healthcare, Artificial Intelligence & Machine Learning","Loudspeakers, Microphone systems, Audio transducers, Acoustic feedback control,Speech recognition, audio coding, voice processing, speech synthesis, text-to-speech systems,Television transmission, pictorial communication, facsimile systems, image data processing",A method of playing back audio/video signals by means of an audio/video system includes a decoder appliance connected to a display screen and to a wireless speaker including a buffer storing audio frames so as to allow latency between receiving encoded audio frames and supplying decoded audio frames. The method comprises the steps of: determining a filling level of the buffer; comparing the filling level of the buffer with a filling target for the buffer and deducing a filling difference therefrom; and increasing or decreasing the number of audio frames that are stored as a function of the filling difference so as to adjust latency.
12020720,"BACKGROUND

An autonomous vehicle (AV) is a motorized vehicle that can operate without a human driver. An exemplary autonomous vehicle includes a plurality of sensor systems, such as but not limited to, a lidar sensor system, a camera sensor system, and a radar sensor system, amongst others. The autonomous vehicle operates based upon sensor signals output by the sensor systems.

Some vehicles include systems and components that are intended to mitigate various types of noise that result during operation of the vehicle. For instance, some vehicles include components that are intended to isolate the interior of the vehicle from the engine compartment in order to reduce audible engine noise in the interior of the vehicle. Some vehicles also include dampening components that are intended to dampen noise-causing vibrations of various parts of the vehicle. Passive approaches to noise mitigation in vehicles generally still allow significant noise in the interior of the vehicle, however, given the size, weight, and aesthetic constraints for noise mitigation components.

Active noise cancellation now finds application in headphones. Conventionally, active noise cancellation headphones incorporate a microphone, processing circuitry, and a speaker (ordinarily the same speaker used to output whatever the listener is listening to). The microphone is positioned in close proximity to the listener's ear and receives ambient noise from the listener's environment. The microphone outputs an audio signal indicative of the received noise. The processing circuitry receives the audio signal output by the microphone and generates a phase-shifted signal that is 180° out of phase with the audio signal. The phase-shifted signal is then output by the speaker, which is also positioned in close proximity to the listener's ear. This approach may be suitable to attenuate ambient noise when there is a microphone and a speaker positioned close to a listener's ear (e.g., within three inches of the listener's ear), but this approach exhibits poor performance when the microphone used to detect the noise and/or the speaker used to output the phase-shifted signal are farther away from the listener.

SUMMARY

The following is a brief summary of subject matter that is described in greater detail herein. This summary is not intended to be limiting as to the scope of the claims.

Described herein are various technologies that pertain to active noise cancellation within an interior of a vehicle. With more particularity, technologies are described herein for attenuating noise heard by a passenger in a passenger cabin of an AV that originates from noise sources outside the vehicle. In an exemplary embodiment, a microphone mounted on the vehicle receives a sound from a noise source external to the vehicle (e.g., noise from a construction zone, a siren of an emergency-response vehicle, a horn of another vehicle, or substantially any other source of noise in a driving environment of the AV). Responsive to the sound impinging on the microphone, the microphone outputs an audio signal (e.g., an electrical or optical signal) that is representative of the sound. The audio signal is received by a digital signal processing module (DSP). Further, a sensor in the passenger cabin of the AV outputs data indicative of a position of a head of a passenger in the AV. The DSP receives the data and outputs a complementary signal based upon the audio signal and the data indicative of the position of the head of the passenger. As used herein, a complementary signal is configured such that when the complementary signal is output to a speaker inside the passenger cabin, the speaker generates a sound that causes the sound of the noise source to be attenuated at the position of the head of the passenger.

In another exemplary embodiment, the DSP generates the complementary signal based upon the audio signal output by the microphone and a location of the noise source in the driving environment of the AV. In the embodiment, a sensor mounted on the AV outputs a sensor signal that is indicative of objects in the driving environment. By way of example, the sensor can be a lidar sensor that outputs a signal indicative of positions of objects in the driving environment. In another example, the sensor can be a vision sensor (e.g., a camera) that outputs images of the driving environment. A computing system included on the AV can identify a location of the noise source in the driving environment based upon the output of the sensor. The DSP generates the complementary signal based upon the audio signal output by the microphone and the location of the noise source identified by the computing system, which may include or be in communication with the DSP. The complementary signal can then be output to one or more speakers in the passenger cabin of the AV.

The above-described technologies present various advantages over conventional noise mitigation technologies. First, the above-described technologies provide greater noise mitigation than passive noise-dampening approaches alone. Second, the active noise cancellation techniques described herein provide greater attenuation of noise than conventional active noise cancellation techniques when the noise-detecting microphone and the speaker that outputs an interfering signal are not in close proximity to the passenger (e.g., greater than six inches from the passenger's head).

The above summary presents a simplified summary in order to provide a basic understanding of some aspects of the systems and/or methods discussed herein. This summary is not an extensive overview of the systems and/or methods discussed herein. It is not intended to identify key/critical elements or to delineate the scope of such systems and/or methods. Its sole purpose is to present some concepts in a simplified form as a prelude to the more detailed description that is presented later.

",utility,2024-06-25,Systems and methods for active noise cancellation for interior of autonomous vehicle,B2,18.0,,GM Cruise Holdings LLC,"G10K 11/178, G06T 7/70, G10L 21/232, H04R 3/4, H04R 5/2, H04R 5/27, H04R 5/4, H04S 7/0, G10L 21/216","G10K11/17873,G10L21/0232,G06T7/70,G10K11/17815,H04R3/04,H04R5/02,H04R5/027,H04R5/04,H04S7/303,G10K11/178,G10K2210/128,G10K2210/1282,G10K2210/3027,G10K2210/3055,G10L2021/02163,H04R2499/13","inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,additional,additional,additional,additional,additional,additional,additional","Image analysis-Determining position or orientation of objects or cameras , Methods or devices for transmitting, conducting or directing sound in general; Methods or devices for protecting against, or for damping, noise or other acoustic waves in general-Methods or devices for protecting against, or for damping, noise or other acoustic waves in general -using interference effects; Masking sound-by electro-acoustically regenerating the original acoustic waves in anti-phase, Methods or devices for transmitting, conducting or directing sound in general; Methods or devices for protecting against, or for damping, noise or other acoustic waves in general-Methods or devices for protecting against, or for damping, noise or other acoustic waves in general -using interference effects; Masking sound-by electro-acoustically regenerating the original acoustic waves in anti-phase-characterised by the analysis of input or output signals, e.g. frequency range, modes, transfer functions-characterised by the analysis of the acoustic paths, e.g. estimating, calibrating or testing of transfer functions or cross-terms-between the reference signals and the error signals, i.e. primary path, Methods or devices for transmitting, conducting or directing sound in general; Methods or devices for protecting against, or for damping, noise or other acoustic waves in general-Methods or devices for protecting against, or for damping, noise or other acoustic waves in general -using interference effects; Masking sound-by electro-acoustically regenerating the original acoustic waves in anti-phase-General system configurations-using a reference signal without an error signal, e.g. pure feedforward, Details of active noise control [ANC] covered by G10K11/178 but not provided for in any of its subgroups-Applications-Vehicles, Details of active noise control [ANC] covered by G10K11/178 but not provided for in any of its subgroups-Applications-Vehicles-Automobiles, Details of active noise control [ANC] covered by G10K11/178 but not provided for in any of its subgroups-Means-Computational-Feedforward, Details of active noise control [ANC] covered by G10K11/178 but not provided for in any of its subgroups-Means-Computational-Transfer function of the acoustic system, Speech or voice signal processing techniques to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility -Speech enhancement, e.g. noise reduction or echo cancellation -Noise filtering-characterised by the method used for estimating noise-Number of inputs available containing the signal or the noise to be suppressed-Only one microphone, Speech or voice signal processing techniques to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility -Speech enhancement, e.g. noise reduction or echo cancellation -Noise filtering-characterised by the method used for estimating noise-Processing in the frequency domain, Aspects covered by H04R or H04S not otherwise provided for in their subgroups-General applications-Acoustic transducers and sound field adaptation in vehicles, Circuits for transducers ; , loudspeakers or microphones-for correcting frequency response, Stereophonic arrangements -Spatial or constructional arrangements of loudspeakers, Stereophonic arrangements -Spatial or constructional arrangements of microphones, e.g. in dummy heads, Stereophonic arrangements -Circuit arrangements, ; e.g. for selective connection of amplifier inputs/outputs to loudspeakers, for loudspeaker detection, or for adaptation of settings to personal preferences or hearing impairments , Indicating arrangements; Control arrangements, e.g. balance control-Control circuits for electronic adaptation of the sound field-Electronic adaptation of stereophonic sound system to listener position or orientation -Tracking of listener position or orientation","COMPUTING; CALCULATING OR COUNTING, MUSICAL INSTRUMENTS; ACOUSTICS, ELECTRIC COMMUNICATION TECHNIQUE","IMAGE DATA PROCESSING OR GENERATION, IN GENERAL, SOUND-PRODUCING DEVICES; METHODS OR DEVICES FOR PROTECTING AGAINST, OR FOR DAMPING, NOISE OR OTHER ACOUSTIC WAVES IN GENERAL; ACOUSTICS NOT OTHERWISE PROVIDED FOR, SPEECH ANALYSIS TECHNIQUES OR SPEECH SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING TECHNIQUES; SPEECH OR AUDIO CODING OR DECODING, LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS , STEREOPHONIC SYSTEMS ",2.0,32.0,"DETAILED DESCRIPTION

Various technologies pertaining to active noise cancellation in an interior of an autonomous vehicle are now described with reference to the drawings, wherein like reference numerals are used to refer to like elements throughout. In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of one or more aspects. It may be evident, however, that such aspect(s) may be practiced without these specific details. In other instances, well-known structures and devices are shown in block diagram form in order to facilitate describing one or more aspects. Further, it is to be understood that functionality that is described as being carried out by certain system components may be performed by multiple components. Similarly, for instance, a component may be configured to perform functionality that is described as being carried out by multiple components.

Moreover, the term “or” is intended to mean an inclusive “or” rather than an exclusive “or.” That is, unless specified otherwise, or clear from the context, the phrase “X employs A or B” is intended to mean any of the natural inclusive permutations. That is, the phrase “X employs A or B” is satisfied by any of the following instances: X employs A; X employs B; or X employs both A and B. In addition, the articles “a” and “an” as used in this application and the appended claims should generally be construed to mean “one or more” unless specified otherwise or clear from the context to be directed to a singular form.

As used herein, the terms “component” and “system” are intended to encompass computer-readable data storage that is configured with computer-executable instructions that cause certain functionality to be performed when executed by a processor. The computer-executable instructions may include a routine, a function, or the like. It is also to be understood that a component or system may be localized on a single device or distributed across several devices. Further, as used herein, the term “exemplary” is intended to mean “serving as an illustration or example of something.”

Referring now to the drawings,FIG.1illustrates an autonomous vehicle100in an exemplary driving environment101. The autonomous vehicle100can navigate about roadways without human conduction based upon sensor signals outputted by sensor systems of the autonomous vehicle100. The autonomous vehicle100includes a plurality of sensor systems, namely, a sensor system1102, . . . , and a sensor system N104, where N can be substantially any integer greater than 1 (collectively referred to herein as sensor systems102-104). The sensor systems102-104are of different types and are arranged about the autonomous vehicle100. For example, the sensor system1102may be a lidar sensor system and the sensor system N104may be a directional microphone array. Other exemplary sensor systems included in the sensor systems102-104can include radar sensor systems, cameras, GPS sensor systems, sonar sensor systems, infrared sensor systems, and the like.

The autonomous vehicle100further includes several mechanical systems that are used to effectuate appropriate motion of the autonomous vehicle100. For instance, the mechanical systems can include, but are not limited to, a vehicle propulsion system106, a braking system108, and a steering system110. The vehicle propulsion system106may be an electric motor, an internal combustion engine, or a combination thereof. The braking system108can include an engine brake, brake pads, actuators, and/or any other suitable componentry that is configured to assist in decelerating the autonomous vehicle100. The steering system110includes suitable componentry that is configured to control the direction of movement of the autonomous vehicle100.

The autonomous vehicle100additionally includes a computing system112that is in communication with the sensor systems102-104, the vehicle propulsion system106, the braking system108, and the steering system110. The computing system112includes a processor114and memory116. The memory116includes computer-executable instructions that are executed by the processor114. Pursuant to various examples, the processor114can be or include a graphics processing unit (GPU), a plurality of GPUs, a central processing unit (CPU), a plurality of CPUs, an application-specific integrated circuit (ASIC), a microcontroller, a programmable logic controller (PLC), a field programmable gate array (FPGA), or the like.

The memory116of the computing system112also includes a perception system118. Generally, the perception system118is configured to track objects in a driving environment of the AV100based upon sensor signals output by the sensor systems102-104. In various embodiments, the perception system118receives the sensor signals output by the sensor systems102-104and identifies the presence of objects in the driving environment such as other vehicles, pedestrians, cyclists, etc. The perception system118can be configured to classify the identified objects according to one or more of various types such as car, truck, pedestrian, cyclist, static objects (e.g., utility poles, garbage cans, trees, etc.), unknown, etc. Still further, the perception system118can be configured to output locations of objects in the driving environment. Accuracy and precision of the locations output by the perception system118can vary based upon the type of sensor systems102-104included on the AV100. By way of example, where one of the sensor systems102-104is a lidar sensor, the perception system118may be able to identify locations of objects in the driving environment that are accurate to within 5 centimeters or less. In another example, where the perception system118identifies locations of objects based upon output of a microphone array (e.g., using time-difference-of-arrival techniques), the identified locations may be less accurate. The memory116of the computing system112further includes a control system126. The control system126is configured to control at least one of the mechanical systems of the autonomous vehicle100(e.g., at least one of the vehicle propulsion system106, the braking system108, and/or the steering system110). Generally, the control system126controls operation of the AV100based upon data output by the perception system118that pertains to the driving environment of the AV100.

During operation of the AV100in a driving environment, there may be various noise sources that emit sounds that are audible within an interior of the AV100(e.g., in a passenger cabin of the AV100). For instance, as the AV100operates, a noise source120emits a sound122that is potentially audible to a passenger124. The sound122may be distracting or unpleasant to the passenger124, who may be carrying on a conversation or attempting to concentrate on other things. The AV100is configured to provide active cancellation of noise within a passenger cabin of the AV100such that the sound122emitted by the noise source120is attenuated inside the passenger cabin relative to a level of the sound122absent the active noise cancellation.

In connection with providing active noise cancellation within an interior of the AV100, the AV further includes a DSP126, at least one microphone128, and at least one speaker130. Still further, the memory116includes a noise cancellation system132that is in communication with the DSP126. The microphone128receives the sound122emitted by the noise source120. Stated differently, the sound wave122emitted by the noise source120impinges on the microphone128. The microphone128outputs an audio signal that is representative of the sound122to the DSP126. The DSP126generates a complementary signal based upon the audio signal output by the microphone128. The DSP126outputs the complementary signal to the speaker130, whereupon the speaker130emits a sound134toward the passenger124. The complementary signal is configured such that the sound134emitted by the speaker destructively interferes with the sound122emitted by the noise source120at the position of the passenger124in the AV100, thereby attenuating the sound122in the perception of the passenger124.

In exemplary embodiments, the DSP126generates the complementary signal based upon the audio signal output by the microphone128using various signal processing techniques. For example, responsive to receipt of the audio signal, the DSP126can perform a fast Fourier transform (FFT) over the audio signal to generate a frequency-domain representation of the audio signal. In embodiments wherein the microphone outputs an analog audio signal to the DSP126, the DSP126can perform analog-to-digital conversion of the audio signal to generate a digital representation of the audio signal prior to executing the FFT. From the frequency-domain representation of the audio signal, the DSP126can determine relative amplitudes of various spectral components of the audio signal. The DSP126can then generate the complementary signal such that when the complementary signal and the audio signal interfere, the resulting interference combination exhibits attenuation in one or more of the spectral components of the audio signal.

In various exemplary embodiments set forth in greater detail below, the DSP126generates the complementary signal based further upon data pertaining to a position of the head of the passenger124in the AV100and/or an identified location of the noise source120. In an example, the perception system118can be configured to identify, based upon sensor signals output by the sensor systems102-104, a location of a head of the passenger124and/or a position of the noise source102in the driving environment101. The noise cancellation system132can output either or both of the identified positions to the DSP126. As set forth in greater detail below, the DSP126can generate the complementary signal based upon sound propagation models that take into account the position of the head of the passenger124. and/or the location of the noise source120.

It is further to be appreciated that the AV100can also include various passive noise cancellation elements such as insulation in the doors, side panels, or other components of the AV100. Furthermore, passive noise cancellation elements included in the AV100can be configured to dampen noise to a greater extent than would be allowable for a vehicle requiring human conduction. By way of example, a vehicle operated by human conduction would be required to allow the sounds of sirens or car horns to be heard within the passenger cabin of the vehicle to ensure safe operation by the human driver. In contrast, the AV100can include passive noise cancellation elements that generally do not allow such sounds to be heard, or that generally dampen such sounds to a greater extent than allowable by various laws, rules, and regulations pertaining to human-operated vehicles.

With reference now toFIG.2, an exemplary AV200is illustrated in accordance with various embodiments of the AV100. The AV200includes the computing system112, the DSP126, and the microphone128. While not shown, it is to be appreciated that the AV200further includes the sensor systems102-104and the mechanical systems (e.g., systems106-110) as described above with respect to the AV100. In accordance with various embodiments, the AV200further includes an interior sensor system202, an exterior sensor system204, and a plurality of M speakers206-208, where M is an integer greater than zero. The interior sensor system202is configured to output a sensor signal that is indicative of positions of objects in a passenger cabin of the AV200. The exterior sensor system204is generally configured to output a sensor signal that is indicative of positions of objects outside the AV200in a driving environment of the AV200, such as other vehicles in the driving environment.

In the exemplary AV200, the DSP126includes a noise propagation model210that models the propagation of sounds through the driving environment of the AV200. In the exemplary AV200, the DSP126further includes a beamforming component212that generates a respective signal for each of the plurality of speakers206-208based upon an audio signal representative of sound desirably output to a position in the passenger cabin of the AV200. The DSP126further includes an interior propagation model214that models the propagation of sounds inside a passenger cabin of the AV200. The interior propagation model214can include a model indicative of propagation of sound from each of the speakers206-208to various positions in the passenger cabin of the AV200.

Details pertaining to exemplary operations of the AV200in connection with performing active noise cancellation within a passenger cabin of the AV200are now set forth with respect toFIGS.2-5. Referring now toFIG.3, an exemplary driving environment300is illustrated wherein the AV200approaches a four-way stop intersection302. The driving environment300further includes an emergency vehicle304and a construction crew306. The emergency vehicle304and the construction crew306can be considered noise sources in the driving environment300, and they emit respective sounds308,310. The sounds308,310emitted by the emergency vehicle304and the construction crew306are eventually received at the AV200, where they impinge on the microphone128.

Referring again toFIG.2, responsive to a sound impinging on the microphone128, the microphone outputs an audio signal (e.g., an electrical signal or an optical signal) that is indicative of the sound that impinges on the microphone128. The DSP126receives the audio signal, the audio signal being indicative of noise received at the AV200. As noted above, the DSP126generates a complementary signal based on the audio signal. The complementary signal is a signal that is configured such that when the complementary signal is output by way of a speaker, the sound emitted by the speaker destructively interferes with the sound of which the audio signal output by the microphone128is representative (e.g., a sound emitted by a noise source). The destructive interference between the sound emitted by the speaker and the sound emitted from the noise source causes attenuation of the sound emitted from the noise source, thereby resulting in cancellation of the noise.

The DSP126can be further configured to output the complementary signal based upon a position of a head of a passenger in a passenger cabin of the AV200. In an exemplary embodiment, the perception system118receives a sensor signal from the interior sensor system202. The sensor signal output by the interior sensor system202is indicative of objects in an interior of the AV200. The perception system118outputs a position of a head of passenger based upon the sensor signal output by the interior sensor system202. In an exemplary embodiment, the interior sensor can be an imaging system such as a camera, or a depth camera. Referring now toFIG.4, a top-down cutaway view400of an exemplary vehicle402is illustrated, wherein a passenger cabin404of the vehicle402is shown. Several passengers406-410are shown positioned in the passenger cabin404. A camera412is positioned facing the passenger cabin404such that the camera412captures images of the passengers406-410. The camera412can be configured to output a stream of images of the passenger cabin404. In exemplary embodiments, the camera412can be configured to output color images of the passenger cabin404, depth images of the passenger cabin404wherein pixel values of a depth image are indicative of distance from the camera412, or substantially any other images that are suitable for identifying positions of passengers' heads. A perception system of the AV402can receive the images and identify positions of the heads of the passengers406-410based on the images. By way of example, the perception system of the AV402can output, based on the images, a three-dimensional position of a head of one of the passengers406-410within the passenger cabin404.

Referring again toFIG.2, the DSP126receives a position of a head of a passenger in a passenger cabin of the AV200from the perception system118. The DSP126generates a complementary signal based upon the position of the head of the passenger and an audio signal output by the microphone128, the audio signal indicative of a sound received from noise sources outside the AV200. The DSP126can generate the complementary signal based upon the interior propagation model214.

The interior propagation model214can be calibrated based upon signals output by microphones positioned in the passenger cabin of the vehicle200. In an exemplary embodiment, the interior sensor system202includes a plurality of microphones that are positioned in the passenger cabin of the vehicle200. Known calibration signals can be output by way of the speakers206-208. Each of the microphones outputs a respective audio signal that is indicative of the calibration sound received at the microphone. Based upon amplitude, phase, and frequency of the audio signals, the noise cancellation system132can identify a respective transfer function for each of the speakers206-208that indicates a change to the sound output by the speaker to each of a plurality of points in the passenger cabin of the vehicle. The interior propagation model214includes these transfer functions, and allows a resultant sound that is delivered to a point in the passenger cabin to be identified based on the transfer function and the sound originally output by a speaker in the speakers206-208.

In an exemplary embodiment, the DSP126can model propagation of the external noise through the passenger cabin to the identified position of the head of the passenger based upon the interior propagation model214to generate a target complementary signal. In exemplary embodiments, the interior propagation model214is indicative of phase and amplitude changes of sounds as they travel through the passenger cabin of the AV200. The DSP126generates a representation of the noise at the position of the head of the passenger based on the interior propagation model214. In an example, the DSP126generates the representation of the noise at the position of the head of the passenger by imparting a phase and amplitude adjustment to the audio signal output by the microphone128, the phase and amplitude adjustments based on the interior propagation model214. The DSP126then generates a target complementary signal that is representative of sound desirably delivered to the position of the head of the passenger. The DSP126generates the target complementary signal such that the target complementary signal and the representation of the noise at the position of the head of the passenger completely or partially destructively interfere with one another, resulting in attenuation of the noise.

The DSP126generates a complementary signal to be output to one or more of the speakers206-208based on the target complementary signal and the interior propagation model214. By way of example, the DSP126can generate the complementary signal such that the interior propagation model214indicates that when the complementary signal is output as sound by one or more of the speakers206-208, it is approximately equal to the target complementary signal at the position of the head of the passenger. The complementary signal can then be output by one or more of the speakers206-208, whereupon sound emitted by the speakers206-208based upon the complementary signal interferes with the noise at the position of the head of the passenger in order to attenuate the noise.

In various embodiments, the DSP126can generate a plurality of partial complementary signals by way of the beamforming component212, wherein each of the partial complementary signals is configured to be output by a different speaker in the speakers206-208. The beamforming component212generates the partial complementary signals such that when those signals are output by way of the speakers206-208as sounds, those sounds interfere at the position of the head of the passenger to result in a desired signal being heard by the passenger. The beamforming component212is configured to compute the partial complementary signals based upon the position of the head of the passenger and a target complementary signal that is desirably delivered to the position of the head of the passenger as a sound wave. The beamforming component212generates the partial complementary signals using various beamforming algorithms according to which the phase and/or amplitude of each of the partial complementary signals is adjusted in order to yield a desired waveform at a target location (e.g., the position of the head of the passenger).

When the partial complementary signals are output by way of the speakers206-208, the sounds emitted by the speakers interfere at the identified position of the head of the passenger to yield a sound wave that is substantially similar to the target complementary signal. For example, and referring again toFIG.4, a plurality of speakers414-424are positioned about the passenger cabin404. A different partial complementary signal can be output by way of each of the speakers414-424such that each of the speakers414-424emits a different sound. When the sounds emitted by the speakers414-424interfere at a location in the passenger cabin414(e.g., a position of the head of one of the passengers406-410), the sounds constructively interfere with each other to yield a sound substantially similar to the target complementary signal. These sounds destructively interfere with noise at the location, thereby resulting in attenuation of the noise.

Use of the beamforming component212as described herein allows a greater range of sounds to be delivered to a more precise region around the position of the head of the passenger. In turn, this allows for greater attenuation for a greater variety of noise at the position of the head of the passenger than would be possible using a single speaker in connection with the technologies described herein, or that would be possible with conventional active noise cancellation techniques.

The DSP126can be further configured to output the complementary signal based upon a location of a noise source in the driving environment of the AV200. By determining the location of the noise source, the AV200can accurately determine a phase value of the noise at the microphone128and a phase value of the noise when it reaches a particular location in the passenger cabin of the AV200(e.g., a position of a passenger's head). The DSP126can then generate the complementary signal based upon the phase value of the noise at the location in the passenger cabin, such that a phase of the complementary signal is aligned to cause greater attenuation of the noise than can ordinarily be achieved using conventional active noise cancelling systems.

In exemplary embodiments, the exterior sensor system204is an outward-facing sensor system that outputs a sensor signal that is indicative of one or more objects in a driving environment of the AV200. As noted above, the exterior sensor system204can be one of a plurality of sensor systems (e.g., sensor systems102-104) that continuously output data during operation of the AV200in connection with controlling various functions of the AV200. For example, the exterior sensor system204can be any or several of a lidar sensor, a radar sensor, a camera, a directional microphone array, etc. The perception system118computes locations for a plurality of objects in the driving environment of the AV200based upon sensor signals output by the exterior sensor system204.

The noise cancellation system132can be configured to identify which of a plurality of objects identified by the perception system118in the driving environment of the AV200is likely to be the source of a noise received at the microphone128. In an exemplary embodiment, the noise cancellation system132receives data indicative of the audio signal output by the microphone128. For instance, the noise cancellation system132can receive a frequency domain representation of the audio signal from the DSP126, where the frequency-domain representation is indicative of amplitudes of various spectral components of the audio signal. The noise cancellation system132can identify which of a plurality of objects in the driving environment of the AV200is a noise source based upon the data indicative of the audio signal output and other data pertaining to the driving environment that are output by the perception system118.

By way of example, and not limitation, when the audio signal is strongly tonal, the noise cancellation system132can determine that a noise source is a vehicle in the driving environment, on the assumption that the tonal noise is due to a vehicle siren or horn. In another example, the noise cancellation system132can determine that the noise source is a particular type of object in the driving environment based on a comparison of the audio signal to a signature of a certain type of object. For instance, in the driving environment300illustrated inFIG.3, the noise cancellation system132of the AV200can compare the sound310received at the microphone128as represented by the audio signal output by the microphone128to a signature for construction site noise. Responsive to determining that the audio signal is substantially similar to the signature for the construction site noise, the noise cancellation system132of the AV200can determine which, if any, objects in the driving environment300are likely part of a construction site based on data output by the perception system118. If, for example, the perception system118indicates that the construction crew306is likely associated with a construction site (e.g., the perception system118identifies that the construction crew306includes a group of people wearing reflective vests), the noise cancellation system132can determine that the construction crew306is the source of the noise.

In other embodiments, the noise cancellation system132can identify a location of a noise source based upon output of a microphone array. In an exemplary embodiment, the exterior sensor system204comprises a lidar sensor system and a microphone array that comprises a plurality of microphones. By way of example, and referring briefly toFIG.5, an exemplary vehicle500is shown that includes a plurality of microphones502-516arranged about the vehicle500. Depending on a direction and distance from which noise originates, the microphones502-516will receive noise from a noise source in the driving environment at different times. Referring again toFIG.2, if the exterior sensor system204of the AV200includes a microphone array, the noise cancellation system132can use TDOA, angle-of-arrival (AOA), and/or direction finding techniques to determine a coarse position solution for the noise source. Depending on factors such as a number of microphones in the exterior sensor system204, sampling rates of the microphones (e.g., sampling rate of an analog-to-digital converter that digitizes the outputs of the microphones), bandwidth of the received noise, etc., the coarse position solution can indicate a position that is accurate to within ten meters, within 50 meters, or within 100 meters. In some embodiments, the coarse position solution of the noise source may be accurate enough for computation of phase values of the noise from the noise source.

In other embodiments, the coarse position solution can be used to distinguish between more accurate position solutions for objects in the driving environment. By way of example, and referring again toFIG.3, a coarse position solution that indicates that the sound308originates from the left side of the AV200can be used by the noise cancellation system132to determine that the emergency vehicle304is the source of the sound308rather than the construction crew306. The noise cancellation system132can then determine that a position of the emergency vehicle304identified by the perception system118is the position of the noise source, which position may be identified by the perception system118based on more accurate positioning data such as data from a lidar sensor.

Responsive to the noise cancellation system132determining a location of the noise source, the noise cancellation system132outputs the location of the noise source to the DSP126. The DSP126then generates the complementary signal for the noise based upon the location of the noise source and the audio signal output by the microphone128, which is representative of the noise as received at the microphone128. In an exemplary embodiment, the DSP126determines a phase of the noise based upon the audio signal and the noise propagation model210. The DSP126back-propagates the noise waveform, represented by the audio signal, from the location of the microphone128to the location of the noise source based on the noise propagation model210, which is indicative of changes to the noise waveform as it travels in space. In various embodiments, the noise propagation model210incorporates data pertaining to conditions that affect propagation of sound in the driving environment such as temperature, humidity, altitude, etc. The DSP126recovers a representation of the original noise waveform output by the noise source from the back-propagation.

The DSP126then forward-propagates the original noise waveform from the location of the noise source to a desired location for noise cancellation in the passenger cabin of the AV200(e.g., a position of a passenger's head) based upon the noise propagation model210. In addition to incorporating data pertaining to conditions in the driving environment, the noise propagation model210can include a transfer function that represents modification of the noise waveform from the exterior of the vehicle to the interior of the vehicle. The DSP126thereby generates an expected noise signal that is expected to be heard by a passenger at the location of desired noise cancellation, including a precise estimate of phase of the expected noise signal. The DSP126then generates a complementary signal that is configured to attenuate the expected noise signal at the desired location in the passenger cabin. In an exemplary embodiment, the DSP126generates a target complementary signal based upon amplitudes of frequency components of the expected noise signal and the phase of the expected noise signal such that the target complementary signal and the expected noise signal completely destructively interfere. The DSP126can then output a complementary signal or partial complementary signals (e.g., as described above with respect to the beamforming component212) to the speakers206-208, where the complementary or partial complementary signals are configured to yield the target complementary signal at the desired location when output by way of the speakers206-208.

The systems set forth with respect to the AV200are able to attenuate noise in the passenger cabin of the AV200to a greater extent than possible with systems that do not take into account the location of the noise source, or the position of the head of a passenger in the vehicle.

While the above-described aspects have been set forth with respect to autonomous vehicles, it is to be understood that the active noise cancellation technologies described herein are also suitable for use in vehicles that are operated by a human driver. Furthermore, the active noise cancellation technologies set forth herein can be used in conjunction with various other passive noise mitigation componentry. Still further, it is to be understood that various actions described herein as being performed by a DSP can be performed by a computing system or vice versa. By way of example, computations pertaining to generating a complementary signal that are described herein as being performed by the DSP126can be performed by the processor114in connection with executing instructions of the noise cancellation system132.

FIGS.6and7illustrate exemplary methodologies relating to active noise cancellation for the interior of a vehicle. While the methodologies are shown and described as being a series of acts that are performed in a sequence, it is to be understood and appreciated that the methodologies are not limited by the order of the sequence. For example, some acts can occur in a different order than what is described herein. In addition, an act can occur concurrently with another act. Further, in some instances, not all acts may be required to implement a methodology described herein.

Moreover, the acts described herein may be computer-executable instructions that can be implemented by one or more processors and/or stored on a computer-readable medium or media. The computer-executable instructions can include a routine, a sub-routine, programs, a thread of execution, and/or the like. Still further, results of acts of the methodologies can be stored in a computer-readable medium, displayed on a display device, and/or the like.

With reference toFIG.6, a methodology600for performing active noise cancellation in a vehicle interior based upon a position of a head of a passenger in the vehicle is illustrated. The methodology600begins at602, and at604, an audio signal is received from a microphone, the audio signal being indicative of noise emitted by a noise source in a driving environment of the vehicle. At606a position of a head of a passenger of the vehicle in the interior of the vehicle is identified. In an exemplary embodiment, the position of the head of the passenger is identified based upon output of a sensor in the interior of the vehicle. At608a complementary signal is computed based upon the position of the head identified at606and the audio signal received at604. The complementary signal is configured such that when the complementary signal is output by way of a speaker, the complementary signal interferes with the noise at the position of the head of the passenger, thereby attenuating the noise. The complementary signal is output to a speaker in the interior of the vehicle at610, whereupon the methodology600completes612.

With reference now toFIG.7, a methodology700for performing active noise cancellation in a vehicle interior based upon a location of a noise source in the driving environment is illustrated. The methodology700begins at702, and at704an audio signal is received from a microphone, the audio signal indicative of a sound emitted by a noise source in a driving environment of a vehicle. At706, a location of the noise source is determined. In an exemplary embodiment, the location of the noise source is determined based upon output of a sensor that outputs data indicative of objects in the driving environment (e.g., a radar sensor, a lidar sensor, a camera, a microphone array, etc.). A complementary signal is computed at708based upon the location of the noise source and the audio signal received at704. The complementary signal is configured such that when the complementary signal is output by a speaker in the interior of the vehicle, the speaker emits a sound that causes the noise to be attenuated in the interior of the vehicle. At710the complementary signal is output to a speaker in the interior of the vehicle, whereupon the methodology700completes712.

Referring now toFIG.8, a high-level illustration of an exemplary computing device800that can be used in accordance with the systems and methodologies disclosed herein is illustrated. For instance, the computing device800may be or include the computing system112. The computing device800includes at least one processor802that executes instructions that are stored in a memory804. The instructions may be, for instance, instructions for implementing functionality described as being carried out by one or more systems discussed above or instructions for implementing one or more of the methods described above. The processor802may be a GPU, a plurality of GPUs, a CPU, a plurality of CPUs, a multi-core processor, etc. The processor802may access the memory804by way of a system bus806. In addition to storing executable instructions, the memory804may also store audio signal data, sensor data, calibration data, propagation models, computer-implemented machine learning models, and so forth.

The computing device800additionally includes a data store808that is accessible by the processor802by way of the system bus806. The data store808may include executable instructions, audio signal data, sensor data, calibration data, propagation models, computer-implemented machine learning models, etc. The computing device800also includes an input interface810that allows external devices to communicate with the computing device800. For instance, the input interface810may be used to receive instructions from an external computer device, etc. The computing device800also includes an output interface812that interfaces the computing device800with one or more external devices. For example, the computing device800may transmit control signals to the vehicle propulsion system106, the braking system108, and/or the steering system110by way of the output interface812.

Additionally, while illustrated as a single system, it is to be understood that the computing device800may be a distributed system. Thus, for instance, several devices may be in communication by way of a network connection and may collectively perform tasks described as being performed by the computing device800.

Various functions described herein can be implemented in hardware, software, or any combination thereof. If implemented in software, the functions can be stored on or transmitted over as one or more instructions or code on a computer-readable medium. Computer-readable media includes computer-readable storage media. A computer-readable storage media can be any available storage media that can be accessed by a computer. By way of example, and not limitation, such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer. Disk and disc, as used herein, include compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk, and blu-ray disc (BD), where disks usually reproduce data magnetically and discs usually reproduce data optically with lasers. Further, a propagated signal is not included within the scope of computer-readable storage media. Computer-readable media also includes communication media including any medium that facilitates transfer of a computer program from one place to another. A connection, for instance, can be a communication medium. For example, if the software is transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio and microwave are included in the definition of communication medium. Combinations of the above should also be included within the scope of computer-readable media.

Alternatively, or in addition, the functionality described herein can be performed, at least in part, by one or more hardware logic components. For example, and without limitation, illustrative types of hardware logic components that can be used include Field-programmable Gate Arrays (FPGAs), Application-specific Integrated Circuits (ASICs), Application-specific Standard Products (ASSPs), System-on-a-chip systems (SOCs), Complex Programmable Logic Devices (CPLDs), etc.

What has been described above includes examples of one or more embodiments. It is, of course, not possible to describe every conceivable modification and alteration of the above devices or methodologies for purposes of describing the aforementioned aspects, but one of ordinary skill in the art can recognize that many further modifications and permutations of various aspects are possible. Accordingly, the described aspects are intended to embrace all such alterations, modifications, and variations that fall within the scope of the appended claims. Furthermore, to the extent that the term “includes” is used in either the details description or the claims, such term is intended to be inclusive in a manner similar to the term “comprising” as “comprising” is interpreted when employed as a transitional word in a claim.

","Marko Tintor, Matt Fornero","Art & Design, Entertainment, Cybersecurity, Technology,Consumer Goods, Technology,Entertainment, Media, Digital Media,Technology, Construction, Environmental Services, Public Safety & Security,Technology, Media, Healthcare, Artificial Intelligence & Machine Learning","Image processing, enhancement, transformation, 3D modeling, virtual reality, pattern recognition,Loudspeakers, Microphone systems, Audio transducers, Acoustic feedback control,Sound damping, noise reduction, acoustic wave transmission, sound insulation,Speech recognition, audio coding, voice processing, speech synthesis, text-to-speech systems,Stereophonic systems, Spatial sound, Surround sound, Quadraphonic systems","Various technologies described herein pertain to active noise cancellation in the interior of a vehicle. In exemplary embodiments, a microphone mounted on the vehicle outputs an audio signal indicative of noise emitted by a noise source. A computing system of the vehicle determines a position of the noise source based upon sensor signals output by sensors mounted on the vehicle. The computing system further determines a position of a passenger in the vehicle based upon a sensor mounted inside the vehicle. The computing system generates a complementary signal that is configured to attenuate the noise based upon the audio signal, the position of the noise source, and the position of the passenger. The complementary signal is then output by way of a speaker in the interior of the vehicle."
12020721,"BACKGROUND OF THE INVENTION

Embodiments according to the invention are related to a time scaler for providing a time scaled version of an input audio signal.

Further embodiments according to the invention are related to an audio decoder for providing a decoded audio content on the basis of an input audio content.

Further embodiments according to the invention are related to a method for providing a time scaled version of an input audio signal.

Further embodiments according to the invention are related to a computer program for performing said method.

Storage and transmission of audio content (including general audio content, like music content, speech content and mixed general audio/speech content) is an important technical field. A particular challenge is caused by the fact that a listener expects a continuous playback of audio contents, without any interruptions and also without any audible artifacts caused by the storage and/or transmission of the audio content. At the same time, it is desired to keep the requirements with respect to the storage means and the data transmission means as low as possible, to keep the costs within an acceptable limit.

Problems arise, for example, if a readout from a storage medium is temporarily interrupted or delayed, or if a transmission between a data source and a data sink is temporarily interrupted or delayed. For example, a transmission via the internet is not highly reliable, since TCP/IP packets may be lost, and since the transmission delay over the internet may vary, for example, in dependence on the varying load situation of the internet nodes. However, it is necessitated, in order to have a satisfactory user experience, that there is a continuous playback of an audio content, without audible “gaps” or audible artifacts. Moreover, it is desirable to avoid substantial delays which would be caused by a buffering of a large amount of audio information.

In view of the above discussion, it can be recognized that there is a need for a concept which provides for a good audio quality, even in the case of a discontinuous provision of an audio information.

SUMMARY

An embodiment may have a time scaler for providing a time scaled version of an input audio signal, wherein the time scaler is configured to compute or estimate a quality of a time scaled version of the input audio signal obtainable by a time scaling of the input audio signal, and wherein the time scaler is configured to perform the time scaling of the input audio signal in dependence on the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling.

According to another embodiment, an audio decoder for providing a decoded audio content on the basis of an input audio content may have: a jitter buffer configured to buffer a plurality of audio frames representing blocks of audio samples; a decoder core configured to provide blocks of audio samples on the basis of audio frames received from the jitter buffer; a sample-based time scaler according to claim1, wherein the sample-based time scaler is configured to provide time-scaled blocks of audio samples on the basis of blocks of audio samples provided by the decoder core.

According to another embodiment, a method for providing a time scaled version of an input audio signal may have the steps of: computing or estimating a quality of a time scaled version of the input audio signal obtainable by a time scaling of the input audio signal, and performing the time scaling of the input audio signal in dependence on the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling.

Another embodiment may have a non-transitory digital storage medium having a computer program stored thereon to perform the method for providing a time scaled version of an input audio signal, wherein the method includes computing or estimating a quality of a time scaled version of the input audio signal obtainable by a time scaling of the input audio signal, and wherein the method includes performing the time scaling of the input audio signal in dependence on the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling, when said computer program is run by a computer.

An embodiment according to the invention creates a time scaler for providing a time scaled version of an input audio signal. The time scaler is configured to compute or estimate a quality of a time scaled version of the input audio signal obtainable by a time scaling of the input audio signal. Moreover, the time scaler is configured to perform the time scaling of the input audio signal in dependence on the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling. This embodiment according to the invention is based on the idea that there are situations in which a time scaling of an input audio signal would result in substantial audible distortions. Moreover, the embodiment according to the invention is based on the finding that a quality control mechanism helps to avoid such audible distortions by evaluating whether a desired time scaling would actually provide a sufficient quality of the time scaled version of the input audio signal. Accordingly, the time scaling is not only controlled by a desired time stretching or time shrinking, but also by an evaluation of the obtainable quality. Accordingly, it is possible, for example, to postpone a time scaling if the time scaling would result in an unacceptably low quality of the time scaled version of the input audio signal. However, the computational estimation of the (expected) quality of the time scaled version of the input audio signal may also be used to adjust any other parameters of the time scaling. To conclude, the quality control mechanism used in the above mentioned embodiment helps to reduce or avoid audible artifacts in a system in which a time scaling is applied.

In an embodiment, the time scaler is configured to perform an overlap-and-add operation using a first block of samples of the input audio signal and a second block of samples of the input audio signal (wherein the first block of samples of the input audio signal and the second block of samples of the input audio signal may be overlapping or non-overlapping blocks of samples, which belong to a single frame or which belong to different frames). The time scaler is configured to time-shift the second block of samples with respect to the first block of samples (for example, when compared to an original time line associated to the first block of samples and the second block of samples), and to overlap-and-add the first block of samples and the time-shifted second block of samples, to thereby obtain the time-scaled version of the input audio signal. This embodiment according to the invention is based on the finding that an overlap-and-add operation using a first block of samples and a second block of samples typically results in a good time scaling, wherein an adjustment of the time shift of the second block of samples with respect to the first block of samples allows to keep distortions reasonably small in many cases. However, it has also been found that the introduction of an additional quality control mechanism, which checks whether an envisioned overlap-and-add of the first block of samples and the time shifted second block of samples actually results in a sufficiently quality of the time scaled version of the input audio signal, helps to avoid audible artifacts with an even better reliability. In other words, it has been found that it is advantageous to perform a quality check (based on the estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling) after a desired (or advantageous) time shift of the second block of samples with respect to the first block of samples has been identified, since this procedure helps to reduce or avoid audible artifacts.

In an embodiment, the time scaler is configured to compute or estimate a quality (for example, expected quality) of the overlap-and-add operation between the first block of samples and the time-shifted second block of samples, in order to compute or estimate the (expected) quality of the time scaled version of the input audio signal obtainable by the time scaling. It has been found that the quality of the overlap-and-add operation actually has a strong impact on the quality of the time scaled version of the input audio signal obtainable by the time scaling.

In an embodiment, the time scaler is configured to determine the time shift of the second block of samples with respect to the first block of samples in dependence on a determination of a level of similarity between the first block of samples, or a portion of the first block of samples (for example, a right-sided portion, i.e., samples at the end of the first block of samples), and the second block of samples, or a portion of the second block of samples (for example, a left-sided portion, i.e. samples at the beginning of the second block of samples). This concept is based on the finding that the determination of the similarity between the first block of samples and the time-shifted second block of samples provides for an estimate of the quality of the overlap-and-add operation, and consequently also provides for a meaningful estimate of the quality of the time scaled version of the input audio signal obtainable by the time scaling. Moreover, it has been found that the level of similarity between the first block of samples (or the right-sided portion of the first block of samples) and the time-shifted second block of samples (or the left-sided portion of the time-shifted second block of samples) can be determined with good precision using moderate computational complexity.

In an embodiment, the time scaler is configured to determine an information about a level of similarity between the first block of samples, or a portion (for example, a right-sided portion) of the first block of samples, and the second block of samples, or a portion (for example, left-sided portion) of the second block of samples, for a plurality of different time shifts between the first block of samples and the second block of samples, and to determine a (candidate) time shift, to be used for the overlap-and-add operation, on the basis of the information about the level of similarity for the plurality of different time shifts. Accordingly, a time shift of the second block of samples or with respect to the first block of samples can be chosen to be adapted to the audio content. However, the quality control, which includes the computation or estimation of the (expected) quality of the time scaled version of the input audio signal obtainable by a time scaling of the input audio signal, may be performed subsequent to the determination of a (candidate) time shift to be used for the overlap-and-add operation. In other words, by using the quality control mechanism, it can be ensured that the time shift determined on the basis of an information about a level of similarity between the first block of samples (or a portion of the first block of samples) and the second block of samples (or a portion of the second block of samples) for a plurality of different time shifts actually results in a sufficiently good audio quality. Thus, artifacts can be reduced or avoided efficiently.

In an embodiment, the time scaler is configured to determine the time shift of the second block of samples with respect to the first block of samples, which time shift is to be used for the overlap-and-add operation (unless the time shifting operation is postponed in response to an insufficient quality estimate), in dependence on a target time shift information. In other words, the target time shift information is considered, and an attempt is made to determine the time shift of the second block of samples with respect to the first block of samples such that said time shift of the second block of samples with respect to the first block of samples is close to the target time shift described by the target time shift information. Consequently, it can be achieved that a (candidate) time shift, which is obtained by an overlap-and-add of the first block of samples and the time shifted second block of samples, is in agreement with a requirement (defined by the target time shift information), wherein an actual execution of the overlap-and-add operation may be prevented if the computation or estimation of the (expected) quality of the time scaled version of the input audio signal obtainable by the time scaling indicates an insufficient quality.

In an embodiment, the time scaler is configured to compute or estimate a quality (e.g., an expected quality) of the time scaled version of the input audio signal obtainable by a time scaling of the input audio signal on the basis of an information about a level of similarity between the first block of samples, or a portion (for example, a right-sided portion) of the first block of samples, and the second block of samples, time shifted by the determined time shift, or a portion (for example, a left-sided portion) of the second block of samples, time-shifted by the determined time shift. It has been found that the level of similarity between the first block of samples, or the portion of the first block of samples, and the second block of samples, time shifted by the determined time shift, or the portion of the second block of samples, time shifted by the determined time shift, constitutes a good criterion for deciding whether the time scaled version of the input audio signal obtainable by the time scaling would have a sufficient quality or not.

In an embodiment, the time scaler is configured to decide, on the basis of the information about the level of similarity between the first block of samples, or a portion (for example, right-sided portion) of the first block of samples, and the second block of samples, time-shifted by the determined time shift, or a portion (for example, a left-sided portion) of the second block of samples, time-shifted by the determined time shift, whether a time scaling is actually performed. Accordingly, a determination of the time shift, which is identified as a candidate time shift, using a first (typically computationally simpler and not highly reliable) algorithm is followed by a quality check, which is based on information about the level of similarity between the first block of samples (or a portion of the first block of samples) and the second block of samples, time shifted by the determined time shift (or a portion of the second block of samples, time shifted by the determined time shift). The “quality check” on the basis of said information is typically more reliable than the mere determination of the candidate time shift, and is therefore used to finally decide whether the time scaling is actually performed. Thus, the time scaling can be prevented if the time scaling would result in excessive audible artifacts (or distortions).

In an embodiment, the time scaler is configured to time-shift a second block of samples with respect to a first block of samples, and to overlap-and-add the first block of samples and the time-shifted second block of samples, to thereby obtain the time-scaled version of the input audio signal, if the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling indicates a quality which is larger than or equal to a quality threshold value. The time scaler is configured to determine a time shift of the second block of samples with respect to the first block of samples in dependence on a determination of a level of similarity, evaluated using a first similarity measure, between the first block of samples, or a portion (for example, a right-sided portion) of the first block of samples, and the second block of samples, or a portion (for example, a left-sided portion) of the second block of samples. The time scaler is further configured to compute or estimate a quality (e.g., an expected quality) of the time scaled version of the input audio signal obtainable by a time scaling of the input audio signal on the basis of an information about the level of similarity, evaluated using a second similarity measure, between the first block of samples, or a portion (for example, a right-sided portion) of the first block of samples, and the second block of samples, time-shifted by the determined time shift, or a portion (for example, a left-sided portion) of the second block of samples, time-shifted by the determined time shift. The usage of the first similarity measure and of the second similarity measure allows to quickly determine the time shift of the second block of samples with respect to the first block of samples with moderate computational complexity, and it also allows to compute or estimate the quality of the time scaled version of the input audio signal obtainable by a time scaling of the input audio signal with high precision. Thus, the two step procedure, using two different similarity measures, allows to combine a comparatively small computational complexity in the first step with a high precision in the second (quality control) step and allows to reduce or avoid audible artifacts even though the first similarity measure, which is typically computationally simple, is used for the determination of the (candidate) time shift of the second block of samples with respect to the first of samples (wherein it would typically be too demanding to use a high computational complexity similarity measure, like the second similarity measure, when determining a candidate time shift of the second block of samples with respect to the first block of samples).

In an embodiment, the second similarity measure is computationally more complex than the first similarity measure. Accordingly, the “final” quality check can be performed with high precision, while an easy determination of the time shift of the second block of samples with respect to the first block of samples can be performed in an efficient manner.

In an embodiment, the first similarity measure is a cross correlation or a normalized cross correlation or an average magnitude difference function or a sum of squared errors. Advantageously, the second similarity measure is a combination of cross correlations or of normalized cross correlations for a plurality of different time shifts. It has been found that a cross correlation, a normalized cross correlation, an average magnitude difference function or a sum of squared errors allows for a good and efficient determination of the (candidate) time shift of the second block of samples with respect to the first block of samples. Moreover, it has been found that a similarity measure which is a combination of cross correlations or normalized cross correlations for a plurality of different time shifts is a highly reliable quantity for evaluating (computing or estimating) the quality of the time scaled version of the input audio signal obtainable by the time scaling.

In an embodiment, the second similarity measure is a combination of cross correlations for at least four different time shifts. It has been found that the combination of cross correlations for at least four different time shifts allows for a precise evaluation of the quality, since variations of the signal over time can also be considered by determining the correlations for at least four different time shifts. Also, harmonics can be considered to some degree by using cross correlations for at least four different time shifts. Consequently, a particularly good evaluation of the obtainable quality can be achieved.

In an embodiment, the second similarity measure is a combination of a first cross correlation value and of a second cross correlation value, which are obtained for time shifts which are spaced by an integer multiple of a period duration of a fundamental frequency of an audio content of the first block of samples or of the second block of samples, and of a third cross correlation value and a fourth cross correlation value, which are obtained for time shifts which are spaced by an integer multiple of the period duration of the fundamental frequency of the audio content, wherein a time shift for which the first cross correlation value is obtained is spaced from a time shift for which the third cross correlation value is obtained by an odd multiple of half the period duration of the fundamental frequency of the audio content. Accordingly, the first cross correlation value and the second cross correlation value may provide an information whether the audio content is at least approximately stationary over time. Similarly, the third cross correlation value and the fourth cross correlation value also provide an information whether the audio content is at least approximately stationary over time. Moreover, the fact that the third cross correlation value and the fourth cross correlation value are “temporally offset” with respect to the first cross correlation value and the second cross correlation value allows for a consideration of harmonics. To conclude, the computation of the second similarity measure on the basis of a combination of the first cross correlation value, the second cross correlation value, the third cross correlation value, and the fourth cross correlation value brings along a high accuracy, and consequently a reliable result for the computation (or estimation) of the (expected) quality of the time scaled version of the input audio signal obtainable by the time scaling.

In an embodiment, the second similarity measure q is obtained according to q=c(p)*c(2*p)+c(3/2*p)*c(1/2*p) or according to q=c(p)*c(−p)+c(−1/2*p)*c(1/2*p). In the above equations, c(p) is a cross correlation value between a first block of samples and a second block of samples, which are shifted in time (with respect to each other, and with respect to an original time line) by a period duration p of a fundamental frequency of an audio content of the first block of samples or of the second block of samples. c(2*p) is a cross correlation value between a first block of samples and a second block of samples, which are shifted in time by 1/2*p. c(3/2*p) is a cross correlation value between a first block of samples and a second block of samples, which are shifted in time by 3/2*p. c(1/2*p) is a cross correlation value between a first block of samples and a second block of samples, which are shifted in time by 1/2*p. c(−p) is a cross correlation value between a first block of samples and a second block of samples, which are shifted in time by −p and c(−1/2*p) is a cross correlation value between a first block of samples and a second block of samples, which are shifted in time by −1/2*p. It has been found that the usage of the above equations results in a particularly good and reliable computation (or estimation) of the (expected) quality of the time scaled version of the input audio signal obtainable by the time scaling.

In an embodiment, the time scaler is configured to compare a quality value, which is based on a computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling, with a variable threshold value, to decide whether a time scaling should be performed or not. Usage of a variable threshold value allows to adapt the threshold for deciding whether a time scaling should be performed or not to the situation. Accordingly, the quality requirements for performing a time scaling can be increased in some situations, and can be reduced in other situations, for example, depending on previous time scaling operations, or any other characteristics of the signal. Consequently, the significance of the decision whether to perform the time scaling or not can be further increased.

In an embodiment, the time scaler is configured to reduce the variable threshold value, to thereby reduce a quality requirement, in response to a finding that a quality of a time scaling would have been insufficient for one or more previous blocks of samples. By reducing the variable threshold value, it can be avoided that a time scaling is omitted over an extended period of time, because this might result in a buffer underrun or buffer overrun and would therefore be more detrimental than a generation of some artifacts caused by the time scaling. Thus, problems which would be caused by an excessive delaying of a time scaling can be avoided.

In an embodiment, the time scaler is configured to increase the variable threshold value, to thereby increase a quality requirement, in response to the fact that a time scaling has been applied to one or more previous blocks of samples. Accordingly, it can be ensured that subsequent blocks of samples are only time scaled if a comparatively high quality level (higher than a “normal” quality level) can be reached. In contrast, a time scaling of a sequence of subsequent blocks of samples is prevented if the time scaling would not fulfill comparatively high quality requirements. This is appropriate, since an application of a time scaling to a plurality of subsequent blocks of samples would typically result in artifacts unless the time scaling fulfills the comparatively high quality requirements (which are typically higher than “normal” quality requirements applicable if only a single block of samples, rather than a contiguous sequence of blocks of samples, is to be time scaled).

In an embodiment, the time scaler comprises a range-limited first counter for counting a number of blocks of samples or a number of frames which have been time scaled because a respective quality requirement of the time scaled version of the input audio signal obtainable by the time scaling has been reached. Moreover, the time scaler comprises a range-limited second counter for counting a number of blocks of samples or a number of frames which have not been time-scaled because a respective quality requirement of the time scaled version of the input audio signal obtainable by the time scaling has not been reached. The time scaler is configured to compute the variable threshold value in dependence on a value of the first counter and in dependence on a value of the second counter. By using a range limited first counter and a range limited second counter, a simple mechanism for the adjustment of the variable threshold value is obtained, which allows to adapt the variable threshold value to the respective situation while avoiding excessively small or excessively large values of the threshold value.

In an embodiment, the time scaler is configured to add a value which is proportional to the value of the first counter to an initial threshold value, and to subtract a value which is proportional to the value of the second counter therefrom, in order to obtain the variable threshold value. By using such a concept, the variable threshold value can be obtained in a very simply manner.

In an embodiment, the time scaler is configured to perform the time scaling of the input audio signal in dependence on the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling, wherein the computation or estimation of the quality of the time scaled version of the input audio signal comprises an computation or estimation of artifacts in the time scaled version of the input audio signal which would be caused by a time scaling. By computing or estimating artifacts in the time scaled version of the input audio signal which would be caused by the time scaling, a meaningful criterion for the computation or estimation of the quality can be used, because artifacts would typically degrade a hearing impression of a human listener.

In an embodiment, the computational estimation of the (expected) quality of the time scaled version of the input audio signal comprises an computation or estimation of artifacts in the time scaled version of the input audio signal which would be caused by an overlap-and-add operation of subsequent blocks of samples of the input audio signal. It has been recognized that the overlap-and-add operation may be a primary source of artifacts when performing a time scaling. Accordingly, it has been found to be an efficient approach to compute or estimate artifacts of the time scaled version of the input audio signal which would be caused by the overlap-and-add operation of subsequent blocks of samples of the input audio signal.

In an embodiment, the time scaler is configured to compute or estimate the (expected) quality of a time scaled version of the input audio signal obtainable by a time scaling of the input audio signal in dependence on a level of similarity of subsequent blocks of samples of the input audio signal. It has been found that the time scaling can typically be performed with a good quality if the subsequent blocks or samples of the input audio signal comprise a comparatively high similarity, and that distortions are typically generated by the time scaling if the subsequent blocks of samples of the input audio signal comprise substantial differences.

In an embodiment, the time scaler is configured to compute or estimate whether there are audible artifacts in a time scaled version of the input audio signal obtainable by a time scaling of the input audio signal. It has been found that the computation or estimation of audible artifacts provides a quality information which is well adapted to the human hearing impression.

In an embodiment, the time scaler is configured to postpone a time scaling to a subsequent frame or to a subsequent block of samples if the computation or estimation of the (expected) quality of the time scaled version of the input audio signal obtainable by the time scaling indicates an insufficient quality. Accordingly, it is possible to perform the time scaling at a time which is better suited for the time scaling in that less artifacts are generated. In other words, by flexibly selecting the time at which the time scaling is performed in dependence on a quality achievable by the time scaling, a hearing impression of the time scaled version of the input audio signal can be improved. Moreover, this idea is based on the finding that a slight delay of a time scaling operation typically does not provide any substantial problems.

In an embodiment, the time scaler is configured to postpone a time scaling to a time when the time scaling is less audible if the computation or estimation of the (expected) quality of the time scaled version of the input audio signal obtainable by the time scaling indicates an insufficient quality. Accordingly, hearing an impression can be improved by avoiding audible distortions.

An embodiment according to the invention creates an audio decoder for providing a decoded audio content on the basis of an input audio content. The audio decoder comprises a jitter buffer configured to buffer a plurality of audio frames representing blocks of audio samples. The audio decoder also comprises a decoder core configured to provide blocks of audio samples on the basis of audio frames received from the jitter buffer. Moreover, the audio decoder comprises a sample-based time scaler as outlined above. The sample based time scaler is configured to provide time-scaled blocks of audio samples on the basis of blocks of audio samples provided by the decoder core. This audio decoder is based on the idea that a time scaler, which is configured to perform the time scaling of the input audio signal in dependence on the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling is well adapted for usage in an audio decoder comprising a jitter buffer and a decoder core. The presence of a jitter buffer allows, for example, for postponing a time scaling operation if the computation or estimation of the (expected) quality of the time scaled version of the input audio signal obtainable by the time scaling indicates that a bad quality would be obtained. Thus, the sample-based time scaler, which includes a quality control mechanism, allows to avoid, or at least reduce, audible artifacts in the audio decoder comprising the jitter buffer and the decoder core.

In an embodiment, the audio decoder further comprises a jitter buffer control. The jitter buffer control is configured to provide a control information to the sample-based time scaler, wherein the control information indicates whether a sample-based time scaling should be performed or not. Alternatively, or in addition, the control information may indicate a desired amount of time scaling. Accordingly, the sample-based time scalar can be controlled in dependence on the demands of the audio decoder. For example, the jitter buffer control may perform a signal-adaptive controlling, and may select whether a frame-based time scaling or a sample-based time scaling should be performed in a signal-adaptive manner. Accordingly, there is an additional degree of flexibility. However, the quality control mechanism of the sample based time scaler may, for example, overrule the control information provided by the jitter buffer control, such that a sample-based time scaling is avoided (or disabled) even in a case in which the control information provided by the jitter buffer control indicates that a sample based time scaling should be performed. Thus, the “intelligent” sample-based time scaler can overrule the jitter buffer control, because the sample-based time scaler is able to obtain more detailed information about a quality obtainable by the time scaling. To conclude, the sample-based time scaler can be guided by the control information provided by the jitter buffer control, but may nevertheless “refuse” the time scaling if the quality would be substantially compromised by following the control information provided by the jitter buffer control, which helps to ensure a satisfactory audio quality.

Another embodiment according to the invention creates a method for providing a time scaled version of an input audio signal. The method comprises computing or estimating a quality (for example, an expected quality) of a time scaled version of the input audio signal obtainable by a time scaling of the input audio signal. The method further comprises performing the time scaling of the input audio signal in dependence on the computation or estimation of the (expected) quality of the time scaled version of the input audio signal obtainable by the time scaling. This method is based on the same considerations as the above mentioned time scaler.

Yet another embodiment according to the invention creates a computer program for performing said method when the computer program is running on a computer. Said computer program is based on the same considerations as the method and also as the jitter buffer described above.

",utility,2024-06-25,"Time scaler, audio decoder, method and a computer program using a quality control",B2,32.0,,Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e.V.,"G10L 21/4, G10L 19/22, G10L 25/6","G10L21/04,G10L19/022,G10L25/06","inventional,inventional,inventional","Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis -using spectral analysis, e.g. transform vocoders or subband vocoders-Blocking, i.e. grouping of samples in time; Choice of analysis windows; Overlap factoring, Speech or voice signal processing techniques to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility -Time compression or expansion, Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 -characterised by the type of extracted parameters-the extracted parameters being correlation coefficients",MUSICAL INSTRUMENTS; ACOUSTICS,SPEECH ANALYSIS TECHNIQUES OR SPEECH SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING TECHNIQUES; SPEECH OR AUDIO CODING OR DECODING,24.0,81.0,"DETAILED DESCRIPTION OF THE INVENTION

Jitter Buffer Control According toFIG.1

FIG.1shows a block schematic diagram of a jitter buffer control, according to an embodiment of the present invention. The jitter buffer control100for controlling a provision of a decoded audio content on the basis of an input audio content receives an audio signal110or an information about an audio signal (which information may describe one or more characteristics of the audio signal, or of frames or other signal portions of the audio signal).

Moreover, the jitter buffer control100provides a control information (for example, a control signal)112for a frame-based scaling. For example, the control information112may comprise an activation signal (for the frame-based time scaling) and/or a quantitative control information (for the frame-based time scaling).

Moreover, the jitter buffer control100provides a control information (for example, a control signal)114for the sample-based time scaling. The control information114may, for example, comprise an activation signal and/or a quantitative control information for the sample-based time scaling.

The jitter buffer control110is configured to select a frame-based time scaling or a sample-based time scaling in a signal-adaptive manner. Accordingly, the jitter buffer control may be configured to evaluate the audio signal or the information about the audio signal110and to provide, on the basis thereof, the control information112and/or the control information114. Accordingly, the decision whether a frame-based time scaling or a sample-based time scaling is used may be adapted to the characteristics of the audio signal, for example, in such a manner that the computationally simple frame-based time scaling is used if it is expected (or estimated) on the basis of the audio signal and/or on the basis of the information about one or more characteristics of the audio signal that the frame based time scaling does not result in a substantial degradation of the audio content. In contrast, the jitter buffer control typically decides to use the sample-based time scaling if it is expected or estimated (by the jitter buffer control), on the basis of an evaluation of the characteristics of the audio signal110, that a sample based time scaling is necessitated to avoid audible artifacts when performing a time scaling.

Moreover, it should be noted that the jitter buffer control110may naturally also receive additional control information, for example control information indicating whether a time scaling should be performed or not.

In the following, some optional details of the jitter buffer control100will be described. For example, the jitter buffer control100may provide the control information112,114such that audio frames are dropped or inserted to control a depth of a jitter buffer when the frame-based time scaling is to be used, and such that a time shifted overlap-and-add of audio signal portions is performed when the sample-based time scaling is used. In other words, the jitter buffer control100may cooperate, for example, with a jitter buffer (also designated as de-jitter buffer in some cases) and control the jitter buffer to perform the frame-based time scaling. In this case, the depth of the jitter buffer may be controlled by dropping frames from the jitter buffer, or by inserting frames (for example, simple frames comprising a signaling that a frame is “inactive” and that a comfort noise generation should be used) into the jitter buffer. Moreover, the jitter buffer control100may control a time scaler (for example, a sample-based time scaler) to perform a time-shifted overlap-and-add of audio signal portions.

The jitter buffer controller100may be configured to switch between a frame-based time scaling, a sample-based time scaling and a deactivation of the time scaling in a signal adaptive manner. In other words, the jitter buffer control typically does not only distinguish between a frame-based time scaling and a sample-based time scaling, but also selects a state in which there is no time scaling at all. For example, the latter state may be chosen if there is no need for a time scaling because the depth of the jitter buffer is within an acceptable range. Worded differently, the frame-based time scaling and the sample-based time scaling are typically not the only two modes of operation which can be selected by the jitter buffer control.

The jitter buffer control100may also consider an information about a depth of a jitter buffer for deciding which mode of operation (for example, frame-based time scaling, sample-based time scaling or no time scaling) should be used. For example, the jitter buffer control may compare a target value describing a desired depth of the jitter buffer (also designated as de-jitter buffer) and an actual value describing an actual depth of the jitter buffer and select the mode of operation (frame-based time scaling, sample-based time scaling, or no time scaling) in dependence on said comparison, such that the frame-based time scaling or the sample-based time scaling are chosen in order to control a depth of the jitter buffer.

The jitter buffer control100may, for example, be configured to select a comfort noise insertion or a comfort noise deletion if a previous frame was inactive (which may, for example, be recognized on the basis of the audio signal110itself, or on the basis of an information about the audio signal, like, for example, a silence identifier flag SID in the case of a discontinuous transmission mode). Accordingly, the jitter buffer control100may signal to a jitter buffer (also designated as de-jitter buffer) that a comfort noise frame should be inserted, if a time stretching is desired and a previous frame (or the current frame) is inactive. Moreover, the jitter buffer control100may instruct the jitter buffer (or de-jitter buffer) to remove a comfort noise frame (for example, a frame comprising a signaling information indicating that a comfort noise generation should be performed) if it is desired to perform a time shrinking and the previous frame was inactive (or the current frame is inactive). It should be noted that a respective frame may be considered inactive when the respective frame carries a signaling information indicating a generation of a comfort noise (and typically comprises no additional encoded audio content). Such a signaling information may, for example, take the form of a silence indication flag (SID flag) in the case of a discontinuous transmission mode.

In contrast, the jitter buffer control100may be configured to select at time-shifted overlap-and-add of audio signal portions if a previous frame was active (for example, if the previous frame did not comprise signaling information indicating that a comfort noise should be generated). Such a time shifted overlap-and-add of audio signal portions typically allows for an adjustment of a time shift between blocks of audio samples obtained on the basis of subsequent frames of the input audio information with a comparatively high resolution (for example, with a resolution which is smaller than a length of the blocks of audio samples, or which is smaller than a quarter of the length of the blocks of audio samples, or which is even smaller than or equal to two audio samples, or which is as small as a single audio sample). Accordingly, the selection of the sample-based time scaling allows for a very fine-tuned time scaling, which helps to avoid audible artifacts for active frames.

In the case that the jitter buffer control selects a sample-based time scaling, the jitter buffer control may also provide additional control information to adjust, or fine tune, the sample-based time scaling. For example, the jitter buffer control100may be configured to determine whether a block of audio samples represents an active but “silent” audio signal portion, for example an audio signal portion which comprises a comparatively small energy. In this case, i.e. if the audio signal portion is “active” (for example, not an audio signal portion for which a comfort noise generation is used in the audio decoder, rather than a more detailed decoding of an audio content) but “silent” (for example, in that the signal energy is below a certain energy threshold value, or even equal to zero), the jitter buffer control may provide the control information114to select an overlap-and-add mode, in which a time shift between a block of audio samples representing the “silent” (but active) audio signal portion and a subsequent block of audio samples is set to a predetermined maximum value. Accordingly, a sample-based time scaler does not need to identify a proper amount of time scaling on the basis of a detailed comparison of subsequent blocks of audio samples, but can rather simply use the predetermined maximum value for the time shift. It can be understood that a “silent” audio signal portion will typically not cause substantial artifacts in an overlap-and-add operation, irrespective of the actual choice of the time shift. Consequently, the control information114provided by the jitter buffer control can simplify the processing to be performed by the sample based time scaler.

In contrast, if the jitter buffer control110finds that a block of audio samples represents an “active” and non-silent audio signal portion (for example, an audio signal portion for which there is no generation of comfort noise, and which also comprises a signal energy which is above a certain threshold value), the jitter buffer control provides the control information114to thereby select an overlap-and-add mode in which the time shift between blocks of audio samples is determined in a signal-adaptive manner (for example, by the sample-based time scaler and using a determination of similarities between subsequent blocks of audio samples).

Moreover, the jitter buffer control100may also receive an information on an actual buffer fullness. The jitter buffer control100may select an insertion of a concealed frame (i.e., a frame which is generated using a packet loss recovery mechanism, for example using a prediction on the basis of previously decoded frames) in response to a determination that a time stretching is necessitated and that a jitter buffer is empty. In other words, the jitter buffer control may initiate an exceptional handling for a case in which, basically, a sample-based time scaling would be desired (because the previous frame, or the current frame, is “active”), but wherein a sample based time scaling (for example using an overlap-and-add) cannot be performed appropriately because the jitter buffer (or de-jitter buffer) is empty. Thus, the jitter buffer control100may be configured to provide appropriate control information112,114even for exceptional cases.

In order to simplify the operation of the jitter buffer control100, the jitter buffer control100may be configured to select the frame-based time scaling or the sample-based time scaling in dependence on whether a discontinuous transmission (also briefly designated as “DTX”) in conjunction with comfort noise generation (also briefly designated as “CNG”) is currently used. In other words, the jitter buffer control100may, for example, select the frame-based time scaling if this is recognized, on the basis of the audio signal or on the basis of an information about the audio signal, that a previous frame (or a current frame) is an “inactive” frame, for which a comfort noise generation should be used. This can be determined, for example, by evaluating a signaling information (for example, a flag, like the so-called “SID” flag), which is included in an encoded representation of the audio signal. Accordingly, the jitter buffer control may decide that the frame-based time scaling should be used if a discontinuous transmission in conjunction with a comfort noise generation is currently used, since it can be expected that only small audible distortions, or no audible distortions, are caused by such a time scaling in this case. In contrast, the sample-based time scaling may be used otherwise (for example, if a discontinuous transmission in conjunction with a comfort noise generation is not currently used), unless there are any exceptional circumstances (like, for example, an empty jitter buffer).

Advantageously, the jitter buffer control may select between one out of (at least) four modes in the case that a time scaling is necessitated. For example, the jitter buffer control may be configured to select a comfort noise insertion or a comfort noise deletion for a time scaling if a discontinuous transmission in conjunction with a comfort noise generation is currently used. In addition, the jitter buffer control may be configured to select an overlap-add-operation using a predetermined time shift for a time scaling if a current audio signal portion is active but comprises a signal energy which is smaller than or equal to an energy threshold value, and if a jitter buffer is not empty. Moreover, the jitter buffer control may be configured to select an overlap-add operation using a signal-adaptive time shift for a time scaling if a current audio signal portion is active and comprises a signal energy which is larger than or equal to the energy threshold value and if the jitter buffer is not empty. Finally, the jitter buffer control may be configured to select an insertion of a concealed frame for a time scaling if a current audio signal portion is active and if the jitter buffer is empty. Accordingly, it can be seen that the jitter buffer control may be configured to select a frame-based time scaling or a sample-based time scaling in a signal-adaptive manner.

Moreover, it should be noted that the jitter buffer control may be configured to select an overlap-and-add operation using a signal-adaptive time shift and a quality control mechanism for a time scaling if a current audio signal portion is active and comprises a signal energy which is larger than or equal to the energy threshold value and if the jitter buffer is not empty. In other words, there may be an additional quality control mechanism for the sample-based time scaling, which supplements the signal adaptive selection between a frame-based time scaling and a sample-based time scaling, which is performed by the jitter buffer control. Thus, a hierarchical concept may be used, wherein the jitter buffer performs the initial selection between the frame-based time scaling and the sample-based time scaling, and wherein an additional quality control mechanism is implemented to ensure that the sample-based time scaling does not result in an inacceptable degradation of the audio quality.

To conclude, a fundamental functionality of the jitter buffer control100has been explained, and optional improvements thereof have also been explained. Moreover, it should be noted that the jitter buffer control100can be supplemented by any of the features and functionalities described herein.

Time Scaler According toFIG.2

FIG.2shows a block schematic diagram of a time scaler200according to an embodiment of the present invention. The time scaler200is configured to receive an input audio signal210(for example, in the form of a sequence of samples provided by a decoder core) and provides, on the basis thereof, a time scaled version212of the input audio signal. The time scaler200is configured to compute or estimate a quality of a time scaled version of the input audio signal obtainable by a time scaling of the input audio signal. This functionality may be performed, for example, by a computation unit. Moreover, the time scaler200is configured to perform a time scaling of the input audio signal210in dependence on the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling, to thereby obtain the time scaled version of the input audio signal212. This functionality may, for example, be performed by a time scaling unit.

Accordingly, the time scaler may perform a quality control to ensure that excessive degradations of an audio quality are avoided when performing the time scaling. For example, the time scaler may be configured to predict (or estimate), on the basis of the input audio signal, whether an envisaged time scaling operation (like, for example, an overlap-and-add operation performed on the basis of time shifted blocks of (audio) samples is expected to result in a sufficiently good audio quality. In other words, the time scaler may be configured to compute or estimate the (expected) quality of the time scaled version of the input audio signal obtainable by time scaling of the input audio signal before the time scaling of the input audio signal is actually executed. For this purpose, the time scaler may, for example, compare portions of the input audio signal which are involved in the time scaling operation (for example, in that said portions of the input audio signal are to be overlapped and added to thereby perform the time scaling). To conclude, the time scaler200is typically configured to check whether it can be expected that an envisaged time scaling will result in a sufficient audio quality of the time scaled version of the input audio signal, and to decide whether to perform the time scaling or not on the basis thereof. Alternatively, the time scaler may adapt any of the time scaling parameters (for example, a time shift between blocks of samples to be overlapped and added) in dependence on a result of the computational estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling of the input audio signal.

In the following, optional improvements of the time scaler200will be described.

In an embodiment, the time scaler is configured to perform an overlap-and-add operation using a first block of samples of the input audio signal and a second block of samples of the input audio signal. In this case, the time scaler is configured to time-shift the second block of samples with respect to the first block of samples, and to overlap-and-add the first block of samples and the time-shifted second block of samples, to thereby obtain the time scaled version of the input audio signal. For example, if a time shrinking is desired, the time scaler may input a first number of samples of the input audio signal and provide, on the basis thereof, a second number of samples of the time scaled version of the input audio signal, wherein the second number of samples is smaller than the first number of samples. In order to achieve a reduction of the number of samples, the first number of samples may be separated into at least a first block of samples and a second block of samples (wherein the first block of samples and the second block of samples may be overlapping or non-overlapping), and the first block of samples and the second block of samples may be temporally shifted together, such that the temporally shifted versions of the first block of samples and of the second block of samples overlap. In the overlap region between the shifted version(s) of the first block of samples and of the second block of samples, an overlap-and-add operation is applied. Such an overlap-and-add operation can be applied without causing substantial audible distortions if the first block of samples and the second block of samples are “sufficiently” similar in the overlap region (in which the overlap-and-add operation is performed) and advantageously also in an environment of the overlapping region. Thus, by overlapping and adding signal portions which were originally not temporally overlapping, a time shrinking is achieved, since a total number of samples is reduced by a number of samples which have not been overlapping originally (in the input audio signal210), but which are overlapped in the time scaled version212of the input audio signal.

In contrast, a time stretching can also be achieved using such an overlap-and-add operation. For example, a first block of samples and a second block of samples may be chosen to be overlapping and may comprise a first overall temporal extension. Subsequently, the second block of samples may be time shifted with respect to the first block of samples, such that the overlap between the first block of samples and the second block of samples is reduced. If the time shifted second block of samples fits well to the first block of samples, an overlap-and-add can be performed, wherein the overlap region between the first block of samples and the time shifted version of the second block of samples may be shorter both in terms of a number of samples and in terms of a time than the original overlap region between the first block of samples and the second block of samples. Accordingly, the result of the overlap-and-add operation using the first block of samples and the time shifted version of the second block of samples may comprise a larger temporal extension (both in terms of time and in terms of a number of samples) than the total extension of the first block of samples and of the second block of samples in their original form.

Accordingly, it is apparent that both a time shrinking and a time stretching can be obtained using an overlap-and-add operation using a first block of samples of the input audio signal and a second block of samples of the input audio signals, wherein the second block of samples is time shifted with respect to the first block of samples (or wherein both the first block of samples and the second block of samples are time-shifted with respect to each other).

Advantageously, the time scaler200is configured to compute or estimate a quality of the overlap-and-add operation between the first block of samples and the time-shifted version of the second block of samples, in order to compute or estimate the (expected) quality of the time scaled version of the input audio signal obtainable by the time scaling. It should be noted that there are typically hardly any audible artifacts if the overlap-and-add operation is performed for portions of the blocks of samples which are sufficiently similar. Worded differently, the quality of the overlap-and-add operation substantially influences the (expected) quality of the time scaled version of the input audio signals. Thus, estimation (or computation) of the quality of the overlap-and-add operation provides for a reliable estimate (or computation) of the quality of the time scaled version of the input audio signal.

Advantageously, the time scaler200is configured to determine the time shift of the second block of samples with respect to the first block of samples in dependence on the determination of the level of similarity between the first block of samples, or a portion (for example, right-sided portion) of the first block of samples, and the time shifted second block of samples, or a portion (for example, left sided portion) of the time shifted second block of samples. In other words, the time scaler may be configured to determine, which time shift between the first block of samples and the second block of samples is most appropriate in order to obtain a sufficiently good overlap-and-add result (or at least the best possible overlap-and-add result). However, in an additional (“quality control”) step, it may be verified whether such a determined time shift of the second block of samples with respect to the first block of samples actually brings along a sufficiently good overlap-and-add result (or is expected to bring along a sufficiently good overlap-and-add result).

Advantageously, the time scaler determines information about a level of similarity between the first block of samples, or a portion (for example, right-sided portion) of the first block of samples, and the second block of samples, or a portion (for example, left-sided portion) of the second block of samples, for a plurality of different time shifts between the first block of samples and the second block of samples, and determines a (candidate) time shift to be used for the overlap-and-add operation on the basis of the information about the level of similarity for the plurality of different time shifts. Worded differently, a search for a best match may be performed, wherein information about the level of similarity for different time shifts may be compared, to find a time shift for which the best level of similarity can be reached.

Advantageously, the time scaler is configured to determine the time shift of the second block of samples with respect to the first block of samples, which time shift is to be used for the overlap-and-add operation, in dependence on a target time shift information. In other words, a target time shift information, which may, for example, be obtained on the basis of an evaluation of a buffer fullness, a jitter and possibly other additional criteria, may be considered (taken into account) when determining which time shift is to be used (for example, as a candidate time shift) for the overlap-and-add operation. Thus, the overlap-and-add is adapted to the requirements of the system.

In some embodiments, the time scaler may be configured to compute or estimate a quality of the time scaled version of the input audio signal obtainable by a time scaling of the input audio signal on the basis of an information about a level of a similarity between the first block of samples, or a portion (for example, right-sided portion) of the first block of samples, and the second block of samples, time-shifted by the determined (candidate) time-shift, or a portion (for example, left-sided portion) of the second block of samples, time-shifted by the determined (candidate) time shift. Said information about the level of similarity provides an information about the (expected) quality of the overlap-and-add operation, and consequently also provides an information (at least an estimate) about the quality of the time scaled version of the input audio signal obtainable by the time scaling. In some cases, the computed or estimated information about the quality of the time scaled version of the input audio signal obtainable by the time scaling may be used to decide whether the time scaling is actually performed or not (wherein the time scaling may be postponed in the latter case). In other words, the time scaler may be configured to decide, on the basis of the information about the level of similarity between the first block of samples, or a portion (for example, right-sided portion) of the first block of samples, and the second block of samples, time shifted by the determined (candidate) time shift, or a portion (for example, left-sided portion) of the second block of samples, time shifted by the determined (candidate) time shift, whether a time scaling is actually performed (or not). Thus, the quality control mechanism, which evaluates the computed or estimated information on the quality of the time scaled version of the input audio signal obtainable by the time scaling, may actually result in omission of the time scaling (at least for a current block or frame of audio samples) if it is expected that an excessive degradation of an audio content would be caused by the time scaling.

In some embodiments, different similarity measures may be used for the initial determination of the (candidate) time shift between the first block of samples and the second block of samples and for the final quality control mechanism. In other words, the time scaler may be configured to time shift a second block of samples with respect to the first block of samples, and to overlap-and-add the first block of samples and the time shifted second block of samples, to thereby obtain the time scaled version of the input audio signal, if the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling indicates a quality which is larger than or equal to a quality threshold value. The time scaler may be configured to determine a (candidate) time shift of the second block of samples with respect to the first block of samples in dependence on a determination of a level of similarity, evaluated using a first similarity measure, between the first block of samples, or a portion (for example right-sided portion) of the first block of samples, and the second block of samples, or a portion (for example, left-sided portion) of the second block of samples. Also, the time scaler may be configured to compute or estimate a quality of the time scaled version of the input audio signal obtainable by a time scaling of the input audio signal on the basis of an information about a level of similarity, evaluated using a second similarity measure, between the first block of samples, or a portion (for example, right-sided portion) of the first block of samples, and the second block of samples, time shifted by the determined (candidate) time shift, or a portion (for example, left-sided portion) of the second block of samples, time shifted by the determined (candidate) time shift. For example, the second similarity measure may be computationally more complex than the first similarity measure. Such a concept is useful, since it is typically necessitated to compute the first similarity measure multiple times per time scaling operation (in order to determine the “candidate” time shift between the first block of samples and the second block of samples out of a plurality of possible time shift values between the first block of samples and the second block of samples). In contrast, the second similarity measure typically only needs to be computed one time per time shift operation, for example as a “final” quality check whether the “candidate” time shift determined using the first (computationally less complex) quality measure can be expected to result in a sufficiently good audio quality. Consequently, it is possible to still avoid the execution of an overlap-and-add, if the first similarity measure indicates a reasonably good (or at least sufficient) similarity between the first block of samples (or a portion thereof) and the time shifted second block of samples (or a portion thereof) for the “candidate” time shift but the second (and typically more meaningful or precise) similarity measure indicates that the time scaling would not result in a sufficiently good audio quality. Thus, the application of the quality control (using the second similarity measure) helps to avoid audible distortions in the time scaling.

For example, the first similarity measure may be a cross correlation or a normalized cross correlation, or an average magnitude difference function, or a sum of squared errors. Such similarity measures can be obtained in a computationally efficient manner and are sufficient to find a “best match” between the first block of samples (or a portion thereof) and the (time-shifted) second block of samples (or a portion thereof), i.e. to determine the “candidate” time shift. In contrast, the second similarity measure may, for example, be a combination of cross correlation values or normalized cross correlation values for a plurality of different time shifts. Such a similarity measure provides more accuracy and helps to consider additional signal components (like, for example, harmonics) or a stationarity of the audio signal when evaluating the (expected) quality of the time scaling. However, the second similarity measure is computationally more demanding than the first similarity measure, such that it would be computationally inefficient to apply the second similarity measure when searching for a “candidate” time shift.

In the following, some options for a determination of the second similarity measure will be described. In some embodiments, the second similarity measure may be a combination of cross correlations for at least four different time shifts. For example, the second similarity measure may be a combination of a first cross correlation value and of a second cross correlation value, which are obtained for time shifts which are spaced by an integer multiple of a period duration of a fundamental frequency of an audio content of the first block of samples or of the second block of samples, and of a third cross correlation value and a fourth cross correlation value, which are obtained for time shifts which are spaced by an integer multiple of the period duration of the fundamental frequency of the audio content. A time shift for which the first cross correlation value is obtained may be spaced from a time shift for which the third cross correlation value is obtained, by an odd multiple of half the period duration of the fundamental frequency of the audio content. If the audio content (represented by the input audio signal) is substantially stationary, and dominated by the fundamental frequency, it can be expected that the first cross correlation value and the second cross correlation value which may, for example, be normalized, are both close to one. However, since the third cross correlation value and the fourth cross correlation value are both obtained for time shifts which are spaced, by an odd multiple of half the period duration of the fundamental frequency, from the time shifts for which the first cross correlation value and the second cross correlation value are obtained, it can be expected that the third cross correlation value and the fourth cross correlation value are opposite with respect to the first cross correlation value and the second cross correlation value in case the audio content is substantially stationary and dominated by the fundamental frequency. Accordingly, a meaningful combination can be formed on the basis of the first cross correlation value, the second cross correlation value, the third cross correlation value and the fourth cross correlation value, which indicates whether the audio signal is sufficiently stationary and dominated by a fundamental frequency in a (candidate) overlap-and-add region.

It should be noted that particularly meaningful similarity measures can be obtained by computing the similarity measure q according to
q=c(p)*c(2*p)+c(3/2*p)*c(1/2*p)
or according to
q=c(p)*c(−p)+c(−1/2*p)*c(1/2*p).

In the above, c(p) is a cross correlation value between a first block of samples (or a portion thereof) and a second block of samples (or a portion thereof), which are shifted in time (for example, with respect to an original temporal position within the input audio content) by a period duration p of a fundamental frequency of an audio content of the first block of samples and/or of the second block of samples (wherein the fundamental frequency of the audio content is typically substantially identical in the first block of samples and in the second block of samples). In other words, a cross correlation value is computed on the basis of blocks of samples which are taken from the input audio content and additionally time shifted with respect to each other by the period duration p of the fundamental frequency of the input audio content (wherein the period duration p of the fundamental frequency may be obtained, for example, on the basis of a fundamental frequency estimation, an auto correlation, or the like). Similarly, c(2*p) is a cross correlation value between a first block of samples (or a portion thereof) and a second block of samples (or a portion thereof) which are shifted in time by 2*p. Similar definitions also apply to c(3/2*p), c(1/2*p), c(−p) and c(−1/2*p), wherein the argument of c(·) designates the time shift.

In the following, some mechanisms for deciding whether or not time scaling should be performed will be explained, which may optionally be applied in the time scaler200. In an implementation, the time scaler200may be configured to compare a quality value, which is based on a computation or estimation of the (expected) quality of the time scaled version of the input audio signal obtainable by the time scaling, with a variable threshold value, to decide whether or not a time scaling should be performed. Accordingly, the decision whether or not to perform the time scaling can also be made dependent on the circumstances, like, for example, a history representing previous time scalings.

For example, the time scaler may be configured to reduce the variable threshold value, to thereby reduce a quality requirement (which is to be reached in order to enable a time scaling), in response to a finding that a quality of a time scaling would have been insufficient for one or more previous blocks of samples. Accordingly, it is ensured that a time scaling is not prevented for a long sequence of frames (or blocks of samples) which could cause a buffer overrun or buffer underrun. Moreover, the time scaler may be configured to increase the variable threshold value, to thereby increase a quality requirement (which is to be reached in order to enable a time scaling), in response to the fact that a time scaling has been applied to one or more previous blocks or samples. Accordingly, it can be prevented that too many subsequent blocks or samples are time scaled, unless a very good quality (increased with respect to a normal quality requirement) of the time scaling can be obtained. Accordingly, artifacts can be avoided which would be caused if the conditions for a quality of the time scaling were too low.

In some embodiments, the time scaler may comprise a range-limited first counter for counting a number of blocks of samples or a number of frames which have been time scaled because the respective quality requirement of the time-scaled version of the input audio signal obtainable by the time scaling has been reached. Moreover, the time scaler may also comprise a range-limited second counter for counting a number of blocks of samples or a number of frames which have not been time scaled because a respective quality requirement of the time-scaled version of the input audio signal obtainable by the time scaling has not been reached. In this case, the time scaler may be configured to compute the variable threshold value in dependence on a value of the first counter and in dependence on a value of the second counter. Accordingly, the “history” of the time scaling (and also the “quality” history) can be considered with moderate computational effort.

For example, the time scaler may be configured to add a value which is proportional to the value of the first counter to an initial threshold value, and to subtract a value which is proportional to the value of a second counter therefrom (for example, from the result of the addition) in order to obtain the variable threshold value.

In the following, some important functionalities, which may be provided in some embodiments of the time scaler200will be summarized. However, it should be noted that the functionalities described in the following are not essential functionalities of the time scaler200.

In an implementation, the time scaler may be configured to perform the time scaling of the input audio signal in dependence on the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling. In this case, the computation or estimation of the quality of the time scaled version of the input audio signal comprises a computation or estimation of the artifacts in the time scaled version of the input audio signal which would be caused by the time scaling. However, it should be noted that the computation or estimation of artifacts may be performed in an indirect manner, for example by computing a quality of an overlap-and-add operation. In other words, the computation or the estimation of the quality of the time scaled version of the input audio signal may comprise a computation or estimation of artifacts in the time scaled version of the input audio signal which would be caused by an overlap-and-add operation of subsequent blocks of samples of the input audio signal (wherein, naturally, some time shift may be applied to the subsequent blocks of samples).

For example, the time scaler may configured to compute or estimate the quality of a time scaled version of the input audio signal obtainable by a time scaling of the input audio signal in dependence on a level of similarity of the subsequent (and possibly overlapping) blocks of samples of the input audio signal.

In an embodiment, the time scaler may be configured to compute or estimate whether there are audible artifacts in a time scaled version of the input audio signal obtainable by a time scaling of the input audio signal. The estimation of audible artifacts may be performed in an indirect manner, as mentioned in the above.

As a consequence of the quality control, the time scaling may be performed at times which are well suited for the time scaling and avoided at times which are not well-suited for the time scaling. For example, the time scaler may be configured to postpone a time scaling to a subsequent frame or to a subsequent block of samples if the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the timed scaling indicates an insufficient quality (for example, a quality which is below a certain quality threshold value). Thus, the time scaling may be performed at a time which is more suitable for the time scaling, such that less artifacts (in particular, audible artifacts) are generated. In other words, the time scaler may be configured to postpone a time scaling to a time when the time scaling is less audible if the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling indicates an insufficient quality.

To conclude, the time scaler200may be improved in a number of different ways, as discussed above.

Moreover, it should be noted that the time scaler200may optionally be combined with the jitter buffer control100, wherein the jitter buffer control100may decide whether the sample-based time scaling, which is typically performed by the time scaler200, should be used or whether a frame-based time scaling should be used.

Audio Decoder According toFIG.3

FIG.3shows a block schematic diagram of an audio decoder300, according to an embodiment of the present invention.

The audio decoder300is configured to receive an input audio content310, which may be considered as an input audio representation, and which may, for example, be represented in the form of audio frames. Moreover, the audio decoder300provides, on the basis thereof, a decoded audio content312, which may, for example, be represented in the form of decoded audio samples. The audio decoder300may, for example, comprise a jitter buffer320, which is configured to receive the input audio content310, for example, in the form of audio frames. The jitter buffer320is configured to buffer a plurality of audio frames representing blocks of audio samples (wherein a single frame may represent one or more blocks of audio samples, and wherein the audio samples represented by a single frame may be logically subdivided into a plurality of overlapping or non-overlapping blocks of audio samples). Moreover, the jitter buffer320provides “buffered” audio frames322, wherein the audio frames322may comprise both audio frames included in the input audio content310and audio frames which are generated or inserted by the jitter buffer (like, for example, “inactive” audio frames comprising a signaling information signaling the generation of comfort noise). The audio decoder300further comprises a decoder core330, which receives the buffered audio frames322from the jitter buffer320and which provides audio samples332(for example, blocks with audio samples associated with audio frames) on the basis of the audio frames322received from the jitter buffer. Moreover, the audio decoder300comprises a sample-based time scaler340, which is configured to receive the audio samples332provided by the decoder core330and to provide, on the basis thereof, time-scaled audio samples342, which make up the decoded audio content312. The sample-based time scaler340is configured to provide the time-scaled audio samples (for example, in the form of blocks of audio samples) on the basis of the audio samples332(i.e., on the basis of blocks of audio samples provided by the decoder core). Moreover, the audio decoder may comprise an optional control350. The jitter buffer control350, which is used in the audio decoder300may, for example, be identical to the jitter buffer control100according toFIG.1. In other words, the jitter buffer control350may be configured to select a frame-based time scaling, which is performed by the jitter buffer320, or a sample-based time scaling, which is performed by the sample-based time scaler340in a signal-adaptive manner. Accordingly, the jitter buffer control350may receive the input audio content310, or an information about the input audio content310as the audio signal110, or as the information about the audio signal110. Moreover, the jitter buffer control350may provide the control information112(as described with respect to jitter buffer control100) to the jitter buffer320, and the jitter buffer control350may provide the control information114, as described with respect to the jitter buffer control100, to the sample-based time scaler140. Accordingly, the jitter buffer320may be configured to drop or insert audio frames in order to perform a frame-based time scaling. Moreover, the decoder core330may be configured to perform a comfort noise generation in response to a frame carrying a signaling information indicating the generation of a comfort noise. Accordingly, a comfort noise may be generated by the decoder core330in response to the insertion of an “inactive” frame (comprising a signaling information indicating that a comfort noise should be generated) into the jitter buffer320. In other words, a simple form of a frame-based time scaling may effectively result in the generation of a frame comprising comfort noise, which is triggered by the insertion of a “inactive” frame into the jitter buffer (which may be performed in response to the control information112provided by the jitter buffer control). Moreover, the decoder core may be configured to perform a “concealing” in response to an empty jitter buffer. Such a concealing may comprise the generation of an audio information for a “missing” frame (empty jitter buffer) on the basis of an audio information of one or more frames preceding the missing audio frame. For example, a prediction may be used, assuming that the audio content of the missing audio frame is a “continuation” of the audio content of one or more audio frames preceding the missing audio frame. However, any of the frame loss concealing concepts known in the art may be used by the decoder core. Consequently, the jitter buffer control350may instruct the jitter buffer320(or the decoder core330) to initiate a concealing in the case that the jitter buffer320runs empty. However, the decoder core may perform the concealing even without an explicit control signal, based on an own intelligence.

Moreover, it should be noted that the sample-based time scaler340may be equal to the time scaler200described with respect toFIG.2. Accordingly, the input audio signal210may correspond to the audio samples332, and the time scaled version212of the input audio signal may correspond to the time scaled audio samples342. Accordingly, the time scaler340may be configured to perform the time scaling of the input audio signal in dependence on a computation or estimation of the quality of the time-scaled version of the input audio signal obtainable by the time scaling. The sample-based time scaler340may be controlled by the jitter buffer control350, wherein a control information114provided by the jitter buffer control to the sample based time scaler340may indicate whether a sample-based time scaling should be performed or not. In addition, the control information114may, for example, indicate a desired amount of time scaling to be performed by the sample-based time scaler340.

It should be noted that the time scaler300may be supplemented by any of the features and functionalities described with respect to the jitter buffer control100and/or with respect to the time scaler200. Moreover, the audio decoder300may also be supplemented by any other features and functionalities described herein, for example, with respect toFIGS.4to15.

Audio Decoder According toFIG.4

FIG.4shows a block schematic diagram of an audio decoder400, according to an embodiment of the present invention. The audio decoder400is configured to receive packets410, which may comprise a packetized representation of one or more audio frames. Moreover, the audio decoder400provides a decoded audio content412, for example in the form of audio samples. The audio samples may, for example, be represented in a “PCM” format (i.e., in a pulse-code-modulated form, for example, in the form of a sequence of digital values representing samples of an audio waveform).

The audio decoder400comprises a depacker420, which is configured to receive the packets410and to provide, on the basis thereof, depacketized frames422. Moreover, the depacker is configured to extract, from the packets410, a so called “SID flag”, which signals an “inactive” audio frame (i.e., an audio frame for which a comfort noise generation should be used, rather than a “normal” detailed decoding of an audio content). The SID flag information is designated with424. Moreover, the depacker provides a real-time-transport-protocol time stamp (also designated as “RTP TS”) and an arrival time stamp (also designated as “arrival TS”). The time stamp information is designated with426. Moreover, the audio decoder400comprises a de-jitter buffer430(also briefly designated as jitter buffer430), which receives the depacketized frames422from the depacker420, and which provides buffered frames432(and possibly also inserted frames) to a decoder core440. Moreover, the de-jitter buffer430receives a control information434for a frame-based (time) scaling from a control logic. Also, the de-jitter buffer430provides a scaling feedback information436to a playout delay estimation. The audio decoder400also comprises a time scaler (also designated as “TSM”)450, which receives decoded audio samples442(for example, in the form of pulse-code-modulated data) from the decoder core440, wherein the decoder core440provides the decoded audio samples442on the basis of the buffered or inserted frames432received from the de-jitter buffer430. The time scaler450also receives a control information444for a sample-based (time) scaling from a control logic and provides a scaling feedback information446to a playout delay estimation. The time scaler450also provides time scaled samples448, which may represent time scaled audio content in a pulse-code-modulated form. The audio decoder400also comprises a PCM buffer460, which receives the time scaled samples448and buffers the time scaled samples448. Moreover, the PCM buffer460provides a buffered version of time scaled samples448as a representation of the decoded audio content412. Moreover, the PCM buffer460may provide a delay information462to a control logic.

The audio decoder400also comprises a target delay estimation470, which receives the information424(for example the SID flag) as well as the time stamp information426comprising the RTP time stamp and the arrival time stamp. On the basis of this information, the target delay estimation470provides a target delay information472, which describes a desirable delay, for example a desirable delay which should be caused by the de-jitter buffer430, the decoder440, the time scaler450and the PCM buffer460. For example, the target delay estimation470may compute or estimate the target delay information472such that the delay is not chosen unnecessarily large but sufficient to compensate for some jitter of the packets410. Moreover, the audio decoder400comprises a playout delay estimation480, which is configured to receive the scaling feedback information436from the de-jitter buffer430and the scaling feedback information446from the time scaler460. For example, the scaling feedback information436may describe a time scaling which is performed by the de-jitter buffer. Moreover, the scaling feedback information446describes a time scaling which is performed by the time scaler450. Regarding the scaling feedback information446, it should be noted that the time scaling performed by the time scaler450is typically signal adaptive such that an actual time scaling which is described by the scaling feedback information446may be different from a desired time scaling which may be described by the sample-based scaling information444. To conclude, the scaling feedback information436and the scaling feedback information446may describe an actual time scaling, which may be different from a desired time scaling because of the signal-adaptivity provided in accordance with some aspects of the present invention.

Moreover, the audio decoder400also comprises a control logic490, which performs a (primary) control of the audio decoder. The control logic490receives the information424(for example, the SID flag) from the depacker420. In addition, the control logic490receives the target delay information472from the target delay estimation470, the playout delay information482from the playout delay estimation480(wherein the playout delay information482describes an actual delay, which is derived by the playout delay estimation480on the basis of the scaling feedback information436and the scaling feedback information446). Moreover, the control logic490(optionally) receives the delay information462from the PCM buffer460(wherein, alternatively, the delay information of the PCM buffer may be a predetermined quantity). On the basis of the received information, the control logic490provides the frame-based scaling information434and the sample-based scaling information442to the de-jitter buffer430and to the time scaler450. Accordingly, the control logic sets the frame-based scaling information434and the sample-based scaling information442in dependence on the target delay information472and the playout delay information482in a signal adaptive manner, considering one or more characteristics of the audio content (like, for example, the question whether there is an “inactive” frame for which a comfort noise generation should be performed in accordance to the signaling carried by the SID flag).

It should be noted here that the control logic490may perform some or all of the functionalities of the jitter buffer control100, wherein the information424may correspond to the information110about the audio signal, wherein the control information112may correspond to the frame-based scaling information434, and wherein the control information114may correspond to the sample-based scaling information444. Also, it should be noted that the time scaler450may perform some or all of the functionalities of the time scaler200(or vice versa), wherein the input audio signal210corresponds to the decoded audio samples442, and wherein the time-scaled version212of the input audio signal corresponds to the time-scaled audio samples448.

Moreover, it should be noted that the audio decoder400corresponds to the audio decoder300, such that the audio decoder300may perform some or all of the functionalities described with respect to the audio decoder400, and vice versa. The jitter buffer320corresponds to the de-jitter buffer430, the decoder core330corresponds to the decoder440, and the time scaler340corresponds to the time scaler450. The control350corresponds to the control logic490.

In the following, some additional details regarding the functionality of the audio decoder400will be provided. In particular, the proposed jitter buffer management (JBM) will be described.

A jitter buffer management (JBM) solution is described, which can be used to feed received packets410with frames, containing coded speech or audio data, into a decoder440while maintaining continuous playout. In packet-based communications, for example, voice-over-internet-protocol (VoIP), the packets (for example, packets410) are typically subject to varying transmission times and are lost during transmission, which leads to inter-arrival jitter and missing packets for the receiver (for example, a receiver comprising the audio decoder400). Therefore, jitter buffer management and packet loss concealment solutions are desired to enable a continuous output signal without stutter.

In the following, a solution overview will be provided. In the case of the described jitter buffer management, coded data within the received RTP packets (for example, packets410) is at first depacketized (for example, using the depacker420) and the resulting frames (for example, frames422) with coded data (for example, voice data within an AMR-WB coded frame) are fed into a de-jitter buffer (for example, de-jitter buffer430). When new pulse-code-modulated data (PCM data) is necessitated for the playout, it needs to be made available by the decoder (for example, by the decoder440). For this purpose, frames (for example, frames432) are pulled from the de-jitter buffer (for example, from the de-jitter buffer430). By the use of the de-jitter buffer, fluctuations in arrival time can be compensated. To control the depth of the buffer, time scale modification (TSM) is applied (wherein the time scale modification is also briefly designated as time scaling). Time scale modification can happen on a coded frame basis (for example, within the de-jitter buffer430) or in a separate module (for example, within the time scaler450), allowing more-fine granular adaptations of the PCM output signal (for example, of the PCM output signal448or of the PCM output signal412).

The above described concept is illustrated, for example, inFIG.4which shows a jitter buffer management overview. To control the depth of the de-jitter buffer (for example, de-jitter buffer430) and therefore also the levels of time scaling within the de-jitter buffer (for example, de-jitter buffer430) and/or the TSM module (for example, within the time scaler450), a control logic (for example, the control logic490, which is supported by the target delay estimation470and the playout delay estimation480) is used. It employs information on the target delay (for example, information472) and playout delay (for example, information482) and whether discontinuous transmission (DTX) in conjunction with comfort noise generation (CNG) is currently used (for example, information424). The delay values are generated, for example, from separate modules (for example, modules470and480) for target and playout delay estimation, and an active/inactive bit (SID flag) is provided, for example, by the depacker module (for example, depacker420).

Depacker

In the following, the depacker420will be described. The depacker module splits RTP packets410into single frames (access units)422. It also calculates the RTP time stamp for all frames that are not the only or first frame in a packet. For example, the time stamp contained in the RTP packet is assigned to its first frame. In case of aggregation (i.e. for RTP packets containing more than one single frame) the time stamp for following frames is increased by the frame duration divided by the scale of the RTP time stamps. In addition, to the RTP time stamp, each frame is also tagged with the system time at which the RTP packet was received (“arrival time stamp”). As can be seen, the RTP time stamp information and the arrival time stamp information426may be provided, for example, to the target delay estimation470. The depacker module also determines if a frame is active or contains a silence insertion descriptor (SID). It should be noted that within non-active periods, only the SID frames are received in some cases. Accordingly, information424, which may for example comprise the SID flag, is provided to the control logic490.

De-Jitter Buffer

The de-jitter buffer module430stores frames422received on network (for example, via a TCP/IP type network) until decoding (for example, by the decoder440). Frames422are inserted in a queue sorted in ascending RTP time stamp order to undo reordering which could have happened on network. A frame at the front of the queue can be fed to the decoder440and is then removed (for example, from the de-jitter buffer430). If the queue is empty or a frame is missing according to the time stamp difference of the frame at the front (of the queue) and the previously read frame, an empty frame is returned (for example, from the de-jitter buffer430to the decoder440) to trigger packet loss concealment (if a last frame was active) or comfort noise generation (if a last frame was “SID” or inactive) in the decoder module440.

Worded differently, the decoder440may be configured to generate a comfort noise in the case that it is signaled, in a frame, that a comfort noise should be used, for example using an active “SID” flag. On the other hand, the decoder may also be configured to perform packet loss concealment, for example, by providing predicted (or extrapolated) audio samples in the case that a previous (last) frame was active (i.e., comfort noise generation deactivated) and the jitter buffer runs empty (such that an empty frame is provided to the decoder440by the jitter buffer430).

The de-jitter buffer module430also supports frame-based time scaling by adding an empty frame to the front (for example, of the queue of the jitter buffer) for time stretching or dropping the frame at the front (for example, of the queue of the jitter buffer) for time shrinking. In the case of non-active periods, the de-jitter buffer may behave as if “NO_DATA” frames were added or dropped.

Time Scale Modification (TSM)

In the following, the time-scale modification (TSM), which is also briefly designated as time scaler or sample-based time scaler herein, will be described. A modified packet-based WSOLA (waveform-similarity-based-overlap-add) (confer, for example, [Lia01]) algorithm with built-in quality control is used to perform time scale modification (briefly designated as time scaling) of the signal. Some details can be seen, for example, inFIG.9, which will be explained below. A level of time scaling is signal-dependent; signals that would create severe artifacts when scaled are detected by a quality control and low-level signals, which are close to silence, are scaled by a most possible extent. Signals that are well time-scalable, like periodic signals, are scaled by an internally derived shift. The shift is derived from a similarity measure, such as a normalized cross correlation. With an overlap-add (OLA), the end of a current frame (also designated as “second block of samples” herein) is shifted (for example, with respect to a beginning of a current frame, which is also designated as “first block of samples” herein) to either shorten or lengthen the frame.

As already mentioned, additional details regarding the time scale modification (TSM) will be described below, taking reference toFIG.9, which shows a modified WSOLA with quality control, and also taking reference toFIGS.10aand10band11.

PCM Buffer

In the following, the PCM buffer will be described. The time-scale modification module450changes a duration of PCM frames outputted by the decoder module with a time varying scale. For example, 1024 samples (or 2048 samples) may be outputted by the decoder440per audio frame432. In contrast, a varying number of audio samples may be outputted by the time scaler450per audio frame432due to the sample-based time scaling. In contrast, a loudspeaker sound card (or, generally, a sound output device) typically expects a fixed framing, for example, 20 ms. Therefore, an additional buffer with first-in, first-out behavior is used to apply a fixed framing on the time-scaler output samples448.

When looking at the whole chain, this PCM buffer460does not create an additional delay. Rather, the delay is just shared between the de-jitter buffer430and the PCM buffer460. Nevertheless, it is a goal to keep the number of samples stored in the PCM buffer460as low as possible, because this increases a number of frames stored in the de-jitter buffer430and thus reduces a probability of late-loss (wherein the decoder conceals a missing frame which is received later).

The pseudo program code shown inFIG.5shows an algorithm to control the PCM buffer level. As can be seen from the pseudo program code ofFIG.5, a sound card frame size (“soundCardFrameSize”) is computed on the basis of a sample rate (“sampleRate”), where it is assumed, as an example, that a frame duration is 20 ms. Accordingly, a number of samples per sound card frame is known. Subsequently, the PCM buffer is filled by decoding audio frames432(also designated as “accessUnit”) until a number of samples in the PCM buffer (“pcmBuffer_nReadableSamples( )”) is no longer smaller than the number of samples per sound card frame (“soundCardFrameSize”). First, a frame (also designated as “accessUnit”) is obtained (or requested) from the de-jitter buffer430, as shown at reference numeral510. Subsequently, a “frame” of audio samples is obtained by decoding the frame432requested from the de-jitter buffer, as can be seen at reference512. Accordingly, a frame of decoded audio samples (for example, designated with442) is obtained. Subsequently, the time scale modification is applied to the frame of decoded audio samples442, such that a “frame” of time scaled audio samples448is obtained, which can be seen at reference numeral514. It should be noted that the frame of time scaled audio samples may comprise a larger number of audio samples or a smaller number of audio samples than the frame of decoded audio samples442input into the time scaler450. Subsequently, the frame of time scaled audio samples448is inserted into the PCM buffer460, as can be seen at reference numeral516.

This procedure is repeated, until a sufficient number of (time scaled) audio samples is available in the PCM buffer460. As soon as a sufficient number of (time scaled) samples is available in the PCM buffer, a “frame” of time scaled audio samples (having a frame length as necessitated by a sound playback device, like a sound card) is read out from the PCM buffer460and forwarded to the sound playback device (for example, to the sound card), as shown at reference numerals520and522.

Target Delay Estimation

In the following, the target delay estimation, which may be performed by the target delay estimator470, will be described. The target delay specifies the desired buffering delay between the time when a previous frame was played and the time this frame could have been received if it had the lowest transmission delay on network compared to all frames currently contained in a history of the target delay estimation module470. To estimate the target delay, two different jitter estimators are used, one long term and one short term jitter estimator.

Long Term Jitter Estimation

To calculate a long term jitter, a FIFO data structure may be used. A time span stored in the FIFO might be different from the number of stored entries if DTX (discontinuous transmission mode) is used. For that reason, the window size of the FIFO is limited in two ways. It may contain at most 500 entries (equals 10 seconds at 50 packets per second) and at most a time span (RTP time stamp difference between newest and oldest packet) of 10 seconds. If more entries are to be stored, the oldest entry is removed. For each RTP packet received on network, an entry will be added to the FIFO. An entry contains three values: delay, offset and RTP time stamp. These values are calculated from the receive time (for example, represented by the arrival time stamp) and RTP time stamp of the RTP packet, a shown in the pseudo code ofFIG.6.

As can be seen at reference numerals610and612, a time difference between RTP time stamps of two packets (for example, subsequent packets) is computed (yielding “rtpTimeDiff”) and a difference between receive time stamps of two packets (for example, subsequent packets) is computed (yielding “rcvTimeDiff”). Moreover, the RTP time stamp is converted from a time base of a transmitting device to a time base of the receiving device, as can be seen at reference numeral614, yielding “rtpTimeTicks”. Similarly, the RTP time differences (difference between RTP time stamps) are converted to a receiver time scale/time-base of the receiving device), as can be seen at reference numeral616, yielding “rtpTimeDiff”.

Subsequently, a delay information (“delay”) is updated on the basis of a previous delay information, as can be seen at reference numeral618. For example, if a receive time difference (i.e. a difference in times when packets have been received) is larger than a RTP time difference (i.e. a difference between times at which the packets have been sent out), it can be concluded that the delay has increased. Moreover, an offset time information (“offset”) is computed, as can be seen at reference numeral620, wherein the offset time information represents the difference between a receive time (i.e. a time at which a packet has been received) and a time at which a packet has been sent (as defined by the RTP time stamp, converted to the receiver time scale). Moreover, the delay information, the offset time information and a RTP time stamp information (converted to the receiver time scale) are added to the long term FIFO, as can be seen at reference numeral622.

Subsequently, some current information is stored as “previous” information for a next iteration, as can be seen at reference numeral624.

A long term jitter can be calculated as a difference between a maximum delay value currently stored in the FIFO and a minimum delay value:
longTermJitter=longTermFifo_getMaxDelay( )−longTermFifo_getMinDelay( );

Short Term Jitter Estimation

In the following, the short term jitter estimation will be described. The short term jitter estimation is done, for example, in two steps. In a first step, the same jitter calculation as done for long term estimation is used with the following modifications: the window size of the FIFO is limited to at most 50 entries and at most a time span of 1 second. The resulting jitter value is calculated as the difference between the 94% percentile delay value currently stored in the FIFO (the three highest values are ignored) and the minimum delay value:
shortTermJitterTmp=shortTermFifo1_getPercentileDelay(94)−shortTermFifo1_getMinDelay( );

In a second step, first the different offsets between the short term and long term FIFOs are compensated for this result:
shortTermJitterTmp+=shortTermFifo1_getMinOffset( );
shortTermJitterTmp−=longTermFifo_getMinOffset( );

This result is added to another FIFO with a window size of at most 200 entries and a time span of at most four seconds. Finally, the maximum value stored in the FIFO is increased to an integer multiplier of the frame size and used as short term jitter:
shortTermFifo2_add(shortTermJitterTmp);
shortTermJitter=ceil(shortTermFifo2_getMax( )/20.f)*20;

Target Delay Estimation by a Combination of Long/Short Term Jitter Estimation

To calculate the target delay (for example the target delay information472), the long term and short term jitter estimations (for example, as defined above as “longTermJitter” and “shortTermJitter”) are combined in different ways depending on the current state. For active signals (or signal portions, for which a comfort noise generation is not used), a range (for example, defined by “targetMin” and “targetMax”) is used as target delay. During DTX and for startup after DTX, two different values are calculated as target delay (for example, “targetDtx” and “targetStartUp”).

Details on how the different target delay values can be computed can be seen, for example, inFIG.7. As can be seen at reference numerals710and712, the values “targetMin” and “targetMax”, which assign a range for active signals, are computed on the basis of the short term jitter (“shortTermJitter”) and the long term jitter (“longTermJitter”). The computation of the target delay during DTX (“targetDtx”) is shown at reference numeral714, and the calculation of the target delay value for a startup (for example, after DTX) (“targetStartUp”) is shown at reference numeral716.

Playout Delay Estimation

In the following, the playout delay estimation, which may be performed by the playout delay estimator480, will be described. The playout delay specifies the buffering delay between the time when the previous frame was played and the time this frame could have been received if it had the lowest possible transmission delay on network compared to all frames currently contained in the history of the target delay estimation module. It is calculated in milliseconds using the following formula:
playoutDelay=prevPlayoutOffset−longTermFifo_getMinOffset( )+pcmBufferDelay;
The variable “prevPlayoutOffset” is recalculated whenever a received frame is popped from the de-jitter buffer module430using the current system time in milliseconds and the RTP time stamp of the frame converted to milliseconds:
prevPlayoutOffset=sysTime−rtpTimestamp

To avoid that “prevPlayoutOffset” will get outdated if a frame is not available, the variable is updated in case of frame-based time scaling. For frame-based time stretching, “prevPlayoutOffset” is increased by the duration of the frame, and for a frame-based time shrinking, “PrevPlayoutOffset” is decreased by the duration of the frame. The variable “pcmBufferDelay” describes the duration of time buffered in the PCM buffer module.

Control Logic

In the following, the control (for example, the control logic490) will be described in detail. However, it should be noted that the control logic800according toFIG.8may be supplemented by any of the features and functionalities described with respect to the jitter buffer control100and vice versa. Moreover, it should be noted that the control logic800may take the place of the control logic490according toFIG.4, but may optionally comprise additional features and functionalities. Moreover, it is not required that all of the features and functionalities described above with respect toFIG.4are also present in the control logic800according toFIG.8, and vice versa.

FIG.8shows a flow chart of a control logic800, which may naturally be implemented in hardware as well.

The control logic800comprises pulling810a frame for decoding. In other words, a frame is selected for decoding, and it is determined in the following how this decoding should be performed. In a check814, it is checked whether a previous frame (for example, a previous frame preceding the frame pulled for decoding in step810) was active or not. If it is found in the check814that the previous frame was inactive, a first decision path (branch)820is chosen which is used to adapt an inactive signal. In contrast, if it is found in the check814that the previous frame was active, a second decision path (branch)830is chosen, which is used to adapt an active signal. The first decision path820comprises determining a “gap” value in a step840, wherein the gap value describes a difference between a playout delay and a target delay. Moreover, the first decision path820comprises deciding850on a time scaling operation to be performed on the basis of the gap value. The second decision path830comprises selecting860a time scaling in dependence on whether an actual playout delay lies within a target delay interval.

In the following, additional details regarding the first decision path820and the second decision path830will be described.

In the step840of the first decision path820, a check842is performed whether a next frame is active. For example, the check842may check whether the frame pulled for decoding in the step810is active or not. Alternatively, the check842may check whether the frame following the frame pulled for decoding in the step810is active or not. If it is found, in the check842, that the next frame is not active, or that the next frame is not yet available, the variable “gap” is set, in a step844, as a difference between an actual playout delay (defined by a variable “playoutDelay”) and a DTX target delay (represented by variable “targetDtx”), is described above in the section “Target Delay Estimation”. In contrast, if it is found in the check840that the next frame is active, the variable “gap” is set to a difference between the playout delay (represented by the variable “playoutDelay”) and the startup target delay (as defined by the variable “targetStartUp”) in step846.

In the step850, it is first checked whether a magnitude of the variable “gap” is larger than (or equal to) a threshold. This is done in a check852. If it is found that the magnitude of the variable “gap” is smaller than (or equal to) the threshold value, no time scaling is performed. In contrast, if it is found in the check852that the magnitude of the variable “gap” is larger than the threshold (or equal to the threshold values, depending on the implementation), it is decided that a scaling is needed. In another check854, it is checked whether the value of the variable “gap” is positive or negative (i.e. if the variable “gap” is larger than zero or not). If it is found that the value of the variable “gap” is not larger than zero (i.e. negative) a frame is inserted into the de-jitter buffer (frame-based time stretching in step856), such that a frame-based time scaling is performed. This may, for example, be signaled by the frame-based scaling information434. In contrast, if it is found in the check854, that the value of the variable “gap” is larger than zero, i.e. positive, a frame is dropped from the de-jitter buffer (frame-based time shrinking in step856), such that a frame-based time scaling is performed. This may be signaled using the frame-based scaling information434.

In the following, the second decision branch860will be described. In a check862, it is checked whether the playout delay is larger than (or equal to) a maximum target value (i.e. an upper limit of a target interval) which is described, for example, by a variable “targetMax”). If it is found that the playout delay is larger than (or equal to) the maximum target value, a time shrinking is performed by the time scaler450(step866, sample-based time shrinking using the TSM), such that a sample-based time scaling is performed. This may be signaled, for example, by the sample-based scaling information444. However, if it is found in the check862that the playout delay is smaller than (or equal to) the maximum target delay, a check864is performed, in which it is checked whether the playout delay is smaller than (or equal to) a minimum target delay, which is described, for example, by the variable “targetMin”. If it is found that the playout delay is smaller than (or equal to) the minimum target delay, a time stretching is performed by the time scaler450(step866, sample-based time stretching using the TSM), such that a sample-based time scaling is performed. This may be signaled, for example, by the sample based scaling information444. However, if it is found in the check864that the playout delay is not smaller than (or equal to) the minimum target delay, no time scaling is performed.

To conclude, the control logic module (also designated as jitter buffer management control logic) shown inFIG.8compares the actual delay (playout delay) with the desired delay (target delay). In case of a significant difference, it triggers time scaling. During comfort noise (for example, when the SID-flag is active) frame-based time scaling will be triggered and executed by the de-jitter buffer module. During active periods, sample-based time scaling is triggered and executed by the TSM module.

FIG.12shows an example for target and playout delay estimation. An abscissa1210of the graphical representation1200describes a time, and ordinate1212of the graphical representation1200describes a delay in milliseconds. The “targetMin” and “targetMax” series create a range of delay desired by the target delay estimation module following the windowed network jitter. The playout delay “playoutDelay” typically stays within the range, but the adaptation might be slightly delayed because of the signal adaptive time scale modification.

FIG.13shows the time scale operations executed in theFIG.12trace. An abscissa1310of the graphical representation1300describes a time in seconds, and an ordinate1312describes a time scaling in milliseconds. Positive values indicate time stretching, negative values time shrinking in the graphical representation1300. During the burst, both buffers just get empty once, and one concealed frame is inserted for stretching (plus 20 milliseconds at 35 seconds). For all other adaptations, the higher quality sample-based time scaling method can be used which results in varying scales because of the signal adaptive approach.

To conclude, the target delay is dynamically adapted in response to an increase of the jitter (and also in response to a decrease of the jitter) over a certain window. When the target delay increases or decreases, a time scaling is typically performed, wherein a decision about the type of time scaling is made in a signal-adaptive manner. Provided that the current frame (or the previous frame) is active, a sample-based time scaling is performed, wherein the actual delay of the sample-based time scaling is adapted in a signal-adaptive manner in order to reduce artifacts. Accordingly, there is typically not a fixed amount of time scaling when sample-based time scaling is applied. However, when the jitter buffer runs empty, it is necessitated (or recommendable)—as an exceptional handling—to insert a concealed frame (which constitutes a frame-based time scaling) even though a previous frame (or a current frame) is active.

Time Scale Modification According toFIG.9

In the following, details regarding the time scale modification will be described taking reference toFIG.9. It should be noted that the time scale modification has been briefly described in section 5.4.3. However, the time scale modification, which may, for example, be performed by the time scaler150, will be described in more detail in the following.

FIG.9shows a flowchart of a modified WSOLA with quality control, according to an embodiment of the present invention. It should be noted that the time scaling900according toFIG.9may be supplemented by any of the features and functionalities described with respect to the time scaler200according toFIG.2and vice versa. Moreover, it should be noted that the time scaling900according toFIG.9may correspond to the sample based time scaler340according toFIG.3and to the time scaler450according toFIG.4. Moreover, the time scaling900according toFIG.9may take the place of sample-based time scaling866.

The time scaling (or time scaler, or time scaler modifier)900receives decoded (audio) samples910, for example, in a pulse-code-modulated (PCM) form. The decoded samples910may correspond to the decoded samples442, to the audio samples332or to the input audio signal210. Moreover, the time scaler900receives a control information912, which may, for example, correspond to the sample based scaling information444. The control information912may, for example, describe a target scale and/or a minimum frame size (for example, a minimum number of samples of a frame of audio samples448to be provided to the PCM buffer460). The time scaler900comprises a switch (or a selection)920, wherein it is decided, on the basis of the information about the target scale, whether a time shrinking should be performed, whether a time stretching should be performed or whether no time scaling should be performed. For example, the switching (or check, or selection)920may be based on the sample-based scaling information444received from the control logic490.

If it is found, on the basis of the target scale information, that no scaling should be performed, the received decoded samples910are forwarded in an unmodified form as an output of the time scaler900. For example, the decoded samples910are forwarded, in an unmodified form, to the PCM buffer460as the “time scaled” samples448.

In the following, a processing flow will be described for the case that a time shrinking is to be performed (which can be found, by the check920, on the basis of the target scale information912). In the case that a time shrinking is desired, an energy calculation930is performed. In this energy calculation930, an energy of a block of samples (for example, of a frame comprising a given number of samples) is calculated. Following the energy calculation930, a selection (or switching, or check)936is performed. If it is found that an energy value932provided by the energy calculation930is larger than (or equal to) an energy threshold value (for example, an energy threshold value Y), a first processing path940is chosen, which comprises a signal adaptive determination of an amount of time scaling within a sample-based time scaling. In contrast, if it is found that the energy value932provided by the energy calculation930is smaller than (or equal to) the threshold value (for example, the threshold value Y), a second processing path960is chosen, wherein a fixed amount of time shift is applied in a sample-based time scaling. In the first processing path940, in which an amount of time shift is determined in a signal adaptive manner, a similarity estimation942is performed on the basis of the audio samples. The similarity estimation942may consider a minimum frame size information944and may provide an information946about a highest similarity (or about a position of highest similarity). In other words, the similarity estimation942may determine which position (for example, which position of samples within a block of samples) is best suited for a time shrinking overlap-and-add operation. The information946about the highest similarity is forwarded to a quality control950, which computes or estimates whether an overlap-and-add operation using the information946about the highest similarity would result in an audio quality which is larger than (or equal to) a quality threshold value X (which may be constant or which may be variable). If it is found, by the quality control950, that a quality of an overlap-and-add operation (or equivalently, of a time scaled version of the input audio signal obtainable by the overlap-and-add operation) would be smaller than (or equal to) the quality threshold value X, a time scaling is omitted and unscaled audio samples are output by the time scaler900. In contrast, if it is found, by the quality control950, that the quality of an overlap-and-add operation using the information946about the highest similarity (or about the position of highest similarity) would be larger than or equal to the quality threshold value X, an overlap-and-add operation954is performed, wherein a shift, which is applied in the overlap-and-add operation, is described by the information946about the highest similarity (or about the position of the highest similarity). Accordingly, a scaled block (or frame) of audio samples is provided by the overlap-and-add operation.

The block (or frame) of time scaled audio samples956may, for example, correspond to the time scaled samples448. Similarly, a block (or frame) of unscaled audio samples952, which are provided if the quality control950finds that an obtainable quality would be smaller than or equal to the quality threshold value X, may also correspond to the “time scaled” samples448(wherein there is actually no time scaling in this case).

In contrast, if it is found in the selection936that the energy of a block (or frame) of input audio samples910is smaller than (or equal to) the energy threshold value Y, an overlap-and-add operation962is performed, wherein a shift, which is used in the overlap-and-add operation, is defined by the minimum frame size (described by a minimum frame size information), and wherein a block (or frame) of scaled audio samples964is obtained, which may correspond to the time scaled samples448.

Moreover, it should be noted that a processing, which is performed in the case of a time stretching, is analogous to a processing performed in the time shrinking with a modified similarity estimation and overlap-and-add.

To conclude, it should be noted that three different cases are distinguished in the signal adaptive sample-based time scaling when a time shrinking or a time stretching is selected. If an energy of a block (or frame) of input audio samples comprises a comparatively small energy (for example, smaller than (or equal to) the energy threshold value Y), a time shrinking or a time stretching overlap-and-add operation is performed with a fixed time shift (i.e. with a fixed amount of time shrinking or time stretching). In contrast, if the energy of the block (or frame) of input audio samples is larger than (or equal to) the energy threshold value Y, an “optimal” (also sometimes designated as “candidate” herein) amount of time shrinking or of time stretching is determined by the similarity estimation (similarity estimation942). In a subsequent quality control step, it is determined whether a sufficient quality would be obtained by such an overlap-and-add operation using the previously determined “optimal” amount of time shrinking or time stretching. If it is found that a sufficient quality could be reached, the overlap-and-add operation is performed using the determined “optimal” amount of time shrinking or time stretching. If, in contrast, it is found that a sufficient quality may not be reached using an overlap-and-add operation using the previously determined “optimal” amount of time shrinking or time stretching, the time shrinking or time stretching is omitted (or postponed to a later point in time, for example, to a later frame).

In the following, some further details regarding the quality adaptive time scaling, which may be performed by the time scaler900(or by the time scaler200, or by the time scaler340, or by the time scaler450), will be described. Time scaling methods using overlap-and-add (OLA) are widely available, but in general are not performing signal adaptive time scaling results. In the described solution, which can be used in the time scalers described herein, the amount of time scaling not only depends on the position extracted by the similarity estimation (for example, by the similarity estimation942), which seems optimal for a high quality time scaling, but also on an expected quality of the overlap-add (for example of the overlap-add 954). Therefore, two quality control steps are introduced in the time scaling module (for example, in the time scaler900, or in the other time scalers described herein), to decide whether the time scaling would result in audible artifacts. In case of potential artifacts, the time scaling is postponed up to a point in time where it would be less audible.

A first quality control step calculates an objective quality measure using the position p extracted by the similarity measure (for example, by the similarity estimation942) as input. In the case of a periodic signal, p will be the fundamental frequency of the current frame. The normalized cross correlation c( ) is calculated for the positions p, 2*p, 3/2*p, and 1/2*p. c(p) is expected to be a positive value and c(1/2*p) might be positive or negative. For harmonic signals, the sign of c(2p) should also be positive and the sign of c(3/2*p) should equal the sign of c(1/2*p). This relationship can be used to create an objective quality measure q:
q=c(p)*c(2*p)+c(3/2*p)*c(1/2*p).

The range of values for q is [−2; +2]. An ideal harmonic signal would result in q=2, while very dynamic and broadband signals which might create audible artifacts during time scaling will produce a lower value. Due to the fact that time scaling is done on a frame-by-frame basis, the whole signal to calculate c(2*p) and c(3/2*p) might not be available yet. However, the evaluation can also be done by looking at past samples. Therefore, c(−p) can be used instead of c(2*p), and similarly c(−1/2*p) can be used instead of c(3/2*p). A second quality control step compares the current value of the objective quality measure q with a dynamic minimum quality value qMin (which may correspond to the quality threshold value X) to determine if time-scaling should be applied to the current frame.

There are different intentions for having a dynamic minimum quality value: if q has a low value because the signal is evaluated as bad to scale over a long period, qMin should be reduced slowly to make sure that the expected scaling is still executed at some point in time with a lower expected quality. On the other hand, signals with a high value for q should not result in scaling many frames in a row which would reduce the quality regarding long-term signal characteristics (e.g. rhythm).

Therefore, the following formula is used to calculate the dynamic minimum quality qMin (which may, for example, be equivalent to the quality threshold value X):
qMin=qMinInitial−(nNotScaled*0.1)+(nScaled*0.2)
qMinInitial is a configuration value to optimize between a certain quality and the delay until a frame can be scaled with the requested quality, of which a value of 1 is a good compromise. nNotScaled is a counter of frames which have not been scaled because of insufficient quality (q<qMin). nScaled counts the number of frames which have been scaled because the quality requirement was reached (q>=qMin). The range of both counters is limited: they will not be decreased to negative values and will not be increased above a designated value which is set to be 4 by default (for example).

The current frame will be time-scaled by the position p if q>=qMin, otherwise time-scaling will be postponed to a following frame where this condition is met. The pseudo code ofFIG.11illustrates the quality control for time scaling.

As can be seen, the initial value for qMin is set to 1, wherein said initial value is designated with “qMinInitial” (confer reference numeral1110). Similarly, a maximum counter value of nScaled (designated as “variable qualityRise”) is initialized to 4, as can be seen at reference numeral1112. A maximum value of counter nNotScaled is initialized to 4 (variable “qualityRed”), confer reference numeral1114. Subsequently, a position information p is extracted by a similarity measure, as can be seen at reference numeral1116. Subsequently, a quality value q is computed for the position described by the position value p in accordance with the equation which can be seen at reference numeral1116. A quality threshold value qMin is computed in dependence on the variable qMinInitial, and also in dependence on the counter values nNotScaled and nScaled, as can be seen at reference numeral1118. As can be seen, the initial value qMinInitial for the quality threshold value qMin is reduced by a value which is proportional to the value of the counter nNotScaled, and increased by a value which is proportional to the value nScaled. As can be seen, maximum values for the counter values nNotScaled and nScaled also determine a maximum increase of the quality threshold value qMin and a maximum decrease of the quality threshold value qMin. Subsequently, a check is performed whether the quality value q is larger than or equal to the quality threshold value qMin, a can be seen at reference numeral1120.

If this is the case, an overlap-add operation is executed, as can be seen at reference numeral1122. Moreover, the counter variable nNotScaled is reduced, wherein it is ensured that said counter variable does not get negative. Moreover, the counter variable nScaled is increased, wherein it is ensured that nScaled does not exceed the upper limit defined by the variable (or constant) qualityRise. An adaptation of the counter variables can be seen at reference numerals1124and1126.

In contrast, if it is found in the comparison shown at reference numeral1120that the quality value q is smaller than the quality threshold qMin, an execution of the overlap-and-add operation is omitted, the counter variable nNotScaled is increased, taking into account that the counter variable nNotScaled does not exceed a threshold defined by the variable (or constant) qualityRed, and the counter variable nScaled is reduced, taking into account that the counter variable nScaled does not become negative. The adaptation of the counter variables for the case that the quality is insufficient is shown at reference numerals1128and1130.

Time Scaler According toFIGS.10aand10b

In the following, a signal adaptive time scaler will be explained taking reference toFIGS.10and10b.FIGS.10and10bshow a flow chart of a signal adaptive time scaling. It should be noted that the signal adaptive time scaling, as shown inFIGS.10aand10bmay, for example, be applied in the time scaler200, in the time scaler340, in the time scaler450or in the time scaler900.

The time scaler1000according toFIGS.10aand10b, comprises an energy calculation1010, wherein an energy of a frame (or a portion, or a block) of audio samples is computed. For example, the energy calculation1010may correspond to the energy calculation930. Subsequently, a check1014is performed, wherein it is checked whether the energy value obtained in the energy calculation1010is larger than (or equal to) an energy threshold value (which may, for example, be a fixed energy threshold value). It is found, in the check1014, that the energy value obtained in the energy calculation1010is smaller than (or equal to) the energy threshold value, it may be assumed that a sufficient quality can be obtained by an overlap-add operation, and the overlap-and-add operation is performed with a maximum time shift (to thereby obtain a maximum time scaling) in a step1018. In contrast, if it is found in the check1014that the energy value obtained in the energy calculation1010is not smaller than (or equal to) the energy threshold value, a search for a best match of a template segment within a search region is performed using a similarity measure. For example, the similarity measure may be a cross correlation, a normalized cross correlation, an average magnitude difference function or a sum of squared errors. In the following, some details regarding this search for a best match will be described, and it will also be explained how a time stretching or a time shrinking can be obtained.

Reference is now made to a graphic representation at reference numeral1040. A first representation1042shows a block (or frame) of samples which starts at time t1and which ends at time t2. As can be seen, the block of samples which starts t1and which ends at time t2can be split up logically into a first block of samples, which starts at time t1and which ends at time t3and a second block of samples which starts at time t4and which ends at time t2. However, the second block of samples is then time shifted with respect to the first block of samples, which can be seen at reference numeral1044. For example, as a result of a first time shift, the time shifted second block of samples starts at time t4′ and ends at time t2′. Accordingly, there is a temporal overlap between the first block of samples and the time shifted second block of samples between times t4′ and t3. However, as can be seen, there is no good match (i.e. no high similarity) between the first block of samples and the time shifted version of the second block of samples, for example, in the overlap region between times t4′ and t3(or within a portion of said overlap region between times t4′ and t3). In other words, the time scaler may, for example, time shift the second block of samples, as shown at reference numeral1044, and determine a measure of similarity for the overlap region (or for a part of the overlap region) between times t4′ and t3. Moreover, the time scaler may also apply an additional time shift to the second block of samples, as shown at reference numeral1046, such that the (twice) time shifted version of the second block of samples starts at time t4″ and ends at time t2″ (with t2″>t2′>t2and similarly t4″>t4′>t4). The time scaler may also determine a (quantitative) similarity information representing a similarity between the first block of samples and the twice shifted version of the second block of samples, for example, between times t4″ and t3(or, for example, within a portion between times t4″ and t3). Accordingly, the time scaler evaluates for which time shift of the time shifted version of the second block of samples the similarity, in the overlap region with the first block of samples, is maximized (or at last larger than a threshold value). Accordingly, a time shift can be determined which results in a “best match” in that the similarity between the first block of samples and the time shifted version of the second block of samples is maximized (or at least sufficiently large). Accordingly, if there is a sufficient similarity between the first block of samples and the twice time shifted version of the second block of samples within the temporal overlap region (for example between times t4″ and t3), it can be expected, with a reliability determined by the used measure of similarity, that an overlap-and-add operation overlapping and adding the first block of samples and the twice time shifted version of the second block of samples results in an audio signal without substantial audible artifacts. Moreover, it should be noted that an overlap-and-add between the first block of samples and the twice time shifted version of the second block of samples results in an audio signal portion which has a temporal extension between times t1and t2″, which is longer than the “original” audio signal, which extends from time t1to time t2. Accordingly, a time stretching can be achieved by overlapping and adding the first block of samples and the twice time shifted version of the second block of samples.

Similarly, a time shrinking can be achieved, as will be explained taking reference to the graphical representation at reference numeral1050. As can be seen at reference numeral1052, there is an original block (or frame) of samples, which extends between times t11and t12. The original block (or frame) of samples can be divided, for example into a first block of samples which extends from time t11to time t13and a second block of samples which extends from time t13to time t12. The second block of samples is time shifted to the left, as can be seen at reference numeral1054. Consequently, the (once) time shifted version of the second block of samples starts at time t13′ and ends at time t12′. Also, there is a temporal overlap between the first block of samples and the once time shifted version of the second block of samples between times t13′ and t13. However, the time scaler may determine a (quantitative) similarity information representing a similarity of the first block of samples and of the (once) time shifted version of the second block of samples between times t13′ and t13(or for a portion of the time between times t13′ and t13) and find out that the similarity is not particularly good. Furthermore, the time scaler may further time shift the second block of samples, to thereby obtain a twice time shifted version of the second blocks of samples, which is shown at reference numeral1056, and which starts at time t13″ and ends at time t12″. Thus there is an overlap between the first block of samples and the (twice) time shifted version of the second block of samples between times t13″ and t13. It may be found, by the time scaler, that a (quantitative) similarity information indicates a high similarity between the first block of samples and the twice time shifted version of the second block of samples between times t13″ and t13. Accordingly, it may be concluded, by the time scaler, that an overlap-and-add operation can be performed with good quality and less audible artifacts between the first block of samples and the twice time shifted version of the second block of samples (at least with the reliability provided by the similarity measure used). Moreover, a three times time shifted version of the second block of samples, which is shown at reference numeral1058may also be considered. The three times time shifted version of the second block of samples may start at time t13′″ and end as time t12′″. However, the three times time shifted version of the second block of samples may not comprise a good similarity with the first block of samples in the overlap region between times t13′″ and t13, because the time shift was not appropriate. Consequently, the time scaler may find that the twice time shifted version of the second block of samples comprises a best match (best similarity in the overlap region, and/or in an environment of the overlap region, and/or in a portion of the overlap region) with the first block of samples. Accordingly, the time scaler may perform the overlap-and-add of the first block of samples and of the twice time shifted version of the second block of samples, provided an additional quality check (which may rely on a second, more meaningful similarity measure) indicates a sufficient quality. As a result of the overlap-and-add operation, a combined block of samples is obtained, which extends from time t11to time t12″, and which is temporally shorter than the original block of samples from time t11to time t12. Accordingly, a time shrinking can be performed.

It should be noted that the above functionalities, which have been described taking reference to the graphical representations at reference numerals1040and1050, may be performed by the search1030, wherein an information about the position of highest similarity is provided as a result of the search for a best match (wherein the information or value describing the position of the highest similarity is also designated with p herein). The similarity between the first block of samples and the time shifted version of the second block of samples within the respective overlap regions may be determined using a cross correlation, using a normalized cross correlation, using an average magnitude difference function or using a sum of squared errors.

Once the information about the position of highest similarity (p) is determined, a calculation1060of a matching quality for the identified position (p) of highest similarity is performed. This calculation may be performed, for example, as shown at reference numeral1116inFIG.11. In other words, the (quantitative) information about the matching quality (which may, for example, be designated with q) may be calculated using the combination of four correlation values, which may be obtained for different time shifts (for example, time shifts p, 2*p, 3/2*p and 1/2*p). Accordingly, the (quantitative) information (q) representing the matching quality can be obtained.

Taking reference now toFIG.10ba check1064is performed, in which the quantitative information q describing the matching quality is compared with a quality threshold value qMin. This check or comparison1064may evaluate whether the matching quality, represented by a variable q, is larger than (or equal to) the variable quality threshold value qMin. If it is found in the check1064that the matching quality is sufficient (i.e. larger than or equal to the variable quality threshold value), an overlap-add operation is applied (step1068) using the position of highest similarity (which is described, for example, by the variable p). Accordingly, an overlap-and-add operation is performed, for example, between the first block of samples and the time shifted version of the second block of samples which results in a “best match” (i.e. in a highest value of a similarity information). For details, reference is made, for example, to the explanations made with respect to the graphic representation1040and1050. The application of the overlap-and-add is also shown at reference numeral1122inFIG.11. Moreover, an update of a frame counter is performed in step1072. For example, a counter variable “nNotScaled” and a counter variable “nScaled”, are updated, for example as described with reference toFIG.11at reference numerals1124and1126. In contrast, if it is found in the check1064that the matching quality is insufficient (for example, smaller than (or equal to) the variable quality threshold value qmin), the overlap-and-add operation is avoided (for example, postponed), which is indicated at reference numeral1076. In this case, the frame counters are also updated, as shown in step1080. The updating of the frame counters may be performed, for example, as shown at reference numerals1128and1130inFIG.11. Moreover, the time scaler described with reference toFIGS.10aand10bmay also compute the variable quality threshold value qMin, which is shown at reference numeral1084. The computation of the variable quality threshold value qMin may be performed, for example, as shown at reference numeral1118inFIG.11.

To conclude, the time scaler1000, the functionality of which has been described taking reference toFIGS.10aand10bin the form of a flow chart, may perform a sample-based time scaling using a quality control mechanism (steps1060to1084).

Method According toFIG.14

FIG.14shows a flow chart of a method for controlling a provision of a decoded audio content on the basis of an input audio content. The method1400according toFIG.14comprises selecting1410a frame-based time scaling or a sample-based time scaling in a signal-adaptive manner.

In addition, it should be noted that the method1400can be supplemented by any of the features and functionalities described herein, for example, with respect to the jitter buffer control.

Method According toFIG.15

FIG.15shows a block schematic diagram of a method1500for providing a time scaled version of an input audio signal. The method comprises computing or estimating1510a quality of a time-scaled version of the input audio signal obtainable by a time scaling of the input audio signal. Moreover, the method1500comprises performing1520the time scaling of the input audio signal in dependence on the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling.

The method1500can be supplemented by any of the features and functionalities described herein, for example, with reference to the time scaler.

CONCLUSIONS

To conclude, embodiments according to the invention create a jitter buffer management method and apparatus for high quality speech and audio communication. The method and the apparatus can be used together with communication codecs, such as MPEG ELD, AMR-WB, or future codecs. In other words, embodiments according to the invention create a method and apparatus for compensation of inter-arrival jitter in packet-based communication.

Embodiments of the invention can be applied, for example, in the technology called “3GPP EVS”.

In the following, some aspects of embodiments according to the invention will be described briefly.

The jitter buffer management solution described herein creates a system, wherein a number of described modules are available and are combined in the manner described above. Moreover, it should be noted that aspects of the invention also relate to features of the modules themselves.

An important aspect of the present invention is a signal adaptive selection of a time scaling method for adaptive jitter buffer management. The described solution combines frame-based time scaling and sample-based time scaling in the control logic so that the advantages of both methods are combined. Available time scaling methods are:Comfort noise insertion/deletion in DTXOverlap-and-add (OLA) without correlation in low signal energy (for example, for frames having low signal energy);WSOLA for active signals;Insertion of concealed frame for stretching in case of empty jitter buffer.

The solution described herein describes a mechanism to combine frame-based methods (comfort noise insertion and deletion, and insertion of concealed frames for stretching) with sample-based methods (WSOLA for active signals, and unsynchronized overlap-add (OLA) for low-energy signals). InFIG.8, the control logic is illustrated that selects the optimum technology for time-scale modification according to an embodiment of the invention.

According to a further aspect described herein, multiple targets for adaptive jitter buffer management are used. In the described solution, the target delay estimation employs different optimization criteria for calculating a single target playout delay. Those criteria result in different targets at first, optimized for high quality or low delay.

The multiple targets for calculating the target playout delay are:Quality: avoid late-loss (evaluates jitter);Delay: limit delay (evaluates jitter).

It is an (optional) aspect of the described solution to optimize the target delay estimation so that the delay is limited but also late-losses are avoided and furthermore a small reserve in the jitter buffer is kept to increase the probability of interpolation to enable high quality error concealment for the decoder.

Another (optional) aspect relates to TCX concealment recovery with late frames. Frames that arrive late are discarded by most jitter buffer management solutions to date. Mechanisms have been described to use late frames in ACELP-based decoders [Lef03]. According to an aspect, such a mechanism is also used for frames other than ACELP frames, e.g. frequency domain coded frames like TCX, to aid in recovery of the decoder state in general. Therefore, frames that are received late and already concealed are still fed to the decoder to improve recovery of the decoder state.

Another important aspect according to the present invention is the quality-adaptive time scaling, which was described above.

To further conclude, embodiments according to the present invention create a complete jitter buffer management solution that can be used for improved user experience in packet-based communications. It was an observation that the presented solutions perform superior than any other known jitter buffer management solution known to the inventors.

Implementation Alternatives

Although some aspects have been described in the context of an apparatus, it is clear that these aspects also represent a description of the corresponding method, where a block or device corresponds to a method step or a feature of a method step. Analogously, aspects described in the context of a method step also represent a description of a corresponding block or item or feature of a corresponding apparatus. Some or all of the method steps may be executed by (or using) a hardware apparatus, like for example, a microprocessor, a programmable computer or an electronic circuit. In some embodiments, some one or more of the most important method steps may be executed by such an apparatus.

The inventive encoded audio signal can be stored on a digital storage medium or can be transmitted on a transmission medium such as a wireless transmission medium or a wired transmission medium such as the Internet.

Depending on certain implementation requirements, embodiments of the invention can be implemented in hardware or in software. The implementation can be performed using a digital storage medium, for example a floppy disk, a DVD, a Blu-Ray, a CD, a ROM, a PROM, an EPROM, an EEPROM or a FLASH memory, having electronically readable control signals stored thereon, which cooperate (or are capable of cooperating) with a programmable computer system such that the respective method is performed. Therefore, the digital storage medium may be computer readable.

Some embodiments according to the invention comprise a data carrier having electronically readable control signals, which are capable of cooperating with a programmable computer system, such that one of the methods described herein is performed.

Generally, embodiments of the present invention can be implemented as a computer program product with a program code, the program code being operative for performing one of the methods when the computer program product runs on a computer. The program code may for example be stored on a machine readable carrier.

Other embodiments comprise the computer program for performing one of the methods described herein, stored on a machine readable carrier.

In other words, an embodiment of the inventive method is, therefore, a computer program having a program code for performing one of the methods described herein, when the computer program runs on a computer.

A further embodiment of the inventive methods is, therefore, a data carrier (or a digital storage medium, or a computer-readable medium) comprising, recorded thereon, the computer program for performing one of the methods described herein. The data carrier, the digital storage medium or the recorded medium are typically tangible and/or non-transitionary.

A further embodiment of the inventive method is, therefore, a data stream or a sequence of signals representing the computer program for performing one of the methods described herein. The data stream or the sequence of signals may for example be configured to be transferred via a data communication connection, for example via the Internet.

A further embodiment comprises a processing means, for example a computer, or a programmable logic device, configured to or adapted to perform one of the methods described herein.

A further embodiment comprises a computer having installed thereon the computer program for performing one of the methods described herein.

A further embodiment according to the invention comprises an apparatus or a system configured to transfer (for example, electronically or optically) a computer program for performing one of the methods described herein to a receiver. The receiver may, for example, be a computer, a mobile device, a memory device or the like. The apparatus or system may, for example, comprise a file server for transferring the computer program to the receiver.

In some embodiments, a programmable logic device (for example a field programmable gate array) may be used to perform some or all of the functionalities of the methods described herein. In some embodiments, a field programmable gate array may cooperate with a microprocessor in order to perform one of the methods described herein. Generally, the methods may be performed by any hardware apparatus.

The apparatus described herein may be implemented using a hardware apparatus, or using a computer, or using a combination of a hardware apparatus and a computer.

The methods described herein may be performed using a hardware apparatus, or using a computer, or using a combination of a hardware apparatus and a computer.

While this invention has been described in terms of several embodiments, there are alterations, permutations, and equivalents which will be apparent to others skilled in the art and which fall within the scope of this invention. It should also be noted that there are many alternative ways of implementing the methods and compositions of the present invention. It is therefore intended that the following appended claims be interpreted as including all such alterations, permutations, and equivalents as fall within the true spirit and scope of the present invention.

REFERENCES

[Lia01] Y. J. Liang, N. Faerber, B. Girod: “Adaptive playout scheduling using time-scale modification in packet voice communications”, 2001[Lef03] P. Gournay, F. Rousseau, R. Lefebvre: “Improved packet loss recovery using late frames for prediction-based speech coders”, 2003

","Stefan Reuschl, Stefan Doehla, Jérémie Lecomte, Manuel Jander, Nikolaus Faerber","Technology, Media, Healthcare, Artificial Intelligence & Machine Learning","Speech recognition, audio coding, voice processing, speech synthesis, text-to-speech systems",A time scaler for providing a time scaled version of an input audio signal is configured to compute or estimate a quality of a time scaled version of the input audio signal obtainable by a time scaling of the input audio signal. The time scaler is configured to perform the time scaling of the input audio signal in dependence on the computation or estimation of the quality of the time scaled version of the input audio signal obtainable by the time scaling. An audio decoder has such a time scaler.
12020722,"2. BACKGROUND OF THE INVENTION

Certain embodiments of the present invention are directed to signal processing. More particularly, some embodiments of the invention provide systems and methods for processing and presenting conversations. Merely by way of example, some embodiments of the invention have been applied to conversations captured in audio form. But it would be recognized that the invention has a much broader range of applicability.

Conversations, such as human-to-human conversations, include information that is often difficult to comprehensively, efficiently, and accurately extract, using conventional methods and systems. For example, conventional note-taking performed during a conversation not only distracts the note-taker from the conversation but can also lead to inaccurate recordation of information due to human-error, such as for human's inability to multitask well and process information efficiently with high accuracy in real time.

Hence it is highly desirable to provide systems and methods for processing and presenting conversations (e.g., in an automatic manner) to increase the value of conversations, such as human-to-human conversations, at least by increasing the comprehensiveness and accuracy of information extractable from the conversations.

3. BRIEF SUMMARY OF THE INVENTION

Certain embodiments of the present invention are directed to signal processing. More particularly, some embodiments of the invention provide systems and methods for processing and presenting conversations. Merely by way of example, some embodiments of the invention have been applied to conversations captured in audio form. But it would be recognized that the invention has a much broader range of applicability.

According to some embodiments, a system for processing and presenting a conversation includes a sensor, a processor, and a presenter. The sensor is configured to capture an audio-form conversation. The processor is configured to automatically transform the audio-form conversation into a transformed conversation. The transformed conversation includes a synchronized text, wherein the synchronized text is synchronized with the audio-form conversation. The presenter is configured to present the transformed conversation including the synchronized text and the audio-form conversation. The presenter is further configured to present the transformed conversation to be navigable, searchable, assignable, editable, and shareable.

According to some embodiments, a computer-implemented method for processing and presenting a conversation includes receiving an audio-form conversation; automatically transforming the audio-form conversation into a transformed conversation, the transformed conversation including a synchronized text, the synchronized text being synchronized with the audio-form conversation; and presenting the transformed conversation including the synchronized text and the audio-form conversation. The presenting the transformed conversation including the synchronized text and the audio-form conversation includes presenting the transformed conversation to be navigable, searchable, assignable, editable, and shareable.

According to some embodiments, a non-transitory computer-readable medium with instructions stored thereon, that when executed by a processor, perform the processes including: receiving an audio-form conversation; automatically transforming the audio-form conversation into a transformed conversation, the transformed conversation including a synchronized text, the synchronized text being synchronized with the audio-form conversation; and presenting the transformed conversation including the synchronized text and the audio-form conversation. The presenting the transformed conversation including the synchronized text and the audio-form conversation includes presenting the transformed conversation to be navigable, searchable, assignable, editable, and shareable.

According to various embodiments, a system for presenting a conversation includes a sensor configured to capture an audio-form conversation and send the captured audio-form conversation to a processor. The processor is configured to automatically transform the audio-form conversation into a transformed conversation. The transformed conversation includes a synchronized text. The synchronized text is synchronized with the audio-form conversation. The system further includes a presenter configured to receive the transformed conversation from the processor and present the transformed conversation including the synchronized text and the audio-form conversation. The presenter is further configured to present the transformed conversation to be navigable, searchable, assignable, editable, and shareable.

According to certain embodiments, a computer-implemented method for processing and presenting a conversation includes receiving an audio-form conversation; sending the received audio-form conversation to automatically transform the audio-form conversation into a transformed conversation, wherein the transformed conversation includes a synchronized text, that is synchronized with the audio-form conversation; receiving the transformed conversation; and presenting the transformed conversation including the synchronized text and the audio-form conversation, wherein the presenting the transformed conversation including the synchronized text and the audio-form conversation includes presenting the transformed conversation to be navigable, searchable, assignable, editable, and shareable.

According to certain examples, a non-transitory computer-readable medium with instructions stored thereon, that when executed by a processor, perform the processes including receiving an audio-form conversation; sending the received audio-form conversation to automatically transform the audio-form conversation into a transformed conversation, the transformed conversation including a synchronized text, the synchronized text being synchronized with the audio-form conversation; receiving the transformed conversation; and presenting the transformed conversation including the synchronized text and the audio-form conversation, wherein the presenting the transformed conversation including the synchronized text and the audio-form conversation includes presenting the transformed conversation to be navigable, searchable, assignable, editable, and shareable.

According to some examples, a system for transforming a conversation includes a processor configured to receive from a sensor a captured audio-form conversation; automatically transform the audio-form conversation into a transformed conversation, the transformed conversation including a synchronized text, the synchronized text being synchronized with the audio-form conversation; and send the transformed conversation to a presenter configured to present the transformed conversation including the synchronized text and the audio-form conversation, wherein the processor is further configured to send the transformed conversation to the presenter further configured to present the transformed conversation to be navigable, searchable, assignable, editable, and shareable.

According to certain examples, a computer-implemented method for transforming a conversation includes receiving an audio-form conversation; automatically transforming the audio-form conversation into a transformed conversation, the transformed conversation including a synchronized text, the synchronized text being synchronized with the audio-form conversation; and sending the transformed conversation to present the transformed conversation including the synchronized text and the audio-form conversation, wherein the sending the transformed conversation to present the transformed conversation including the synchronized text and the audio-form conversation includes sending the transformed conversation to present the transformed conversation to be navigable, searchable, assignable, editable, and shareable.

According to various examples, a non-transitory computer-readable medium with instructions stored thereon, that when executed by a processor, perform the processes including receiving an audio-form conversation; automatically transforming the audio-form conversation into a transformed conversation, the transformed conversation including a synchronized text, the synchronized text being synchronized with the audio-form conversation; and sending the transformed conversation to present the transformed conversation including the synchronized text and the audio-form conversation, wherein the sending the transformed conversation to present the transformed conversation including the synchronized text and the audio-form conversation includes sending the transformed conversation to present the transformed conversation to be navigable, searchable, assignable, editable, and shareable.

Depending upon embodiment, one or more benefits may be achieved. These benefits and various additional objects, features and advantages of the present invention can be fully appreciated with reference to the detailed description and accompanying drawings that follow.

",utility,2024-06-25,Systems and methods for processing and presenting conversations,B2,14.0,,"Otter.ai, Inc.","G10L 21/0, G06F 16/438, G10L 17/2, G10L 17/4, G10L 17/22, G10L 21/10, H04L 9/40","G10L15/26,G10L21/10,G06F16/438,G06F16/685,G10L17/02,G10L17/04,G10L17/22,H04L12/1822,H04L51/066,H04L63/104,H04M3/567,G10L17/00,H04L12/1831,H04M3/42127,H04M3/42391,H04M2201/22,H04M2201/38,H04M2201/40","inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,additional,additional,additional,additional,additional,additional,additional","Information retrieval; Database structures therefor; File system structures therefor-of multimedia data, e.g. slideshows comprising image and additional audio data -Querying-Presentation of query results, Information retrieval; Database structures therefor; File system structures therefor-of audio data-Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually-using metadata automatically derived from the content-using automatically derived transcript of audio data, e.g. lyrics , Speech recognition -Speech to text systems , Speaker identification or verification techniques, Speaker identification or verification techniques-Preprocessing operations, e.g. segment selection; Pattern representation or modelling, e.g. based on linear discriminant analysis [LDA] or principal components; Feature selection or extraction, Speaker identification or verification techniques-Training, enrolment or model building, Speaker identification or verification techniques-Interactive procedures; Man-machine interfaces, Speech or voice signal processing techniques to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility -Transformation of speech into a non-audible representation, e.g. speech visualisation or speech processing for tactile aids -Transforming into visible information, Data switching networks -Details-Arrangements for providing special services to substations-for broadcast or conference ; , e.g. multicast-for computer conferences, e.g. chat rooms -Conducting the conference, e.g. admission, detection, selection or grouping of participants, correlating users to one or more conference sessions, prioritising transmission, Data switching networks -Details-Arrangements for providing special services to substations-for broadcast or conference ; , e.g. multicast-for computer conferences, e.g. chat rooms -Tracking arrangements for later retrieval, e.g. recording contents, participants activities or behavior, network status, User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail-Message adaptation to terminal or network requirements-Format adaptation, e.g. format conversion or compression, Network architectures or network communication protocols for network security -for controlling access to devices or network resources-Grouping of entities, Electronic components, circuits, software, systems or apparatus used in telephone systems-Synchronisation circuits, Electronic components, circuits, software, systems or apparatus used in telephone systems-Displays, Electronic components, circuits, software, systems or apparatus used in telephone systems-using speech recognition , Automatic or semi-automatic exchanges-Systems providing special services or facilities to subscribers -Systems providing several special services or facilities from groups H04M3/42008 - H04M3/58, Automatic or semi-automatic exchanges-Systems providing special services or facilities to subscribers -where the subscribers are hearing-impaired persons, e.g. telephone devices for the deaf, Automatic or semi-automatic exchanges-Systems providing special services or facilities to subscribers -Arrangements for connecting several subscribers to a common circuit, i.e. affording conference facilities -Multimedia conference systems","COMPUTING; CALCULATING OR COUNTING, MUSICAL INSTRUMENTS; ACOUSTICS, ELECTRIC COMMUNICATION TECHNIQUE","ELECTRIC DIGITAL DATA PROCESSING , SPEECH ANALYSIS TECHNIQUES OR SPEECH SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING TECHNIQUES; SPEECH OR AUDIO CODING OR DECODING, TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION , TELEPHONIC COMMUNICATION ",1.0,109.0,"5. DETAILED DESCRIPTION OF THE INVENTION

Certain embodiments of the present invention are directed to signal processing. More particularly, some embodiments of the invention provide systems and methods for processing and presenting conversations. Merely by way of example, some embodiments of the invention have been applied to conversations captured in audio form. But it would be recognized that the invention has a much broader range of applicability.

FIG.1is a simplified diagram showing a system for processing and presenting one or more conversations, according to some embodiments of the present invention;FIG.2is a simplified diagram showing a method for processing and presenting one or more conversations, according to some embodiments of the present invention;FIG.3is a simplified diagram showing a process for automatically transforming one or more conversations as shown inFIG.2, according to some embodiments of the present invention;FIGS.4-101are simplified diagrams showing a user interface and/or a presenter related toFIG.1,FIG.2,FIG.3, and/orFIG.102, according to some embodiments of the present invention; andFIG.102is a simplified diagram showing a process for presenting one or more transformed conversations as shown inFIG.2, according to some embodiments of the present invention. These diagrams are merely examples, which should not unduly limit the scope of the claims. One of ordinary skill in the art would recognize many variations, alternatives, and modifications.

FIG.1is a simplified diagram showing a system100for processing and presenting one or more conversations according to some embodiments of the present invention. This diagram is merely an example, which should not unduly limit the scope of the claims. One of ordinary skill in the art would recognize many variations, alternatives, and modifications. The system100includes a controller102, an interface104, a sensor106, a processor108, and a presenter110. In some examples, the presenter110includes a mobile device, a web browser, a computer, a watch, a phone, a tablet, a robot, a projector, a television, and/or a display. In certain examples, the presenter110includes part of a mobile device, part of a web browser, part of a computer, part of a watch, part of a phone, part of a tablet, part of a robot, part of a projector, part of a television, and/or part of a display. Although the above has been shown using a selected group of components for the system, there can be many alternatives, modifications, and variations. For example, some of the components may be expanded and/or combined. Other components may be inserted to those noted above. Depending upon the embodiment, the arrangement of components may be interchanged with others replaced. Further details of these components are found throughout the present specification.

In some embodiments, the controller102is configured to receive and/or send one or more instructions to other components of the system100. For example, the controller102is configured to receive a first instruction from the interface104and send a second instruction to the sensor106. In some examples, the controller102is or is part of a computing device (e.g., a computer, a phone, a laptop, a tablet, a watch, a television, a recording device, and/or a robot). In some embodiments, the controller includes hardware (e.g., a processor, a memory, a transmitter, a receiver, and/or software) for receiving, transmitting, and/or transforming instructions.

According to some embodiments, the interface104includes a user interface and/or is configured to receive a user instruction from a user of the system100, and send a system instruction to one or more other components of the system100(e.g., the controller102). For example, the interface includes a touchscreen, a button, a keyboard, a dialer (e.g., with number pad), an audio receiver, a gesture receiver, an application such as Otter for IOS or Android, and/or a webpage. In another example, the user is a human or another hardware and/or software system. In some embodiments, the interface104is configured to receive a first start instruction (e.g., when a user taps a start-record button in a mobile application) and to send a second start instruction to the controller102which in turn sends a third start instruction to, for example, the sensor106. In some embodiments, the interface104is controlled by the controller102to provide one or more selectable actions (e.g., by the user). For example, the controller102controls the interface104to display a search bar and/or a record button for receiving instructions such as user instructions. In some embodiments, the interface104is communicatively coupled to the controller102and/or structurally contained or included in a common device (e.g., a phone).

In some embodiments, the sensor106is configured to receive an instruction and sense, receive, collect, detect, and/or capture a conversation in audio form (e.g., an audio file and/or an audio signal). For example, the sensor106includes an audio sensor and is configured to capture a conversation in audio form, such as to record a conversation (e.g., a human-to-human conversation). In some examples, the audio sensor is a microphone, which is included as part of a device (e.g., a mobile phone) and/or a separate component coupled to the device (e.g., the mobile phone), and the device (e.g., the mobile phone) includes one or more components of the system100(e.g., controller102). In some examples, the human-to-human conversation captured by the sensor106is sent (e.g., transmitted) to other components of the system100. For example, the audio-form conversation captured by the sensor106(e.g., the audio recorded by the sensor106) is sent to the processor108of the system100. In some embodiments, the sensor106is communicatively coupled to the controller such that the sensor is configured to send a status signal (e.g., a feedback signal) to the controller to indicate whether the sensor is on (e.g., recording or capturing) or off (e.g., not recording or not capturing).

According to some embodiments, the processor108is configured to receive input including data, signal, and/or information from other components of the system100, and to process, transform, transcribe, extract, and/or summarize the received input (e.g., audio recording). In some examples, the processor108is further configured to send, transmit, and/or present the processed output (e.g., transformed conversation). For example, the processor108is configured to receive the captured audio-form conversation (e.g., the audio recorded by the sensor106) from the sensor106. As an example, the processor108is configured to receive the conversation in audio form (e.g., an audio file and/or an audio signal) from the sensor106. In some examples, the processor108is configured to be controlled by the controller102, such as to process the data, signal, and/or information transmitted by the sensor106, when an instruction sent from the controller102is received by the processor108. In some embodiments, the processor108includes an automated speech recognition system (ASR) that is configured to automatically transform and/or transcribe a conversation (e.g., a captured conversation sent from the sensor106), such as transforming the conversation from audio recording to synchronized transcription.

In some embodiments, the processor108is communicatively coupled to the controller102such that the processor108is configured to send a status signal (e.g., a feedback signal) to the controller102to indicate whether the processor108is processing or idling and/or to indicate a progress of a processing job. In some examples, the processor108includes an on-board processor of a client device such as a mobile phone, a tablet, a watch, a wearable, a computer, a television, and/or a robot. In some examples, the processor108includes an external processor of a server device and/or an external processor of another client device, such that the capturing (e.g., by the sensor106) and the processing (e.g., by the processor108) of the system100are performed with more than one device. For example, a sensor106is a microphone on a mobile phone (e.g., located at a client position) and is configured to capture a phone conversation in audio form, which is transmitted (e.g., wirelessly) to a server computer (e.g., located at a server position). For example, the server computer (e.g., located at a server position) includes the processor108configured to process the input (e.g., an audio file and/or an audio signal) that is sent by the sensor106and received by the processor108.

According to some embodiments, the processor108is configured to output processed data, signal, and/or information, to the presenter110(e.g., a display) of the system100. In some examples, the output is a processed or transformed form of the input received by the processor108(e.g., an audio file and/or an audio signal sent by the sensor106). For example, the processor108is configured to generate a transformed conversation and send the transformed conversation to the presenter110(e.g., a display) of the system100. As an example, the processor108is configured to output synchronized text accompanied by a timestamped audio recording by transforming the conversation that is captured in audio form (e.g., captured by the sensor106). In some embodiments, the processing and/or transforming performed by the processor108is real-time or near real-time. In some embodiments, the processor108is configured to process a live recording (e.g., a live recording of a human-to-human conversation) and/or a pre-recording (e.g., a pre-recording of a human-to-human conversation).

In some embodiments, the presenter110is configured to present, display, play, project, and/or recreate the conversation that is captured, for example, by the sensor106, before and/or after transformation by the processor108. For example, the presenter110(e.g., a display) is configured to receive the transformed conversation from the processor108and present the transformed conversation. As an example, the presenter110(e.g., a display) receives the captured conversation from the processor108before and/or after input (e.g., an audio file and/or an audio signal) to the processor108is transformed by the processor108into output (e.g., transformed conversation).

In some examples, the presenter110is or is part of a mobile device, a web browser, a computer, a watch, a phone, a tablet, a robot, a projector, a television, and/or a display. In some embodiments, the presenter110is provided similarly to the interface104by the same device. In some examples, a mobile phone is configured to provide both the interface104(e.g., touchscreen) and the presenter110(e.g., display). In certain examples, the interface104(e.g., touchscreen) of the mobile phone is configured to also function as the presenter110(e.g., display).

In certain embodiments, the presenter110includes a presenter interface configured for a user, analyzer, and/or recipient to interact with, edit, and/or manipulate the presented conversation. In some examples, the presenter110is communicatively coupled to the controller102such that the controller102provides instructions to the presenter110, such as to switch the presenter110on (e.g., presenting a transformed conversation) and/or switch the presenter110off.

As discussed above and further emphasized here,FIG.1is merely an example, which should not unduly limit the scope of the claims. One of ordinary skill in the art would recognize many variations, alternatives, and modifications. In certain examples, the system100further includes other components and/or features in addition to the controller102, the interface104, the sensor106, the processor108, and/or the presenter110. For example, the system100includes one or more sensors additional to sensor106, such as a camera, an accelerometer, a temperature sensor, a proximity sensor, a barometer, a biometric sensor, a gyroscope, a magnetometer, a light sensor, and/or a positioning system (e.g. a GPS).

FIG.2is a simplified diagram showing a method200for processing and presenting one or more conversations according to some embodiments of the present invention. This diagram is merely an example, which should not unduly limit the scope of the claims. One of ordinary skill in the art would recognize many variations, alternatives, and modifications. The method200includes process202for receiving one or more instructions, process204for capturing one or more conversations, process206for automatically transforming one or more conversations, and process208for presenting one or more transformed conversations. Although the above has been shown using a selected group of processes for the method, there can be many alternatives, modifications, and variations. For example, some of the processes may be expanded and/or combined. Other processes may be inserted to those noted above. Depending upon the embodiment, the sequence of processes may be interchanged with others replaced.

In some examples, some or all processes (e.g., steps) of the method200are performed by the system100. In certain examples, some or all processes (e.g., steps) of the method200are performed by a computer and/or a processor directed by a code. For example, a computer includes a server computer and/or a client computer (e.g., a smartphone). In some examples, some or all processes (e.g., steps) of the method200are performed according to instructions included by a non-transitory computer-readable medium (e.g., in a computer program product, such as a mobile app and/or a web app). For example, a non-transitory computer-readable medium is readable by a computer including a server computer and/or a client computer (e.g., a smartphone). As an example, instructions included by a non-transitory computer-readable medium are executed by a processor including a processor of a server computer and/or a processor of a client computer (e.g., a smartphone).

At the process202, one or more instructions are received. In some examples, one or more instructions are provided by a user (e.g., a human, and/or a hardware and/or software system) and received by one or more components of the system100described above, such as received by the interface104, the controller102, the sensor106, the processor108, and/or the presenter110. For example, the one or more instructions include a direct instruction (e.g., when the instruction is provided directly to a component) and/or an indirect instruction (e.g., when the instruction is provided to a gateway component which then instructs the component of interest to perform a process).

In certain examples, the one or more instructions cause the controller102to switch the sensor106between a capturing state and an idling state. For example, in the capturing state, the sensor106captures one or more conversations. In another example, in the idling state, the sensor106does not capture any conversation. In some examples, receiving a direct instruction includes a user directly switching on the sensor106to start the capturing of a conversation. In certain examples, receiving an indirect instruction includes receiving a start instruction via the interface104, which then instructs the controller102to instruct the sensor120to start capturing a conversation.

At the process204, one or more conversations (e.g., one or more human-to-human conversations) are captured. In some examples, one or more conversations (e.g., a meeting conversation and/or a phone conversation) are captured by live recording via the sensor106(e.g., a microphone, a phone, a receiver, and/or a computing device). In certain examples, one or more conversations are captured by loading (e.g., by wire and/or wirelessly) one or more conversations in audio form (e.g., a .mp3 file, a .wav file, and/or a .m4a file). In some embodiments, capturing one or more conversations include capturing an incoming and/or outgoing phone conversation. In some embodiments, capturing one or more conversations includes capturing minutes, notes, ideas, and/or action items (e.g., of a meeting). In some embodiments, capturing one or more conversations includes capturing metadata corresponding to the one or more conversations, and the metadata include date of capture, time of capture, duration of capture, and/or title of the capture (e.g., a title that is entered via the interface104).

In some embodiments, capturing one or more conversations includes utilizing one or more components (e.g., the sensor106, the controller102, the processor108, and/or the interface104) of the system100and/or utilizing one or more components external to the system100. In some examples, the sensor106of the system100is configured to capture a live conversation. In certain examples, the controller102and/or the processor108are configured to receive a pre-recorded conversation (e.g., a .mp3 file, a .wav file, and/or a .m4a file). In some examples, the interface104is configured to capture metadata associated to the conversation. In certain examples, a clock (e.g., of the system100or external to the system100) is configured to provide date and time information associated with the conversation.

At the process206, one or more conversations (e.g., the one or more conversations captured at the process204) are transformed (e.g., transcribed, extracted, converted, summarized, and/or processed) automatically. In some examples, the captured conversations are transformed by the processor108. In certain examples, the process206is implemented according toFIG.3.

FIG.3is a simplified diagram showing the process206for automatically transforming one or more conversations, according to some embodiments of the present invention. This diagram is merely an example, which should not unduly limit the scope of the claims. One of ordinary skill in the art would recognize many variations, alternatives, and modifications. The process206includes process302for receiving a conversation, process304for automatically transcribing the conversation to synchronized text (e.g., synchronized transcript), process306for automatically segmenting the conversation in audio form and the synchronized text, process308for automatically assigning a speaker label to each conversation segment, and process310for sending the transformed conversation (e.g., including synchronized text with speaker-labeled conversation segments). Although the above has been shown using a selected group of processes for the process206, there can be many alternatives, modifications, and variations. For example, some of the processes may be expanded and/or combined. Other processes may be inserted to those noted above. Depending upon the embodiment, the sequence of processes may be interchanged with others replaced.

In some examples, some or all processes (e.g., steps) of the process206are performed by the system100. In certain examples, some or all processes (e.g., steps) of the process206are performed by a computer and/or a processor directed by a code. For example, a computer includes a server computer and/or a client computer (e.g., a smartphone). In some examples, some or all processes (e.g., steps) of the process206are performed according to instructions included by a non-transitory computer-readable medium (e.g., in a computer program product, such as a mobile app and/or a web app). For example, a non-transitory computer-readable medium is readable by a computer including a server computer and/or a client computer (e.g., a smartphone). As an example, instructions included by a non-transitory computer-readable medium are executed by a processor including a processor of a server computer and/or a processor of a client computer (e.g., a smartphone).

At the process302, a conversation (e.g., a human-to-human conversation) is received. For example, a conversation is received by the system100, such as by the processor108. In some embodiments, the conversation (e.g., a human-to-human conversation) received in process302is in audio form (e.g., sound wave and/or digital signal) and is captured by and/or sent from the sensor106of the system100. In some embodiments, the conversation received in process302is a live recording (e.g., a live recording of a human-to-human conversation). In some examples, the conversation is received (e.g., by the processor108of the system100) continuously and/or intermittently (e.g., via fixed frequency push). In certain examples, the conversation is received (e.g., by the processor108of the system100) in real-time and/or in near real-time (e.g., with a time delay less than 5 minutes, 1 minutes, or 4 seconds between capture and reception of a conversation).

In certain embodiments, the conversation (e.g., a human-to-human conversation) received in process302is a pre-recorded conversation in audio form (e.g., sound wave and/or digital signal). For example, the pre-recorded conversation is an audio recording (e.g., a .mp3 file, a .wav file, and/or a .m4a file) uploaded from an internal device and/or an external device (e.g., a local storage device such as a hard drive, and/or a remote storage device such as cloud storage). In some examples, the conversation received in process302is a phone conversation. In certain examples, the conversation is automatically received in process302, such as by the processor108, such as whenever a conversation is sent to the processor (e.g., from the sensor106and/or from the controller102).

At the process304, a conversation (e.g., an audio-form conversation received at process302) is automatically transcribed into synchronized text. In some embodiments, the conversation is automatically transcribed (e.g., with no user input or with minimal user input). In some examples, the transcribing is performed by at least the processor108of the system100. In certain examples, the transcribing is performed by the processor108and modifiable by a human. In some embodiments, the conversation transcribed at process304includes the conversation received at process302, which is in audio form (e.g., sound wave and/or digital signal).

In some embodiments, the text (e.g., the transcript) generated at process304includes English words, phrases, and/or terms. In certain embodiments, the audio-form conversation received at process302and the text generated at process304are timestamped and/or indexed with time, to synchronize the audio and the text. For example, the audio-form conversation received at process302and the text (e.g., the transcript) generated at process304are synchronized. In some examples, the text (e.g., the transcript) generated at process304is searchable. For example, the text (e.g., the transcript) is searchable via a search bar as shown inFIG.70, which is discussed below. In certain examples, once transcribed at process304, the conversation (e.g., from process302) becomes a transcribed conversation including both audio and text that is synchronized with the audio.

At the process306, a conversation in audio form (e.g., the conversation in audio form received at process302) and a synchronized text (e.g., the synchronized text generated at process304) are automatically segmented. In some embodiments, the audio-form conversation and the synchronized text are automatically segmented (e.g., with no user input or with minimal user input), and the segmented audio-form conversation and the segmented synchronized text are automatically generated. In some examples, the segmenting is performed by the processor108of the system100. In certain examples, the segmenting is performed by the processor108and modifiable by a human. In certain embodiments, the conversation (e.g., audio-form conversation and/or the synchronized text) is segmented at process304into different segments when a speaker change occurs and/or a natural pause occurs. In some embodiments, each segment of the audio-form conversation and the synchronized text generated at process306is associated with one or more timestamps, each timestamp corresponding to the start time, and/or the end time. In certain embodiments, each segment of the audio-form conversation and the synchronized text generated at process306is associated with a segment timestamp, the segment timestamp indicating the start time, the segment duration, and/or the end time.

In some embodiments, the audio-form conversation and the synchronized text are segmented at process306into a plurality of segments that include one or more segments corresponding to the same speaker. In some examples, each segment is spoken by a single speaker. For example, the processor108is configured to automatically distinguish one or more speakers of the audio-form conversation. In certain examples, multiple segments spoken by the same speaker are next to each other and/or are separated by one or more segments spoken by one or more other speakers. In some embodiments,FIG.7shows an audio-form conversation and its synchronized text in segmented form, and is discussed below.

In certain embodiments, once segmented at process306, the audio-form conversation (e.g., the conversation in audio form received at process302) and the synchronized text (e.g., the synchronized text generated at process304) becomes a segmented audio-form conversation and a segmented synchronized text. In some embodiments, segments of the audio-form conversation and segments of the synchronized text have one-to-one correspondence relationship. In some examples, each segment of audio-form conversation corresponds to one segment of synchronized text, and the segment of synchronized text is synchronized with that segment of audio-form conversation. In certain examples, different segments of audio-form conversation correspond to different segments of synchronized text, and the different segments of synchronized text is synchronized with the different segments of audio-form conversation respectively.

At the process308, a speaker label is automatically assigned to each segment of text synchronized to one segment of audio-form conversation as generated by the process306. In some embodiments, the speaker label is automatically assigned (e.g., with no user input or minimal user input), and the speaker-assigned segmented synchronized text and corresponding segmented audio-form conversation are automatically generated. In some examples, the assigning of speaker label is performed by the processor108of the system100. In certain examples, the assigning of speaker label is performed by the processor108and modifiable by a human. In some embodiments, the speaker label includes a speaker name and/or a speaker picture, as shown inFIG.7, which is discussed below.

In some embodiments, at the process308, one or more segments of text, which are synchronized to one or more corresponding segments of audio-form conversation, are grouped into one or more segment sets each associated with the same speaker pending a speaker label assignment. In those embodiments, the speaker label is assigned to each segment set, which in turn assign the speaker label to all segments belonging to the segment set.

In some embodiments, at the process308, the speaker label is assigned to each segment of text synchronized to one corresponding segment of audio-form conversation, by matching a voiceprint of the corresponding segment of audio-form conversation to a reference voiceprint corresponding to a speaker label.

In certain embodiments, the process308includes assigning an “unknown” speaker label (e.g., with no name and/or with a placeholder picture) to a segment, as shown inFIG.7, which is discussed below. In some embodiments, once assigned with one or more speaker labels at process308, the segmented text that is synchronized with the segmented audio-form conversation (e.g., as generated at process306) becomes a speaker-assigned segmented text that is synchronized with the segmented audio-form conversation, with a speaker label assigned to each segment.

In some embodiments, a speaker corresponds to a speaker label. In certain examples, a speaker label corresponds to a speaker name. In some examples, the speaker label corresponding to an unknown speaker does not include a speaker name. In certain examples, the process206automatically identifies a new speaker voiceprint, but the user has not provided the name and/or the picture of the speaker yet; hence the speaker is determined to be, for example, an unknown speaker (e.g., as shown inFIG.10).

At the process310, a transformed conversation (e.g., including the speaker-assigned segmented synchronized text and its corresponding segmented audio-form conversation) is sent. For example, the transformed conversation is sent from the processor108to the controller102and/or to the presenter110. In some embodiments, the transformed conversation sent at process310includes the speaker-assigned segmented synchronized text and its corresponding segmented audio-form conversation as generated by the process308. In certain embodiments, the transformed conversation sent at process310includes the segmented audio-form conversation and the segmented synchronized text as generated by the process306.

In some embodiments, the transformed conversation includes segmented audio, segmented text synchronized with segmented audio, speaker labels (e.g., name and/or picture) associated with the segments, and/or metadata (e.g., including a date, a time, a duration and/or a title). In certain embodiments, the transformed conversation is sent automatically, for example, by the processor108.

As discussed above and further emphasized here.FIG.3is merely an example, which should not unduly limit the scope of the claims. One of ordinary skill in the art would recognize many variations, alternatives, and modifications. In some examples, the process304and the process306are modified such that segmenting the conversation in audio form occurs before synchronized text is transcribed for each segment. In certain examples, the process308, at which one or more speaker labels are assigned, occurs before transcribing the conversation in audio form and/or segmenting the conversation in audio form.

In certain embodiments, transcribing, segmenting, and/or assigning speaker label to a conversation are performed with the aid of a user and/or human. For example, a transcript automatically generated (e.g., at process304) is editable (e.g., by a user and/or human). In yet another example, segments automatically generated (e.g., at process306) is editable to split one segment and/or combine multiple segments (e.g., by a user and/or human). In yet another example, speaker labels automatically assigned (e.g., at process308) are editable (e.g., by a user and/or human).

In certain embodiments, the conversation to which transcribing, segmenting, and/or assigning speaker label are performed includes the conversation in audio form or the transcription. In some examples, the conversation in audio form is first segmented and/or speaker-assigned, and followed by having each segment transcribed to generate the synchronized text associated with each segment of conversation in audio form. In certain examples, the conversation in audio form is first transcribed to generate synchronized transcript, and followed by segmenting and/or assigning speaker label to the transcript. For example, the conversation in audio form is not directly segmented, but instead is indirectly segmented or remains unsegmented and merely corresponds to the transcript in a word-by-word relationship (e.g., each transcribed text corresponds to a timestamp with an associated audio).

Returning toFIG.2, at process208, one or more transformed conversations (e.g., the transformed conversation sent at the process310) are presented. In certain embodiments, the process208includes presenting the transformed conversation (e.g., including the speaker-assigned segmented synchronized text and its corresponding segmented audio-form conversation) with the presenter110. In certain examples, the text is synchronized with the audio-form conversation at both the segment level and the word level.

In certain embodiments, the process208includes presenting the metadata associated with the transformed conversation. For example, the metadata include a date (e.g., of capturing, processing, or presenting), a time (e.g., of capturing, processing, or presenting), a duration (e.g., of the conversation), and/or a title, as shown inFIG.5In some embodiments, the process208includes presenting a player, such as an audio player. For example, the audio player is a navigable audio player (e.g., as shown inFIG.7) configured to provide control (e.g., to a user) such that the presenting of the transformed conversation is interactive.

FIG.102is a simplified diagram showing the process208for presenting one or more transformed conversations, according to some embodiments of the present invention. This diagram is merely an example, which should not unduly limit the scope of the claims. One of ordinary skill in the art would recognize many variations, alternatives, and modifications. In various examples, the process208includes process10202for presenting sync status, process10204for presenting recent activities, process10206for presenting grouping information, process10208for presenting labeling information, process10210for presenting editable conversations, process10212for presenting shareable conversations, process10214for presenting a recorder, and/or process10216for presenting search results.

In some examples, some or all processes (e.g., steps) of the process208are performed by the system100. In certain examples, some or all processes (e.g., steps) of the process208are performed by a computer and/or a processor directed by a code. For example, a computer includes a server computer and/or a client computer (e.g., a smartphone). In some examples, some or all processes (e.g., steps) of the process208are performed according to instructions included by a non-transitory computer-readable medium (e.g., in a computer program product, such as a mobile app and/or a web app). For example, a non-transitory computer-readable medium is readable by a computer including a server computer and/or a client computer (e.g., a smartphone). As an example, instructions included by a non-transitory computer-readable medium are executed by a processor including a processor of a server computer and/or a processor of a client computer (e.g., a smartphone).

In certain examples, the process10202includes presenting a sync status indicator402, a sync progress bar404, and a time-remaining indicator406, as shown inFIG.4. In some examples, the sync status indicator404displays a sync incomplete indicator, a sync complete indicator, and/or a syncing indicator. In various embodiments, the sync progress bar404indicates the percentage of sync completed or incomplete. In some examples, the time-remaining indicator406presents the time remaining for the sync to complete. In some examples, the time remaining for the sync to complete corresponds to the sync progress bar404.

In various embodiments, the process102014includes presenting recent conversations408, and/or presenting recent group activities410. In some embodiments, the process10204is implemented according toFIG.4. In some examples, presenting recent conversations408includes presenting one or more of the most recently recorded conversations and/or one or more of the most recently transformed conversations. In various examples, presenting recent group activities410includes presenting recent conversations of one or more groups. In some examples, each conversation is assigned to one or more groups, wherein each group has one or more group members.

In some examples, the process10206includes presenting a first group name indicator3602, presenting a first group member indicator3604, presenting a second group name indicator802, presenting a second group member indicator804, and/or presenting a third group member indicator3502. In some embodiments, the process10206is implemented according toFIG.36,FIG.8, and/orFIG.35. In certain examples, the process10206includes presenting a group listing page having a listing of one or more groups (e.g., created by a user) by group names (e.g., as shown inFIG.36). In some examples, the first group member indicator3604is positioned next to the first group name indicator3602to help indicate at least one or more of the group members associated with the group (e.g., as shown inFIG.36). In various embodiments, the second group name indicator802and the second group member indicator804are presented next to a conversation title to help indicate which group the conversation is assigned to and the names or pictures of one or more of the group members of the group (e.g., as shown inFIG.8). In certain examples, the third member indicator3502helps indicate at least one or more of the group members associated with a group, such as on a group page where a listing of conversations assigned to the group is presented (e.g., as shown inFIG.35).

In certain examples, a conversation is assignable to one or more groups, wherein each group includes one or more group members. In some embodiments, a group is manually created, such as by assigning a group name and manually adding group members. In some embodiments, a group is automatically created (e.g., by the processor108), such as based on the conversation's speaker information, location, time, and/or topic. In some examples, a conversation is associated with a calendar event of a synced calendar that provides information useful for automatically creating a group. In various embodiments, contacts or an address book of a user is synced to provide information useful for automatically creating a group. For example, the system is configured for a user to create a system profile with a Google account or to link his/her Google account such that the user's Google Calendar and/or Google Contacts are synced with the system profile (e.g., as shown inFIG.48and/orFIG.55). In some examples, a group is first created (e.g., by an admin user), which may initially have no group members and have no conversations assigned to the group. In certain examples, one or more conversations can be assigned to a group that has been created but have not yet had any group members joined, such that the group includes the one or more conversations but no group members, where the conversations are not yet shared via the group. In certain examples, one or more group members join a group having one or more conversations, such that the one or more conversations may be shared automatically with the one or more group members. In some examples, one or more group members join a group having no conversations, and may automatically be shared with any future conversations added to the group. In various examples, group information and settings are editable, such as manually by a user and/or automatically (e.g., by the processer108). In certain examples, a conversation assigned to a group is viewable, editable, and/or shareable by one or more of the group's group members. For example, the system is configured for one or more of the group members to edit a conversation assigned to the group at the same time with the edits applied in real time for all group members.

In various embodiments, the process10208includes presenting a listing of labels4402, and/or a listing of conversations assigned with a common label4502. In some embodiments, the process10208is implemented according toFIG.44, and/orFIG.45. For example, a listing of labels4402has a listing of one or more labels (e.g., created by a user). In certain examples, the listing of labels4402includes a conversation count indicator positioned next to each label to indicate the amount of conversations assigned with the label. In certain examples, a listing of conversations assigned with a common label4502is shown in a label page (e.g., as shown inFIG.45). In various embodiments, the system is configured such that one or more labels are assignable to a conversation. In certain examples, one or more labels are manually assignable (e.g., by a user) and/or automatically assignable (e.g., according to conversation title, metadata, and/or transcript). In some examples, the labeling changes apply to only the user making the changes to the conversation. In other examples, the labeling changes apply to all users having access to the conversation (e.g., a shared conversation).

In certain examples, the process10210includes presenting editable conversations to which one or more of delete, hide, merge, split, and speaker-assignment is applicable, such as applicable to one or more of conversation segments of the conversation. In some examples, a conversation segment is referred to as a conversation snippet or a conversation bubble. In some examples, a transformed conversation includes one or more snippets, wherein each snippet includes one or more conversation segments. In various examples, deleting a snippet is not reversible such that a deleted snippet is removed from the conversation for all users who have access to the conversation. In some examples, deleting a snippet removes the transcript text as well as the corresponding audio (e.g. as shown inFIG.9). In some examples, hiding a snippet is reversible such that a hidden snippet remains in the conversation and is revealable. For example, a hidden snippet is viewable by the user who hid the snippet, such as in a stroked-out form (e.g., as shown inFIG.13), whereas the hidden snippet is hidden from others who have access to the conversation. In various embodiments, two or more snippets are mergeable (e.g., as shown inFIG.16,FIG.17,FIG.19, and/orFIG.20). For example, a first snippet having a first speaker label and a first starting timestamp and a second snippet having a second speaker label and a second starting timestamp are mergeable to create a merged snippet having a new speaker label (e.g., same as the first speaker label) and a new timestamp (e.g., same as the first starting timestamp). In certain examples, a snippet is splittable into two or more snippets each having a new speaker label and a new starting timestamp. In various embodiments, speaker label is assignable and/or re-assignable, as shown inFIG.32and/orFIG.33.

In various embodiments, the process10212includes presenting conversations shareable selectively or wholly (e.g., as shown inFIG.23). For example, one or more snippets of a transformed conversation are selectable and shareable such that the non-selected portions of the conversation are not accessible (e.g., viewable) by recipients of a sharing invitation to the selected conversation content. In some examples, a conversation is shareable within the application to another user, such as to a group member. In various examples, a conversation is shareable with external recipients, such as via a universal resource locator (URL). For example, a shared conversation or snippet is accessible by all who has the URL (e.g., as shown inFIG.26). In some examples, access is revocable (e.g., as shown inFIG.31), such as by the user who granted access. In certain examples, access is configurable to be view-only, can-edit, or password-protected (e.g., as shown inFIG.79). In some examples, access is grantable to a recipient, a plurality of recipients, or a group (e.g., as shown inFIG.80). In some examples, the system is configured such that a conversation assigned to a group is automatically shared to the group's group members such that each member is automatically granted with access.

In some embodiments, the process10214includes presenting a recorder in a conversation page (e.g., as shown inFIG.56). In certain examples, content (e.g., grouping, labeling, title, transcript) of the conversation page is editable and/or updatable while a recorder5602indicates the recording status (e.g., recording, paused, recorded time). In certain examples, a shared live conversation (e.g., a live-recording) is updated in real-time while the recorder indicates the status of the recording (e.g., as shown inFIG.59). In some examples, the content (e.g., transcript) of a shared live conversation is configurable to be updated automatically according to the live recording. In certain examples, content update is pushed to all viewers who have access to the shared conversation in real time or near real time. In some examples, the process10214includes presenting a recorder in a non-conversation page, such as a home page or a listing page (e.g., as shown inFIG.61). In certain examples, the recorder6102is configured to display recording status of a conversation while the system receives navigation commands (e.g., from a user). In various examples, the recorder6102displays time recorded, conversation title, a stop button, and/or a return-to-page button.

In some examples, the process10214includes presenting a recorder in an account setup page, such as a voiceprint initialization, calibration, or training page (e.g., as shown inFIG.51). For example, a recorder5102is configured to be activatable for a user to record his/her voice to help improve the system's (e.g., system100) abilities, performance, accuracy, and/or speed in transforming a conversation. In certain embodiments, the system (e.g., the presenter of the system) is configured to present a voiceprint calibration script for a user to read to help record a voiceprint calibration recording. In some examples, the voiceprint calibration script is selected (e.g., by the processor of the system) to specifically help establish a voiceprint for the user. In various examples, the voiceprint includes a language model component and an acoustic model component that are created and/or improved by receiving the voiceprint calibration recording. In some examples, the sensor106ofFIG.1is configured to capture the voiceprint calibration recording to help establish and/or improve a voiceprint of a user.

In some examples, search results presented in process10216include one or more conversations, one or more snippets, one or more groups, and/or one or more labels (e.g., as shown inFIG.67). In certain examples, the type of a search result is indicated via a type label6702, as shown inFIG.67. In various examples, each search result is associated with a search term input (e.g., entered via a search bar6502, as shown inFIG.65). In some embodiments, the search bar6502is configured to accept speaker name, location, group name, and/or date (e.g., of recording, of uploading, of transforming, of sharing), as shown inFIG.65. In some embodiments, a snippet result presented as a search result (e.g., in response to a search term) is presented with the speaker label of the snippet7002, a timestamp associated with the snippet7004, and/or at least part of the snippet's text7006(e.g., as shown inFIG.70). In various examples, the system is configured such that activating (e.g., clicking on) a snippet result leads to the conversation containing the snippet result. In certain examples, the system is configured such that activation (e.g., clicking on) a snippet result leads to the conversation containing the snippet result at the timestamp associated with the snippet result.

As discussed above and further emphasized here,FIG.1,FIG.2,FIG.3, andFIG.102are merely examples, which should not unduly limit the scope of the claims. One of ordinary skill in the art would recognize many variations, alternatives, and modifications. In some examples, the system as shown inFIG.1is used to process and present a speech by a single-speaker and/or a conversation made by a single speaker talking to himself or herself. In certain examples, the method as shown inFIG.1is used to process and present a speech by a single-speaker and/or a conversation made by a single speaker talking to himself or herself. In some examples, the process206as shown inFIG.3is used to automatically transform a speech by a single-speaker and/or a conversation made by a single speaker talking to himself or herself.

FIGS.4-101are simplified diagrams showing a user interface and/or a presenter related toFIG.1,FIG.2,FIG.3, and/orFIG.102, according to some embodiments of the present invention. These diagrams are merely examples, which should not unduly limit the scope of the claims. One of ordinary skill in the art would recognize many variations, alternatives, and modifications.

As shown inFIGS.4-101, in some examples, the user interface (e.g., the user interface104) also functions as the presenter (e.g., the presenter110). In certain examples, one or more ofFIGS.4-101pertain to a user interface for a web browser, and/or pertain to a user interface for an offline application for a stationary device (e.g., desktop computer, television, display) and/or a portable and/or mobile device (e.g., laptop, display, tablet, mobile phone, and/or vehicles).

In some embodiments, a user interface and/or a presenter of system100is navigable and is implemented according to one or more ofFIGS.4-6,21,22,34, and37-40. In some examples, a user profile of system100is configured to be created and configured according to one or more ofFIGS.46-55,72, and73. In certain embodiments, grouping of one or more conversations (e.g., conversations presented at process208) and displaying grouping information are implemented according to one or more ofFIGS.8,14,15,35,36. In certain examples, labeling of one or more conversations (e.g., conversations presented at process208) and displaying labeling information are implemented according to one or more ofFIGS.41-45. In some embodiments, editing of a conversation snippet (e.g., of a transformed conversation from process206) is implemented according to one or more ofFIGS.7,9,10-13,16-20, and32-33. In various examples, sharing of one or more conversations (e.g., conversations presented at process208) and displaying sharing status are implemented according to one or more ofFIGS.23-31and74-80. In some examples, recording a conversation (e.g., a conversation captured at process204) and displaying recording status are implemented according to one or more ofFIGS.56-64. In certain examples, performing searches for one or more conversations and presenting search results are implemented according to one or more ofFIGS.65-71. In some examples,FIGS.81-101show alternative presentations of one or more user interfaces or presenters shown inFIGS.4-80.

According to some embodiments, a system for processing and presenting a conversation includes a sensor, a processor, and a presenter. The sensor is configured to capture an audio-form conversation. The processor is configured to automatically transform the audio-form conversation into a transformed conversation. The transformed conversation includes a synchronized text, wherein the synchronized text is synchronized with the audio-form conversation. The presenter is configured to present the transformed conversation including the synchronized text and the audio-form conversation. The presenter is further configured to present the transformed conversation to be navigable, searchable, assignable, editable, and shareable. For example, the system is implemented according to at leastFIG.1,FIG.2,FIG.3, and/orFIG.102.

In some examples, the transformed conversation presented by the presenter is assignable to a group having one or more group members such that access to the transformed conversation is granted to the one or more group members. In certain examples, the one or more group members are automatically granted access to the transformed conversation when the transformed conversation is assigned to the group. The various examples, changes and updates applied to the transformed conversation are viewable to the one or more group members. In various embodiments, the changes and updates applied to the transformed conversation are viewable to the one or more group members in real time. In certain embodiments, each of the one or more group members can at least one of view, edit, and share the transformed conversation. In some embodiments, the transformed conversation is automatically assigned to a group according to conversation data including at least one of date of capture, time of capture, title of the capture, and speaker identity. In some examples, the conversation data is automatically retrieved from a synced calendar by the processor.

In various examples, the transformed conversation presented by the presenter includes one or more editable conversation snippets that are selectively deletable. In certain examples, the transformed conversation presented by the presenter includes one or more editable conversation snippets that are selectively mergeable. In some embodiments, the transformed conversation presented by the presenter includes one or more editable conversation snippets that are selectively hidable. In certain embodiments, the transformed conversation presented by the presenter includes one or more editable conversation snippets each having a speaker label that is assignable and re-assignable.

In certain examples, the transformed conversation presented by the present includes one or more shareable conversation snippets that are selectively shareable. In some examples, the transformed conversation presented by the presenter includes one or more searchable conversation snippets. In some embodiments, a searchable conversation snippet returned as a search result entry in response to a search is configured to lead to the conversation snippet for viewing. In various examples, the sensor is further configured to capture a voiceprint calibration recording to help establish a voiceprint of a user. In certain examples, the presenter is further configured to present a voiceprint calibration script for the user to read to help create the voiceprint calibration recording. In various examples, the sensor is configured to capture a conversation while the presenter presents the conversation.

According to some embodiments, a computer-implemented method for processing and presenting a conversation includes receiving an audio-form conversation; automatically transforming the audio-form conversation into a transformed conversation, the transformed conversation including a synchronized text, the synchronized text being synchronized with the audio-form conversation; and presenting the transformed conversation including the synchronized text and the audio-form conversation. The presenting the transformed conversation including the synchronized text and the audio-form conversation includes presenting the transformed conversation to be navigable, searchable, assignable, editable, and shareable. For example, the computer-implemented method is implemented according to at leastFIG.1,FIG.2,FIG.3, and/orFIG.102.

According to some embodiments, a non-transitory computer-readable medium with instructions stored thereon, that when executed by a processor, perform the processes including: receiving an audio-form conversation; automatically transforming the audio-form conversation into a transformed conversation, the transformed conversation including a synchronized text, the synchronized text being synchronized with the audio-form conversation; and presenting the transformed conversation including the synchronized text and the audio-form conversation. The presenting the transformed conversation including the synchronized text and the audio-form conversation includes presenting the transformed conversation to be navigable, searchable, assignable, editable, and shareable. For example, the non-transitory computer-readable medium is implemented according to at leastFIG.1,FIG.2,FIG.3, and/orFIG.102.

According to various embodiments, a system for presenting a conversation includes a sensor configured to capture an audio-form conversation and send the captured audio-form conversation to a processor. The processor is configured to automatically transform the audio-form conversation into a transformed conversation. The transformed conversation includes a synchronized text. The synchronized text is synchronized with the audio-form conversation. The system further includes a presenter configured to receive the transformed conversation from the processor and present the transformed conversation including the synchronized text and the audio-form conversation. The presenter is further configured to present the transformed conversation to be navigable, searchable, assignable, editable, and shareable. For example, the system is implemented according to at leastFIG.1,FIG.2,FIG.3, and/orFIG.102.

According to certain embodiments, a computer-implemented method for processing and presenting a conversation includes receiving an audio-form conversation; sending the received audio-form conversation to automatically transform the audio-form conversation into a transformed conversation, wherein the transformed conversation includes a synchronized text, that is synchronized with the audio-form conversation; receiving the transformed conversation; and presenting the transformed conversation including the synchronized text and the audio-form conversation, wherein the presenting the transformed conversation including the synchronized text and the audio-form conversation includes presenting the transformed conversation to be navigable, searchable, assignable, editable, and shareable. For example, the computer-implemented method is implemented according to at leastFIG.1,FIG.2,FIG.3, and/orFIG.102.

According to certain examples, a non-transitory computer-readable medium with instructions stored thereon, that when executed by a processor, perform the processes including receiving an audio-form conversation; sending the received audio-form conversation to automatically transform the audio-form conversation into a transformed conversation, the transformed conversation including a synchronized text, the synchronized text being synchronized with the audio-form conversation; receiving the transformed conversation; and presenting the transformed conversation including the synchronized text and the audio-form conversation, wherein the presenting the transformed conversation including the synchronized text and the audio-form conversation includes presenting the transformed conversation to be navigable, searchable, assignable, editable, and shareable. For example, the non-transitory computer-readable medium is implemented according to at leastFIG.1,FIG.2,FIG.3, and/orFIG.102.

According to some examples, a system for transforming a conversation includes a processor configured to receive from a sensor a captured audio-form conversation; automatically transform the audio-form conversation into a transformed conversation, the transformed conversation including a synchronized text, the synchronized text being synchronized with the audio-form conversation; and send the transformed conversation to a presenter configured to present the transformed conversation including the synchronized text and the audio-form conversation, wherein the processor is further configured to send the transformed conversation to the presenter further configured to present the transformed conversation to be navigable, searchable, assignable, editable, and shareable. For example, the system is implemented according to at leastFIG.1,FIG.2,FIG.3, and/orFIG.102.

According to certain examples, a computer-implemented method for transforming a conversation includes receiving an audio-form conversation; automatically transforming the audio-form conversation into a transformed conversation, the transformed conversation including a synchronized text, the synchronized text being synchronized with the audio-form conversation; and sending the transformed conversation to present the transformed conversation including the synchronized text and the audio-form conversation, wherein the sending the transformed conversation to present the transformed conversation including the synchronized text and the audio-form conversation includes sending the transformed conversation to present the transformed conversation to be navigable, searchable, assignable, editable, and shareable. For example, the computer-implemented method is implemented according to at leastFIG.1,FIG.2,FIG.3, and/orFIG.102.

According to various examples, a non-transitory computer-readable medium with instructions stored thereon, that when executed by a processor, perform the processes including receiving an audio-form conversation; automatically transforming the audio-form conversation into a transformed conversation, the transformed conversation including a synchronized text, the synchronized text being synchronized with the audio-form conversation; and sending the transformed conversation to present the transformed conversation including the synchronized text and the audio-form conversation, wherein the sending the transformed conversation to present the transformed conversation including the synchronized text and the audio-form conversation includes sending the transformed conversation to present the transformed conversation to be navigable, searchable, assignable, editable, and shareable. For example, the non-transitory computer-readable medium is implemented according to at leastFIG.1,FIG.2,FIG.3, and/orFIG.102.

Various embodiments are related to architecture, flow, and presentation of conversations. For example, certain embodiments include systems, methods, and apparatuses for architecture, flow and presentation of conversations. For at least one embodiment, the conversations include human to human conversations. At least some embodiments include transcribing conversations. At least some embodiments provide searching within the conversations. At least some embodiments include automatic word synchronization which includes synchronization of the audio with the transcript. At least some embodiments include speaker identification. For at least some embodiments, the speaker identification includes a label. For at least some embodiments, the label includes a picture of the speaker.

Some embodiments of the present invention improve speech recognition, diarization and/or speaker-identification (e.g., based on machine learning and/or artificial intelligence). Some examples of the present invention collect a large quantity of speech data and select proper training data which match the end-user speech environment to achieve high speech recognition accuracy, by for example, making speech recognition more resilient to background noise, to far-field speech with lower signal-noise ratio, and/or to various speech accents. Certain examples of the present invention can process a conversation quickly. Some examples of the present invention can separate speeches that are spoken by multiple human speakers. Certain examples of the present invention can process one or more long-form conversation (e.g., a long-form conversation that lasts for several hours) accurately and reliably.

Certain embodiments of the present invention provide excellent user experience and help a broad range of users to improve their daily lives and/or daily work. Some examples of the present invention allow users to avoid taking notes manually (e.g., avoid writing on a paper notebook and/or avoid typing on a computer) so that the users can engage better with other speakers in the conversations and also improve effectiveness of their meetings. Certain examples of the present invention can generate notes for conversations in real time, dramatically reducing turn-around time than by using human transcribers. Some examples of the present invention can dramatically improve enterprise productivity. Certain examples of the present invention can function for in-person meetings, phone calls, and/or video conferences. Some examples of the present invention can automatically generate notes that are digital and searchable. Certain examples of the present invention can automatically generate notes that can be easily shared with colleagues, thus improving collaboration. Some examples of the present invention can help students take lecture notes. Certain examples of the present invention can help deaf students to learn, thus improving their educational experience.

In various examples, some or all components of various embodiments of the present invention each are, individually and/or in combination with at least another component, implemented using one or more software components, one or more hardware components, and/or one or more combinations of software and hardware components. In another example, some or all components of various embodiments of the present invention each are, individually and/or in combination with at least another component, implemented in one or more circuits, such as one or more analog circuits and/or one or more digital circuits. In yet another example, while the embodiments described above refer to particular features, the scope of the present invention also includes embodiments having different combinations of features and embodiments that do not include all of the described features. In yet another example, various embodiments and/or examples of the present invention can be combined.

Additionally, the methods and systems described herein may be implemented on many different types of processing devices by program code comprising program instructions that are executable by the device processing subsystem. The software program instructions may include source code, object code, machine code, or any other stored data that is operable to cause a processing system to perform the methods and operations described herein. Other implementations may also be used, however, such as firmware or even appropriately designed hardware configured to perform the methods and systems described herein.

The systems' and methods' data (e.g., associations, mappings, data input, data output, intermediate data results, final data results, etc.) may be stored and implemented in one or more different types of computer-implemented data stores, such as different types of storage devices and programming constructs (e.g., RAM, ROM, EEPROM, Flash memory, flat files, databases, programming data structures, programming variables, IF-THEN (or similar type) statement constructs, application programming interface, etc.). It is noted that data structures describe formats for use in organizing and storing data in databases, programs, memory, or other computer-readable media for use by a computer program.

The systems and methods may be provided on many different types of computer-readable media including computer storage mechanisms (e.g., CD-ROM, diskette, RAM, flash memory, computer's hard drive, DVD, etc.) that contain instructions (e.g., software) for use in execution by a processor to perform the methods' operations and implement the systems described herein. The computer components, software modules, functions, data stores and data structures described herein may be connected directly or indirectly to each other in order to allow the flow of data needed for their operations. It is also noted that a module or processor includes a unit of code that performs a software operation, and can be implemented for example as a subroutine unit of code, or as a software function unit of code, or as an object (as in an object-oriented paradigm), or as an applet, or in a computer script language, or as another type of computer code. The software components and/or functionality may be located on a single computer or distributed across multiple computers depending upon the situation at hand.

The computing system can include client devices and servers. A client device and server are generally remote from each other and typically interact through a communication network. The relationship of client device and server arises by virtue of computer programs running on the respective computers and having a client device-server relationship to each other.

This specification contains many specifics for particular embodiments. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations, one or more features from a combination can in some cases be removed from the combination, and a combination may, for example, be directed to a subcombination or variation of a subcombination.

Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

Although specific embodiments of the present invention have been described, it will be understood by those of skill in the art that there are other embodiments that are equivalent to the described embodiments. Accordingly, it is to be understood that the invention is not to be limited by the specific illustrated embodiments.

","Yun Fu, Simon Lau, Kaisuke Nakajima, Julius Cheng, Gelei Chen, Sam Song Liang, James Mason Altreuter, Kean Kheong Chin, Zhenhao Ge, Hitesh Anand Gupta, Xiaoke Huang, James Francis McAteer, Brian F. Williams, Tao Xing","Cybersecurity, Telecommunications,Technology, Media, Healthcare, Artificial Intelligence & Machine Learning,Technology, Telecommunications, Consumer Goods, Cybersecurity,Telecommunications, Technology, Consumer Goods","Digital data processing, computer architecture, error detection, data transfer protocols, secure processing systems,Digital information transmission, error control, data encryption, network security protocols,Speech recognition, audio coding, voice processing, speech synthesis, text-to-speech systems,Telephonic communication systems, mobile and landline substation equipment, subscriber services, VoIP","A system for processing and presenting a conversation includes a sensor, a processor, and a presenter. The sensor is configured to capture an audio-form conversation. The processor is configured to automatically transform the audio-form conversation into a transformed conversation. The transformed conversation includes a synchronized text, wherein the synchronized text is synchronized with the audio-form conversation. The presenter is configured to present the transformed conversation including the synchronized text and the audio-form conversation. The presenter is further configured to present the transformed conversation to be navigable, searchable, assignable, editable, and shareable."
12020723,"BACKGROUND

Audio composition software such as a digital audio workstation (DAW) typically presents a document organized into a series of tracks, or lanes, with each track containing sequences of data such as audio clips, MIDI data, volume control data, or other kinds of performance or control data. The data contained in each track is presented to the user in a form which assists a user to understand its nature and properties. For example, audio data is often represented as a waveform to give some indication of the overall sonic characteristics and particularly the amplitude of the audio signal, or as a spectrogram, which provides a more detailed view into the signal's spectral content. DAWs generally represent regions of audio data arranged in a sequence of tracks represented as horizontal or vertical lanes. Data for each track is arranged in a linear timeline within the corresponding lane in which spatial position is used to represent the playback time of the data. Individual media elements, such as clips or MIDI notes, are typically represented as separate objects on a track. The user interaction model usually involves document concepts such as region and object selection and a selection point or cursor, as well as tools to interact with the document in terms of these concepts.

FIG.1illustrates timeline window100of a DAW which represents audio data as a waveform showing the data's amplitude over time. The example also shows multiple tracks, each of which contain elements organized into one or more lanes. In addition, the example shows an overlay representation of control data on one track, with a black line representing the control value at different points on the timeline overlaid by the DAW onto the presentation of the track's audio waveform presentation. Other kinds of metadata and control presentations are overlaid by the DAW onto the audio data presentation including a name in the top left corner of each audio clip element. On the left-hand side of the example there are a series of control panes representing different aspects of the track's behavior or state such as audio routing settings. In this example, the control panes are fixed to the track display. The timeline display on the right shows a window of a timeline which can be scrolled back and forth to present data associated with different points in time of a media composition. The track property and settings shown on the left are presented regardless of which part of the timeline is currently shown. The DAW may provide the ability for the user to select which settings are shown from among a menu of possibly many different kinds of track settings and property panes.

Plug-in software modules are used to extend certain aspects of the processing capabilities of the host software (e.g., a DAW). In order to work with the host, the plug-ins adhere to a processing application programming interface (API), such as the audio and MIDI data processing APIs VST (Steinberg), Audio Units (Apple®), AAX (Avid®), ARA (Celemony), and Rack Extensions (Propellerhead). An example of a video processing API for a non-linear video editing host application is AVX (Avid).

The user typically interacts with such plug-in software through a dedicated window or pane which displays a graphical user interface (GUI). The plug-in GUI presents data from the plug-in and provides control over the plug-in's parameters and other state. This GUI display may be embedded into a window already provided by the host software and it may be populated with graphical elements provided by the plug-in, pre-built graphical elements from the host, or a combination of the two.FIG.2shows an example of DAW GUI202with a plug-in presentation contained in separate window204. Thus, in such a plug-in-augmented media editing application, the plug-in's GUI is presented as its own interactive entity, which is separated, both conceptually and spatially on the display, from the media data being processed.

A DAW may also present data which is derived from the plug-in within the context of the track display. For example, pitch information derived from a plug-in may be drawn by a DAW as lines representing pitch information for the audio data contained on a track, as shown in diagrammatic screenshot300illustrated inFIG.3. In this example, horizontal lines representing the pitches and durations of individual notes in top region302of user interface window300are controlled by the host software while bottom part304of the window is controlled by the plug-in. A DAW may also draw its own representation of the plug-in's controls over the track data in region302. In such cases, region302remains fully under the control of the DAW and is implemented as part of the DAW software. Furthermore, the host software presents the data in one form, with one set of tools for manipulating its document data, while the plug-in presents the data in another form, with another set of tools for manipulating or viewing its data with the result that the user's interactions with the plug-in are separated from their interactions with the host and with the media document.

While this user interface separation may adequately support the use of some traditional audio processing plug-ins such as EQs and compressors, for many other kinds of plug-in this separation does not result in the most effective user workflow. In particular, some plug-ins present information involving a temporal range of data derived from the host document. For example, plug-in pane304displays pitch data associated with a temporal range of the host document's track media. Despite the fact that both the plug-in and host software are presenting information based on the same document data, the respective presentations present the data with different temporal scales and temporal ranges. Furthermore, user interactions with the respective representations of the document data require the use of different user interaction functions such as different editing tools and key commands. Users cannot view or operate on the data presented by the plug-in in the same way and in the same space as they can view or operate on the data presentations which are integrated into the host software's track view.

Thus, current editing workflows involving the use of plug-ins require the user to switch frequently between different spatial regions of a user interface, different data representations, different data manipulation tools, and different temporal scales. There is therefore a need to improve the experience of a user who is editing a media composition with a host media editing application in conjunction with one or more plug-in software modules.

SUMMARY

In general, plug-ins are able to generate and display custom representations of a media composition directly within one or more portions of the interface of a host editing application user interface. In various implementations, a host-generated timeline representation of media data of a media composition is replaced, augmented, or overlaid by the custom representation generated by a plug-in.

In general, in one aspect, a method of displaying media data of a media composition comprises: providing a host application for editing the media composition, wherein the host application generates a graphical user interface that includes a representation of the media composition, the representation including a portion displaying a timeline representation of a time-based element of the media composition; and providing a plug-in software module that interfaces with the host application, wherein the plug-in, when executing in conjunction with the host application: generates a custom representation of media data of the time-based element of the media composition; and displays the custom representation of the media data of the time-based element of the media composition within the portion of the graphical user interface generated by the host application representing the time-based element.

Various embodiments include one or more of the following features. Enabling an operator to perform an edit operation on the media composition based on the custom representation of the time-based element of the media composition. The edit operation uses editing functionality provided by the plug-in. The edit operation is performed by interacting with the custom representation of the time-based element. The edit operation includes selecting a portion of the media data represented by the custom representation. The edit operation is applied to media data that appears as a distinct feature within the custom representation. The custom representation assists the operator to focus on a portion of the time-based element upon which the edit operation is to be performed. The edit operation includes selecting an editing tool provided by the plug-in from a tool palette displayed within a region of the host application user interface associated with the timeline representation of the time-based element of the media composition. The edit operation is performed with a standard editing tool of the host application. The edit operation is performed using an editing tool of the host application whose function is modified by the plug-in. The custom representation of the time-based element of the media composition is a spectrogram representation and the edit operation includes altering audio of the time-based media element by selecting a portion of the time-based element in the spectrogram representation. The time-based element is an audio clip, and the edit operation includes altering pitch content within the audio clip. The custom representation augments a display generated by the host application within the portion of the graphical user interface displaying a timeline representation of a time-based element of the media composition. The custom representation replaces a display generated by the host application within the portion of the graphical user interface displaying a timeline representation of a time-based element of the media composition. The time-based element of the media composition is a media track or a media clip. The plug-in further generates and displays material within a portion of the graphical user interface generated by the host application external to and associated with the timeline representation of a time-based element of the media composition. The custom representation of the time-based element of the media composition is a spectrogram representation. The time-based element is an audio clip, and the custom representation includes the display of temporally aligned text derived from the audio clip. The time-based element is an audio clip, and the custom representation includes a representation of pitch content of the media data. The host application is one of a digital audio workstation application, a non-linear video editing application, and a music notation application.

In general, in another aspect, a computer program product comprises: a non-transitory computer-readable medium with computer-readable instructions encoded thereon, wherein the computer-readable instructions, when processed by a processing device instruct the processing device to perform a method of displaying media data of a media composition, the method comprising: providing a host application for editing the media composition, wherein the host application generates a graphical user interface that includes a representation of the media composition, the representation including a portion displaying a timeline representation of a time-based element of the media composition; and providing a plug-in software module that interfaces with the host application, wherein the plug-in, when executing in conjunction with the host application: generates a custom representation of media data of the time-based element of the media composition; and displays the custom representation of the media data of the time-based element of the media composition within the portion of the graphical user interface generated by the host application representing the time-based element.

In general, in a further aspect, a computer program product comprises: a memory for storing computer-readable instructions; and a processor connected to the memory, wherein the processor, when executing the computer-readable instructions, causes the system to perform a method of displaying media data of a media composition, the method comprising: providing a host application for editing the media composition, wherein the host application generates a graphical user interface that includes a representation of the media composition, the representation including a portion displaying a timeline representation of a time-based element of the media composition; and providing a plug-in software module that interfaces with the host application, wherein the plug-in, when executing in conjunction with the host application: generates a custom representation of media data of the time-based element of the media composition; and displays the custom representation of the media data of the time-based element of the media composition within the portion of the graphical user interface generated by the host application representing the time-based element.

",utility,2024-06-25,Embedded plug-in presentation and control of time-based media documents,B2,24.0,,"AVID TECHNOLOGY, INC.","G10L 21/12, G06F 3/482, G06F 9/445, G11B 27/2, G11B 27/31","G10L21/12,G06F3/0481,G06F3/0482,G06F9/44526,G10H1/0008,G11B27/02,G11B27/031,G11B27/34,G10H2210/066,G10H2220/101,G10H2220/116,G10H2250/235,G10L21/013","inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,additional,additional,additional,additional,additional","Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements-Input arrangements or combined input and output arrangements for interaction between user and computer -Interaction techniques based on graphical user interfaces [GUI]-based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance, Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements-Input arrangements or combined input and output arrangements for interaction between user and computer -Interaction techniques based on graphical user interfaces [GUI]-based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance-Interaction with lists of selectable items, e.g. menus, Arrangements for program control, e.g. control units -using stored programs, i.e. using an internal store of processing equipment to receive or retain programs-Arrangements for executing specific programs-Program loading or initiating -Dynamic linking or loading; Link editing at or after load time, e.g. Java class loading-Plug-ins; Add-ons, Details of electrophonic musical instruments-Associated control or indicating means, Aspects or methods of musical processing having intrinsic musical character, i.e. involving musical theory or musical parameters or relying on musical knowledge, as applied in electrophonic musical tools or instruments -Musical analysis, i.e. isolation, extraction or identification of musical elements or musical parameters from a raw acoustic signal or from an encoded audio signal -for pitch analysis as part of wider processing for musical purposes, e.g. transcription, musical performance evaluation; Pitch recognition, e.g. in polyphonic sounds; Estimation or use of missing fundamental, Input/output interfacing specifically adapted for electrophonic musical tools or instruments-Graphical user interface [GUI] specifically adapted for electrophonic musical instruments, e.g. interactive musical displays, musical instrument icons or menus; Details of user interactions therewith -for graphical creation, edition or control of musical data or parameters, Input/output interfacing specifically adapted for electrophonic musical tools or instruments-Graphical user interface [GUI] specifically adapted for electrophonic musical instruments, e.g. interactive musical displays, musical instrument icons or menus; Details of user interactions therewith -for graphical creation, edition or control of musical data or parameters-for graphical editing of sound parameters or waveforms, e.g. by graphical interactive control of timbre, partials or envelope , Aspects of algorithms or signal processing methods without intrinsic musical character, yet specifically adapted for or used in electrophonic musical processing -Mathematical functions for musical analysis, processing, synthesis or composition -Transforms, i.e. mathematical transforms into domains appropriate for musical signal processing, coding or compression-Fourier transform; Discrete Fourier Transform [DFT]; Fast Fourier Transform [FFT], Speech or voice signal processing techniques to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility -Changing voice quality, e.g. pitch or formants-characterised by the process used-Adapting to target pitch, Speech or voice signal processing techniques to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility -Transformation of speech into a non-audible representation, e.g. speech visualisation or speech processing for tactile aids -Transforming into visible information-by displaying time domain information, Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel-Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers, Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel-Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers-Electronic editing of digitised analogue information signals, e.g. audio or video signals, Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel-Indexing; Addressing; Timing or synchronising; Measuring tape travel-Indicating arrangements ","COMPUTING; CALCULATING OR COUNTING, MUSICAL INSTRUMENTS; ACOUSTICS, INFORMATION STORAGE","ELECTRIC DIGITAL DATA PROCESSING , ELECTROPHONIC MUSICAL INSTRUMENTS; INSTRUMENTS IN WHICH THE TONES ARE GENERATED BY ELECTROMECHANICAL MEANS OR ELECTRONIC GENERATORS, OR IN WHICH THE TONES ARE SYNTHESISED FROM A DATA STORE, SPEECH ANALYSIS TECHNIQUES OR SPEECH SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING TECHNIQUES; SPEECH OR AUDIO CODING OR DECODING, INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER ",2.0,8.0,"DETAILED DESCRIPTION

There is a large ecosystem of plug-in software modules that operate in conjunction with a host media processing application to add functionality to the host. As described above, the user interfaces, data representations, and manipulation tools of plug-ins have been kept conceptually and spatially separate from those of the host application.

We describe herein methods of integrating the user interfaces of the host and of a plug-in by allowing the host software to embed the plug-in's GUI directly into its track, clip, or other element or document data structure presentation, either as a complete replacement of the host's presentation of the data or as an overlay which decorates the existing host presentation. The examples presented herein are based on a DAW as the host, but the methods also apply to any media processing application, including non-linear video editing applications and music notation applications. As used herein, media processing application, media editing application, and media composition application are synonymous and used interchangeably. An example of a DAW is Pro Tools®, a product of Avid® Technology, Inc., of Burlington, Massachusetts. An example of a non-linear video editing application is Media Composer®, also a product of Avid Technology, Inc. An example of a music notation application is Sibelius®, also a product of Avid Technology, Inc. As used herein, the term “document” refers to what is being edited, and may refer to a media composition that includes one or more of audio, video, text, and metadata, or to a musical score.

When using this technique, the plug-in logic has control over the same display regions as those which are used by the host software to present details about the document. This allows the plug-in to provide a useful customized representation of the document data, either replacing or augmenting the existing presentation from the host software.

In addition to presenting a view of the document data which is appropriate to the processing being applied or to the analysis which has been requested, standard host tools with their normal functions may be used on a track which includes graphical elements presented by a plug-in. For example, editing operations such as object selection, cuts, and transitions may be performed on a representation of track data within the timeline supplied by a plug-in.

To further facilitate control over the document being edited, the embedding of a plug-in user interface may allow the plug-in to customize the effects of the host tools. For example, in a host that supports a crossfade tool for normal document editing, a plug-in may provide its own custom transitional effect when this crossfade tool is applied to media or track data which is being presented by the plug-in.

Plug-ins may also provide their own custom tools that have no analog in the host. Access to and selection of these tools may be embedded within a region or menu controlled by the host application, into a region controlled by the plug-in, or both. These custom tools may be associated with one or more specific tracks or with the overall document.

Embedding of a plug-in user interface within that of a host media processing application may require creation or extension of a plug-in API with a suitable interface for an embedded GUI. In addition to standard GUI API considerations such as the providing the plug-in with utilities for drawing into the appropriate regions, the interface may include additional features such as a means of identifying the portion of the document data which is currently displayed, randomly accessing the underlying document data, and communicating normal host document editing commands. To this end, when augmenting a timeline view with a plug-in presentation, the interface between the host and the plug-in requires at a minimum: drawing utilities; information about track media; and information about the currently visible timeline region. Other information that may be used by the interface includes: information about track objects; information about the current host timeline selection; play/stop bounds and other global timeline features; and requests for changes to the host data model.

FIG.4shows a user interface of a DAW showing exemplary regions associated with a timeline where portions of a plug-in's user interface may be integrated within that of the host DAW. Region 1402is the area in which a representation of a track's audio data is shown on a timeline. The embedding within the timeline provides an in-context view of the track data that is appropriate to the processing which will be applied by the plug-in or to the analysis by the plug-in which has been requested. Such an in-context view involves the matching of the temporal scale and temporal range within the document shown by the host with that shown by the plug-in. Such temporal synchronization is not present when using traditional plug-ins with host applications, such as those whose user interface is confined to a discrete window superimposed on or adjacent to the host user interface.

A plug-in with an embedded interface may also draw into region 2404. A presentation in this region remains fixed for the track and represents data or tools which apply to the whole track or to a particular region within the track, such as to the current timeline selection. This contrasts with the presentation in region 1, which reflects the media on the track at the respective location in time which is indicated by screen and document scrolling position. One use of this region is to present an overall analysis for the entire track, such as an overall peak loudness measurement. Another use of this region is to present a custom tool palette offering the operator custom editing tools that affect the operator's interactions with the track's timeline data elements. For example, an “enhance pen” tool may be used to draw in regions on the timeline where a vocal articulation enhancement effect is to be applied. The identification of the regions for the enhancement effect may be facilitated by an appropriate custom presentation of the audio data within the track in region 1, for example of a spectrogram presentation.

In another example, the plug-in may draw into a new region that is associated with tools to edit the document as a whole, such as the region visible along the top ofFIG.1. This indicates to the operator that the selected tool may be used for actions taken on any track.

Region 3406illustrates extension of host menus in order to provide the ability to “assign” a plug-in to a particular track and indicates the plug-in that is currently selected for this track, i.e., “Plugin Name”408. Equivalent region 3410for the track displayed above the Audio2track inFIG.4shows an expanded menu with the plug-in (“Plugin Name”412) added to the standard host-provided track view options as a choice for assignment. The custom presentation modes and their names are provided by the plug-in and inserted by the plug-ins into the corresponding extensible list of available presentation modes.

While plug-ins may utilize regions 1, 2, and 3 traditionally controlled exclusively by the host application, these regions continue to be available to the host application for the display of host-generated data or commands. For example, in region 1, the host may overlay a clip name onto a plug-in generated custom view of clip data.

FIG.5illustrates a plug-in embedded in a DAW user interface that utilizes regions 1, 2, and 3 ofFIG.4. The specific plug-in shown is an example of a plug-in used for altering the audio data, in this case for performing audio repair operations. In region 1, the in-track representation502of the data within the DAW timeline is generated by the plug-in. This is accomplished via an API which provides the plug-in with utilities for drawing into the appropriate regions as well as access to a variety of data about the track. At minimum, the position and contents of track media must be provided by the host. Additional features of the API may include facilities to apply changes to the media data or to data derived from the media data, and facilities to provide additional data to the plug-in. Such additional data provided to the plug-in may include: facilities to apply changes to the media data; data about the currently visible timeline region; track object information such as media clip names and boundaries; timeline information such as selection boundaries, cursor position, timeline markers, tempo changes, and play/stop points; and track data such as name, type, and display color. Here, the in-track presentation represents the audio data as a detailed spectrogram display with the vertical position within the timeline indicating frequency, and the brightness and hue representing intensity at that frequency for each vertical time slice. With the representation of the data appropriate to the audio repair function appearing within the DAW track itself, the operator is able to focus on the task directly within the context of the DAW track. Accompanying the spectrogram display, the plug-in provides custom tool palette504, which may be displayed adjacent to the track's timeline data presentation. This allows the operator to select a tool and make specific edits and repairs to the audio by viewing and interacting with the audio data directly within the track without the need for the operator to shift attention to a separate plug-in window or pane. For example, the operator may examine the spectrogram display to identify an anomalous region in the audio data, use a frequency-time-axis two-dimensional selection tool to specify the unwanted frequencies in this region, then use a selection-based effect to suppress the energy in the selected frequencies at the desired points in time. The region thus defined is then corrected by the audio repair plug-in. For example, track502shows three notes played on a guitar with some unwanted string squeaking sounds visible as a series of anomalous high-frequency energy in the upper half of the in-track spectrogram view between time offsets of about 0:45 to 1:05. Using custom tool palette504, the operator selects a rectangular selection tool and positions it over the anomalous region to specify the region to be repaired by the audio repair plug-in, as indicated by rectangle506. Clip name508, provided by the host, is overlaid onto the custom spectrogram view. When the repair is complete, an operator may change the track view back to a more basic view of the audio data, such as a waveform view. Alternatively, rather than revert back to a host-generated view of the audio data after a plug-in mediated function is performed, a plug-in generated custom view may be retained as a layer under or over the host-generated view. For example, the standard waveform view generated by the host may be layered on top of the spectrogram view ofFIG.5,502.

Other audio-specific examples of a custom representation of document data presented by a plug-in within the host's representation include a customized waveform view, a view of audio data displayed as pitch data, a customized spectrogram display of audio data, an overlay highlighting regions of excessive loudness, a display of spatial positioning for a multi-channel audio track, a text overlay labeling each clip with certain metadata, an overlay indicating possible audio artifacts, and an overlay for dialog showing a transcript derived from the audio. Each of these custom representations may be accompanied by a corresponding palette of custom editing tools for editing or analyzing audio data directly within the context of the audio document. In general, a custom representation enhances the presentation of features of the media data in a way that is useful to the operator. For example, the custom representation may: enhance the presentation of features of the media data which are useful to an operator for evaluating the effect of the data on the composition; highlight one or more portions of a time-based element that should be brought to the operator's attention; or present the media data in terms of features over which the plug-in provides some unique aspect of editing control. As a specific example, a plug-that shows audio data as notes on a score enhances the presentation of pitch features of the media data in a way that is useful for evaluation of the data's pitch content. If this plug-in also provides audio editing functions such as grabbing and dragging note objects to different pitch values, then the presentation of the data is being made in terms of pitch features over which the plug-in provides a unique form of control.

FIG.6illustrates customized plug-in-generated spectrogram display602of the audio data within a track with host-provided clip names such as clip name604overlaid onto the spectrogram. In this example the plug-in performs an analysis of the audio and generates the spectrogram representation, which may be useful for analyzing certain kinds of audio data, particularly vocalizations or mixtures of audio signals with different characteristics, such as a recording of a musical instrument playing in a noisy environment. As described above in connection withFIG.5, such a visualization may be provided by a plug-in that provides a spectral repair tool. InFIGS.7and8embedded plug-in-generated overlays702and802respectively highlight regions of excessive loudness. The identification of such “hot spots” within the context of the whole composition may be much more useful to an operator than when such data is presented in a graphical region which is separated from the region used to present the composition. InFIG.9, an embedded plug-in-generated overlay shows transcript text902derived from the corresponding audio speech track.

Another example of the described embedded plug-in presentation is an analysis plug-in that draws the operator's attention to abrupt clip transitions. The plug-in indicates points directly on the host's presentation of the timeline and the user executes a normal host operation, such as inserting a fade, onto a track which is showing the plug-in presentation of the track data.

A different plug-in-generated customization may be used for each of several different tracks or lanes within the host software depending on the data contained in these tracks and on the operator's current requirements for reviewing, comparing, or editing this data.FIG.10illustrates such a multi-representational timeline with each of four tracks1002,1004,1006, and1008. In this example, an operator is able to simultaneously review an audio-to-text display of spoken dialog in track1008while performing loudness correction on background music tracks1002and1006and spectral analysis on sound effect track1004. The task may be performed entirely within the context of the audio document without the need to shift the operator's focus to one or more separate plug-in user interface windows or panes.

In a further integration of a plug-in user interface with that of the host application, document editing tool actions from the host application can be applied in the context of the embedded plug-in display. For example, a “grabber” tool which is normally used to select and move metadata objects such as volume automation handles in the timeline may instead, when applied within the custom track view of a pitch correction plug-in, be used to change the pitch of an audio object displayed within this view. In another example a “scissors” tool which would normally be used to separate a single clip into two clips may be manipulated by an effect plug-in to apply a specially processed transition effect between the separated clips.

As indicated above, the described methods enable a plug-in presentation to be layered over the existing host presentation or vice-versa. The overlay may be applied more than once in order to layer the information from multiple plug-ins onto a single track presentation. For example, an audio-to-text presentation may be layered over a spectrogram presentation from an audio correction plug-in to provide a useful reference when performing audio corrections on the underlying data. In this case the operator may use a tool palette provided by the audio correction plug-in to make edits to the track, such as a “lasso” tool to select and correct a particular burst of high energy in a certain frequency range at a particular point on the timeline, while simultaneously referencing the corresponding dialog in the track overlay from the audio-to-text presentation plug-in.

The methods described herein may be used to integrate plug-in presentations with a host non-linear video editing application. For example, a plug-in that analyzes a video clip or track for illegal colors, such as those that lie outside the gamut of a particular color representation, may highlight regions containing illegal colors directly onto the host timeline representation. The highlighting may be in the form of icons overlaid by the plug-in onto the timeline, e.g., a green check mark for legal regions, a yellow triangle to warn that colors are nearing the edge of the legal range, and a red stop sign indicating colors that have exceeded the legal limits. Other highlighting methods the plug-in may use include applying shading to a waveform displayed in the timeline or drawing lines spanning temporal ranges within the timeline corresponding to the illegal portions. Similarly, a plug-in that analyzes an audio track for regions having excessively high volume levels that may result in audio clipping is able to highlight such regions directly within the timeline.

The methods described herein may also be used with a music notation application as the host application. In a manner analogous to that described above in connection with digital audio workstations, a plug-in may embed a custom representation, a graphical element such as an icon, or other material directly into a region of the notation application used for displaying a musical score. A common type of plug-in for music notation applications is one that is invoked to generate the sound of a musical instrument during playback of a MIDI file corresponding to the score. Such plug-ins may have their own custom controls, such as for volume and expression, which may be overlaid on to the score to show the relationship between the actions of the plug-in and the source score material. For example, if an effect is to be applied to a specific portion of the score, the plug-in may expose its state and controls directly within the score in association with the specific portion. Interfaces for custom plug-in generated controls may be integrated with or replace those generated by the host within other regions of the host interface as well. Such regions include but are not limited to a mixer window where the plugin may provide additional mixing controls and signal level indicators for the audio channels they control, an inspector window, and an ideas window.

The various components of the system described herein may be implemented as a computer program using a general-purpose computer system. Such a computer system typically includes a main unit connected to both an output device that displays information to an operator and an input device that receives input from an operator. The main unit generally includes a processor connected to a memory system via an interconnection mechanism. The input device and output device also are connected to the processor and memory system via the interconnection mechanism.

One or more output devices may be connected to the computer system. Example output devices include, but are not limited to, liquid crystal displays (LCD), plasma displays, various stereoscopic displays including displays requiring viewer glasses and glasses-free displays, cathode ray tubes, video projection systems and other video output devices, loudspeakers, headphones and other audio output devices, printers, devices for communicating over a low or high bandwidth network, including network interface devices, cable modems, and storage devices such as disk, tape, or solid state media including flash memory. One or more input devices may be connected to the computer system. Example input devices include, but are not limited to, a keyboard, keypad, track ball, mouse, pen and tablet, touchscreen, camera, communication device, and data input devices. The invention is not limited to the particular input or output devices used in combination with the computer system or to those described herein.

The computer system may be a general-purpose computer system, which is programmable using a computer programming language, a scripting language or even assembly language. The computer system may also be specially programmed, special purpose hardware. In a general-purpose computer system, the processor is typically a commercially available processor. The general-purpose computer also typically has an operating system, which controls the execution of other computer programs and provides scheduling, debugging, input/output control, accounting, compilation, storage assignment, data management and memory management, and communication control and related services. The computer system may be connected to a local network and/or to a wide area network, such as the Internet. The connected network may transfer to and from the computer system program instructions for execution on the computer, media data such as video data, still image data, or audio data, metadata, review and approval information for a media composition, media annotations, and other data.

A memory system typically includes a computer readable medium. The medium may be volatile or nonvolatile, writeable or nonwriteable, and/or rewriteable or not rewriteable. A memory system typically stores data in binary form. Such data may define an application program to be executed by the microprocessor, or information stored on the disk to be processed by the application program. The invention is not limited to a particular memory system. Time-based media may be stored on and input from magnetic, optical, or solid-state drives, which may include an array of local or network attached disks.

A system such as described herein may be implemented in software, hardware, firmware, or a combination of the three. The various elements of the system, either individually or in combination may be implemented as one or more computer program products in which computer program instructions are stored on a non-transitory computer readable medium for execution by a computer or transferred to a computer system via a connected local area or wide area network. Various steps of a process may be performed by a computer executing such computer program instructions. The computer system may be a multiprocessor computer system or may include multiple computers connected over a computer network or may be implemented in the cloud. The components described herein may be separate modules of a computer program, or may be separate computer programs, which may be operable on separate computers. The data produced by these components may be stored in a memory system or transmitted between computer systems by means of various communication media such as carrier signals.

Having now described an example embodiment, it should be apparent to those skilled in the art that the foregoing is merely illustrative and not limiting, having been presented by way of example only. Numerous modifications and other embodiments are within the scope of one of ordinary skill in the art and are contemplated as falling within the scope of the invention.

",Robert E. Majors,"Technology, Healthcare, Art & Design,Technology, Media, Healthcare, Artificial Intelligence & Machine Learning,Technology, Media, Retail, Information Storage,Technology, Telecommunications, Consumer Goods, Cybersecurity","Acoustic devices, hearing aids, audio visualization, sound reproduction systems,Data storage, recording devices, optical and magnetic storage, retrieval systems,Digital data processing, computer architecture, error detection, data transfer protocols, secure processing systems,Speech recognition, audio coding, voice processing, speech synthesis, text-to-speech systems","A software plug-in module that interfaces to a media editing host application generates and embeds information about a media composition being edited directly within portions of the user interface generated by the host application. The information may include a custom representation of media data of a time-based element of the media composition that replaces, augments, or overlays a timeline representation of the element generated by the host application. Media editing functionality provided by the plug-in may be accessed by an operator based on viewing or interacting with the custom representation. Results of analysis of the media composition by the plug-in may be displayed within the host-generated timeline and used by an operator as a basis for performing edit operations with standard host tools or with plug-in generated tools. Plug-ins may embed their interfaces within user interfaces of host digital audio workstations, non-linear video editing systems, and music notation applications."
12020724,"BACKGROUND

Speech processing is increasingly used across many industries and in many applications. For example, speech processing may be used to transcribe speech, control a personal computing device, diagnose a patient with a health condition, detect dishonesty, or predict a person's risk with respect to an event or a function. Speech processing algorithms may be most effective when provided with quality, noise-free speech samples.

SUMMARY

The present disclosure provides methods and systems for performing, in real-time, automated quality control of speech samples. The quality control system described herein can determine, among other things, whether the speech in a speech sample (1) is responsive to a query, (2) is the appropriate volume, (3) is the appropriate cadence, and (4) contains background noise. These factors may impact the ability of speech processing algorithms to effectively analyze the speech sample. The automated quality control system can, in real time, improve the likelihood of a user generating a usable speech sample by providing specific feedback to the user regarding the quality of the speech sample. Use of the automated quality control system described herein may reduce the likelihood of a speech processing algorithm failing to properly process a speech sample due to the speech sample's quality, improving the efficacy of the speech processing algorithm. This may reduce or prevent the need to collect second speech samples. Additionally, providing specific, real-time feedback to a user regarding the quality of the user's speech sample may be unconventional. Other systems may instead rely on post-processing or manual human review of speech samples.

In an aspect, the present disclosure provides a computer-implemented method, comprising: (a) obtaining a first speech sample from a user; (b) determining that the speech sample does not satisfy at least one quality control requirement of a plurality of quality control requirements selected from the group consisting of a missing response, vocal volume, speed, and a presence or absence of background noise, wherein each quality control requirement of the plurality of quality control requirements is associated with an instruction that indicates (i) that the quality control requirement was not satisfied, and (ii) how to satisfy the quality control requirement in a second speech sample; (c) providing the instruction associated with the quality control requirement to the user; and (d) prompting the user to provide the second speech sample.

In some embodiments, each quality control requirement of the plurality of quality control requirements is associated with a different priority rating. In some embodiments, the method further comprises determining that the first speech sample does not satisfy two or more quality control requirements of the plurality of quality control requirements and providing the instruction associated with a highest priority rating of the different priority ratings. In some embodiments, the quality control requirements have a priority rating, from highest to lowest, of a missing response, too low of vocal volume, too fast of a response, to slow of a response, too loud of a response, and a presence of background noise. In some embodiments, the method further comprises repeating (a)-(d) to prompt the user to provide a third speech sample. In some embodiments, (a)-(d) is repeated less than three times. In some embodiments, the method further comprises repeating (a)-(c) for the second speech sample. In some embodiments, where the second speech sample is determined not to satisfy the at least one quality control requirement, the second speech sample is discarded. In some embodiments, the first speech sample is a portion of an automated interview. In some embodiments, the automated interview is an automated telephone interview. In some embodiments, the automated interview is configured to determine a level of risk of the user. In some embodiments, where the first speech sample is determined to satisfy the at least one quality control requirement, skipping (c) and (d). In some embodiments, the determining is performed in real-time. In some embodiments, the providing the instruction comprises providing an auditory instruction to the user. In some embodiments, the first speech sample is less than 5 seconds long. In some embodiments, (b) further comprises determining that the first speech sample satisfies each quality control requirement of the plurality of quality control requirements.

In another aspect, the present disclosure provides a system, comprising: one or more computer processors operatively coupled to computer memory, wherein the one or more computer processors are individually or collectively configured to (a) determine that a first speech sample of a user does not satisfy at least one quality control requirement of a plurality of quality control requirements selected from the group consisting of a missing response, vocal volume, vocal speed, and a presence or absence of background noise, wherein each quality control requirement of the plurality of quality control requirements is associated with an instruction that indicates: (i) that the quality control requirement was not satisfied, and (ii) how to satisfy the quality control requirement in a second speech sample; (b) provide the instruction associated with the quality control requirement to the user; and (c) prompt the user to provide the second speech sample.

In another aspect, the present disclosure provides one or more non-transitory computer storage media storing instructions that are operable, when executed by one or more computers, to cause the one or more computers to perform operations comprising (a) obtaining a first speech sample from a user; (b) determining that the first speech sample satisfies at least one quality control requirement of a plurality of quality control requirements selected from the group consisting of a missing response, vocal volume, vocal speed, and a presence or absence of background noise, wherein each quality control requirement of the plurality of quality control requirements is associated with an instruction that indicates (i) that the quality control requirement was not satisfied, and (ii) how to satisfy the quality control requirement in a second speech sample; (c) providing the instruction associated with the quality control requirement to the user; and (d) prompting the user to provide the second speech sample.

Another aspect of the present disclosure provides a non-transitory computer readable medium comprising machine executable code that, upon execution by one or more computer processors, implements any of the methods above or elsewhere herein.

Another aspect of the present disclosure provides a system comprising one or more computer processors and computer memory coupled thereto. The computer memory comprises machine executable code that, upon execution by the one or more computer processors, implements any of the methods above or elsewhere herein.

Additional aspects and advantages of the present disclosure will become readily apparent to those skilled in this art from the following detailed description, wherein only illustrative embodiments of the present disclosure are shown and described. As will be realized, the present disclosure is capable of other and different embodiments, and its several details are capable of modifications in various obvious respects, all without departing from the disclosure. Accordingly, the drawings and description are to be regarded as illustrative in nature, and not as restrictive.

INCORPORATION BY REFERENCE

All publications, patents, and patent applications mentioned in this specification are herein incorporated by reference to the same extent as if each individual publication, patent, or patent application was specifically and individually indicated to be incorporated by reference. To the extent publications and patents or patent applications incorporated by reference contradict the disclosure contained in the specification, the specification is intended to supersede and/or take precedence over any such contradictory material.

",utility,2024-06-25,Methods and systems for audio sample quality control,B2,19.0,,Clearspeed Inc.,"G10L 25/60, G10L 25/63, G10L 25/84, H04M 3/22","G06Q10/0635,G10L25/60,G06Q10/06395,G06Q30/01,G06Q30/02,G06Q50/10,G10L25/63,G10L25/84,H04M3/2227,H04M3/2236","inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional","Administration; Management-Resources, workflows, human or project management; Enterprise or organisation planning; Enterprise or organisation modelling-Operations research, analysis or management-Risk analysis of enterprise or organisation activities, Administration; Management-Resources, workflows, human or project management; Enterprise or organisation planning; Enterprise or organisation modelling-Operations research, analysis or management-Performance analysis of employees; Performance analysis of enterprise or organisation operations-Quality analysis or management, Commerce-Customer relationship services, Commerce-Marketing; Price estimation or determination; Fundraising, Information and communication technology [ICT] specially adapted for implementation of business processes of specific business sectors, e.g. utilities or tourism -Services, Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 -specially adapted for particular use-for comparison or discrimination-for measuring the quality of voice signals, Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 -specially adapted for particular use-for comparison or discrimination-for estimating an emotional state, Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 -Detection of presence or absence of voice signals -for discriminating voice from noise, Automatic or semi-automatic exchanges-Arrangements for supervision, monitoring or testing-Quality of service monitoring, Automatic or semi-automatic exchanges-Arrangements for supervision, monitoring or testing-Quality of speech transmission monitoring","COMPUTING; CALCULATING OR COUNTING, MUSICAL INSTRUMENTS; ACOUSTICS, ELECTRIC COMMUNICATION TECHNIQUE","INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR, SPEECH ANALYSIS TECHNIQUES OR SPEECH SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING TECHNIQUES; SPEECH OR AUDIO CODING OR DECODING, TELEPHONIC COMMUNICATION ",0.0,12.0,"DETAILED DESCRIPTION

While various embodiments of the invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only. Numerous variations, changes, and substitutions may occur to those skilled in the art without departing from the invention. It should be understood that various alternatives to the embodiments of the invention described herein may be employed.

Whenever the term “at least,” “greater than,” or “greater than or equal to” precedes the first numerical value in a series of two or more numerical values, the term “at least,” “greater than” or “greater than or equal to” applies to each of the numerical values in that series of numerical values. For example, greater than or equal to 1, 2, or 3 is equivalent to greater than or equal to 1, greater than or equal to 2, or greater than or equal to 3.

Whenever the term “no more than,” “less than,” or “less than or equal to” precedes the first numerical value in a series of two or more numerical values, the term “no more than,” “less than,” or “less than or equal to” applies to each of the numerical values in that series of numerical values. For example, less than or equal to 3, 2, or 1 is equivalent to less than or equal to 3, less than or equal to 2, or less than or equal to 1.

In an aspect, the present disclosure provides a computer-implemented method. The computer-implemented method may comprise obtaining a first speech sample from a user. The speech sample may be determined to not satisfy at least one quality control requirement of a plurality of quality control requirements. The plurality of quality control requirements may comprise a missing response, a vocal volume, a vocal speed, and a presence or absence of background noise. Each quality control requirement of the plurality of quality control requirements may be associated with instructions. The instructions may indicate that the quality control requirement was not satisfied. The instructions may indicate how to satisfy the quality control requirement in a second speech sample. The instruction associated with the quality control requirement may be provided to the user. The user may be prompted to provide the second speech sample. Though described herein with respect to the method, the following may be applied to systems and non-transitory computer readable instructions as described elsewhere herein.

FIG.1is a flow chart of a process100for quantifying a speech sample, according to an embodiment. The process100can be performed by a system of one or more appropriately programmed computers in one or more locations.

In an operation of the process100, the system can obtain a first speech sample from a user (110). The first speech sample may be a portion of an automated interview. The automated interview may be an interview performed without the aid of a human operator. For example, the automated interview may involve a user answering questions presented by the system. The automated interview may be an automated telephone interview, an automated online interview, or the like. For example, in one embodiment, the user can use a telephone to connect to the system that conducts the interview. The automated interview may be configured to determine a level of risk of the user. The first speech sample may be encrypted. For example, the first speech sample may be encrypted to reduce a likelihood that the first speech sample is accessible to a non-intended recipient.

The first speech sample may be at least about 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 9.5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 30, 40, 50, 60, or more seconds long. The first speech sample may be at most about 60, 50, 40, 30, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9.5, 9, 8.5, 8, 7.5, 7, 6.5, 6, 5.5, 5, 4.5, 4, 3.5, 3, 2.5, 2, 1.5, 1, 0.5, or less seconds long. The first speech sample may have a length in a range as defined by any two of the proceeding values. For example, the first speech sample may be about 3-5 seconds long.

In another operation of the process100, the system can determine that the speech sample does not satisfy at least one quality control requirement of a plurality of quality control requirements (120). The plurality of quality control requirements may comprise a missing response, a vocal volume, a vocal speed, and a presence or absence of background noise. Each requirement of the plurality of quality control requirements may be associated with an instruction. The instruction may indicate that the requirement was not satisfied. The instruction may further indicate how to satisfy the requirement in a second speech sample.

The missing response requirement may require that the speech sample contain responsive speech. For example, if the user does not recite a response after being prompted to, the speech sample may not satisfy the missing response requirement. In another example, if a user provides a proper response to a question, and the response is recorded in the first speech sample, the missing response requirement may be met and the user may not be asked to provide an additional speech sample. The missing response requirement may not be met by a response that does not comprise a predetermined response. For example, a user responding “maybe” to a question with predetermined response options of “yes” or “no” may not meet the missing response requirement.

The instruction associated with the missing response requirement may comprise an indication to the user that the response was not detected in the first speech sample. For example, the system can tell the user “The answer was not detected” in response to providing a first speech sample in which an answer was not detected. The instruction associated with the missing response requirement may comprise an indication of the acceptable answers for the first speech sample. For example, in a yes or no question, the instruction may comprise “The system did not hear all of your responses. Please make sure you answer all of the questions with either a yes or no.”

The vocal volume requirement may comprise a requirement that a response submitted in the first speech sample of the user have a volume above a maximum value and/or a volume below a minimum value. The maximum and/or minimum value of the volume signal may be at least about −90, −80, −70, −60, −55, −50, −45, −40, −35, −30, −25, −20, −15, −10, −5, 0, 5, 10, 15, 20, or more decibels. The maximum and/or minimum value of the volume signal may be at most about 20, 15, 10, 5, 0, −5, −10, −15, −20, −25, −30, −35, −40, −45, −50, −55, −60, −70, −80, −90, or fewer decibels. The maximum and/or minimum value of the volume signal may be in a range as bounded by any two of the proceeding values. For example, the minimum to maximum range of the volume signal may be from about −35 decibels to about −10 decibels. In this example, a speech sample in which the user speaks at a peak signal value of −45 decibels may not satisfy the minimum vocal volume requirement.

The instruction associated with the vocal volume requirement may comprise an indication to the user that the response was too loud or too soft. For example, the system can tell the user “The answer was too quiet” if the user's first speech sample or a portion thereof had a volume below the minimum threshold. In this example, the threshold limit may be about −35 decibels. In another example, the system can tell the user “The answer was too loud” if the user's first speech sample or a portion thereof had a volume above the maximum threshold. In this example, the threshold limit may be about −10 decibels. The indication may comprise an indication of the degree to which the user is to change the volume of the response. For example, the indication can indicate that the user can double the volume of their response. In another example, the indication can comprise the phrase “The system was not able to hear all of your responses. Please speak louder when you respond yes or no when the questions are asked again” for an instance where the user did not provide a loud enough response. In another example, the indication may comprise the phrase “Your responses were too loud. Please answer each question in a normal speaking volume when the questions are asked again” in an instance where the user's response was too loud.

The speed requirement may comprise a requirement that the first speech sample have a predetermined length and/or be submitted within a predetermined time after a prompt. For example, a speech sample may fail the speed requirement when a user provides an answer that is too short in duration. In another example, a speech sample may fail the speed requirement when the answer in the speech sample is provided too late in the sample. The speed requirement may be a requirement that a response be at least about 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, or more seconds long. The speed requirement may be a requirement that a response be at most about 1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.09, 0.08, 0.07, 0.06, 0.05, or less seconds long. The speed requirement may be a requirement that a response be within a range as defined by any two of the proceeding values. The speed requirement may be a requirement that an answer be provided within at least about 0.1, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, or more seconds from prompting the user to answer. The speed requirement may be a requirement that an answer be provided within at most about 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0.5, or less seconds from prompting the user to answer. The speed requirement may be a requirement that an answer be provided within a range as defined by any two of the proceeding values. The speed requirement may be related to an average speaking rate. For example, the speed requirement may be related to the average speaking rate of a user of a particular language. The speed requirement may be different for different languages.

The instruction associated with the speed requirement may comprise an indication to the user that the response was received too quickly or too slowly in the first speech sample. For example, a user who begins responding prior to the finish of the question may be told that they are answering the questions too quickly. The instruction associated with the speed requirement may comprise an indication to the user that the response was received too late after the user was prompted to respond. For example, a user who waits too long to respond to a question can be told to respond closer to the prompt. In another example, the instruction can comprise the phrase “You are answering the question too quickly. Please answer each question with a yes or no at a normal speaking rate when the questions are asked again” for a user who answered a question too quickly. In another example, the instruction can comprise “You did not answer all the questions immediately after the tone. When the questions are asked again, please answer all questions with either a yes or no immediately after you hear the following tone (play tone marker)” for a user who answered a question too slowly.

The background noise requirement may comprise a requirement that a level of background noise be at or below a predetermined level. The background noise requirement may comprise a requirement that a level of background noise be at or above a predetermined level. The absence of background noise may provide for higher quality speech samples in which the signal from a user's voice is distinguishable from the background. The presence of background noise may be used to determine that a response is a live response and was not pre-recorded. The background noise requirement may not be satisfied if a level of background noise is more than about −90, −80, −70, −60, −55, −50, −45, −40, −35, −30, −25, −20, −15, −10, −5, 0, 5, 10, 15, 20, or more decibels. The background noise requirement may not be satisfied if a level of background noise is at most about 20, 15, 10, 5, 0, −5, −10, −15, −20, −25, −30, −35, −40, −45, −50, −55, −60, −70, −80, −90, or fewer decibels. For example, the background noise requirement may require a background noise of less than about −50 decibels.

The instruction associated with the background noise requirement may comprise an indication to the user to adjust the user's surroundings to minimize background noise. For example, the indication can inform the user to move to a quieter location. The indication may comprise an instruction to the user to notify an interviewer of the user's situation. For example, the user can be prompted to inform an interviewer of the reason for the high background noise level. In this example, the interviewer can utilize this information to model and reduce the background noise coming from the identified source. In another example, the instruction may comprise “The system detected unknown background sounds during the interview. If the location you are taking the interview from is noisy, please advise the Interview Supervisor. Otherwise, please continue the interview and answer each question with either a yes or no when the questions are asked again” for a user who had too much background noise in their speech sample.

Each requirement of the plurality of quality control requirements may be associated with a different priority rating. The priority rating may be an indication of the importance of correcting a given requirement. For example, a requirement with a higher priority rating may be of a greater importance to fix than a quality control requirement of a lower priority rating. The priority rating may be dynamic. For example, the priority rating may change over time and for different speech samples. In this example, as a speech analysis program develops, the priorities may change as the program changes weights of various factors. The priority rating may be fixed. For example, the priority rating may be set and remain unchanged.

When the first speech sample does not satisfy two or more quality control requirements, the quality control requirement with the highest priority may be prioritized over a quality control requirement with a lower priority. The prioritization may comprise providing the instructions associated with the higher priority quality control requirement. For example, if a first quality control requirement with a priority of 1 (e.g., highest priority) and a second quality control requirement with a priority of 2 (e.g., a lower priority than the first quality control requirement) are not satisfied in a first speech sample, the instructions associated with the first quality control requirement can be provided to the user while the instructions associated with the second quality control requirement may not be provided to the user. The quality control requirements may be prioritized by the effect each has on the ability of a vocal processing algorithm to process the speech sample. For example, a higher priority quality control requirement may be higher priority because it has a larger impact on the efficacy of a vocal processing algorithm. In this example, a missing response can have a higher priority than too much background noise, as a missing response cannot be analyzed at all while a response with too much background noise has the ability to be processed. The quality control requirements may be prioritized by ease of addressing the requirement. For example, vocal volume can be higher priority than background noise, as a user may have the ability to adjust the volume of their voice but be unable to remove themselves from a location with high background noise. The quality control requirements may be prioritized in different ways for different users. For example, a user performing the interview at an industrial plant may have a lower priority background noise requirement than a user performing the interview at an office. A non-limiting example of a priority rating of quality control requirements, from highest to lowest, may be a missing response, too low of vocal volume, too fast of a response, to slow of a response, too loud of a response, and a presence of background noise. Another example of a priority rating of quality control requirements, from highest to lowest, may be too low of vocal volume, to loud of a response, too fast of a response, to slow of a response, a presence of background noise, and a missing response. The priority rating may be any other order.

The determining may be performed in real time (e.g., simultaneously or substantially simultaneously to the obtaining the first speech sample). For example, the determining can be performed as the first speech sample is obtained from the user. In this example, as soon as the user is finished providing the first speech sample, the determining may be complete. The determining may be performed with at least about 0.01, 0.05, 0.1, 0.5, 1, 2, 3, 4, 5, or more seconds from receipt of the first speech sample. The determining may be performed within at most about 5, 4, 3, 2, 1, 0.5, 0.1, 0.05, 0.01, or fewer seconds from receipt of the first speech sample. The determining may be performed within the duration of an interview. For example, the determining can be completed before the end of an interview in which the first speech sample is obtained from the user.

If the system determines that the first speech sample satisfies each quality control requirement of the plurality of quality control requirements, the system can skip subsequent operations of the process100. For example, the method may stop after collection of the first speech sample if all of the quality control requirements are met. When the first speech sample is determined to satisfy the at least one quality control requirement, the process100may terminate at operation120. For example, the process can be terminated once enough speech samples are obtained that satisfy at least one quality control requirement.

In another operation of the process100, the system can provide the instructions associated with the quality control requirements to the user (130). The instructions may be as described elsewhere herein. Providing the instructions may comprise providing an auditory instruction to the user. For example, the user can be told the instructions through a speaker of a telephone. The providing the instructions may comprise providing a written indication of the instructions to the user. For example, text comprising the instructions can be displayed on a screen to the user. By receiving the instructions, the user may be informed how to provide a higher quality speech sample as compared to the first speech sample. By providing the instructions, the overall quality of speech samples may be higher than without providing the instructions, which can improve the performance of systems that use the speech samples (e.g., vocal processing systems, etc.). By associating the instructions with the quality control requirements, the instructions may be tailored to the specific issues presented in the first speech sample and thus may improve the relevance of the instructions to each user.

In another operation of the process100, the system can prompt the user to provide a second speech sample (140). The second speech sample may be obtained using a same system as the first speech sample. For example, the second speech sample can be acquired by a same microphone as the first speech sample. The second speech sample may be obtained in the same session as the first speech sample. For example, the user can be in the same interview session when the user provides the second speech sample. The second speech sample may be obtained subsequent to the session in which the first speech sample was obtained.

The second speech sample may be used as a speech sample in operations110-130. For example, the second speech sample may be subjected to the determination of if the second speech sample satisfies the at least one quality control requirement. If the second speech sample is determined to satisfy the at least one quality control requirement, the second speech sample may be further processed. When the first speech sample is determined not to satisfy the one or more quality control requirements, but the second speech sample is determined to satisfy the one or more quality control requirements, the second speech sample may be used in place of the first speech sample. For example, the first speech sample can be discarded and not used for a vocal processing algorithm while the second speech sample may be used for the algorithm. Alternatively, when both the first and second speech samples are determined not to satisfy the at least one quality control requirement, both the first and second speech samples may be discarded.

Any number of operations110-140may be repeated one or more times to generate one or more additional speech samples. The operations may be repeated at least about 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, or more times. The operations may be repeated at most about 10, 9, 8, 7, 6, 5, 4, 3, 2, or less times. For example, the operations may be repeated at most 2 times resulting in at most 3 speech samples. Each of the one or more additional speech samples may be processed to determine if each speech sample satisfies the at least one quality control requirement. Each speech sample of the one or more additional speech samples that does not satisfy the at least one quality control requirement may be discarded.

In another aspect, the present disclosure provides a system. The system may comprise one or more computer processors operatively coupled to computer memory. The one or more computer processors may be individually or collectively configured to determine that a first speech sample of a user does not satisfy at least one quality control requirement of a plurality of quality control requirements. The plurality of quality control requirements may comprise a missing response, a vocal volume, a vocal speed, and a presence or absence of background noise. Each quality control requirement of the plurality of quality control requirements may be associated with instructions. The instructions may indicate that the quality control requirement was not satisfied. The instructions may indicate how to satisfy the quality control requirement in a second speech sample. The instruction associated with the quality control requirement may be provided to the user. The user may be prompted to provide the second speech sample.

FIG.2is a schematic diagram of a system200. The user201may provide a speech sample to an input device202. The user may be a person taking an interview (e.g., a person answering questions, an applicant to a job, a person under evaluation, etc.). The speech sample may comprise one or more words answering a question provided to the user. For example, the user can receive an audio question from a user interface device205. The speech sample may be a single word. For example, the speech sample can be “yes” or “no.” The input device202may comprise a microphone. Examples of input devices include, but are not limited to, telephones (e.g., smartphones, landline phones, etc.), computers (e.g., a laptop comprising a microphone, a desktop computer, etc.), electronic devices (e.g., a tablet, a personal digital assistant, an electronic kiosk, etc.), a microphone coupled to a recorder, or the like, or any combination thereof. For example, the user can use their smartphone to input the speech samples. In this example, the speech sample may be recorded by a system external to the smartphone (e.g., a cloud server). In some cases, the smartphone may record the speech sample for later transmission to an external system.

The input device may be coupled to a computer system203. The computer system203may be local to the input device (e.g., the input device is physically attached to the computer system). The computer system may be remote from the input device and/or the user (e.g., the computer system is a remote server). The computer system may be a cloud computer system (e.g., remote to the user). The computer system may be coupled to the input device via the internet. The computer system may be coupled to the input device over a local connection. The speech samples may be encrypted. The computer system may comprise one or more processors configured to process the speech samples. An example of processing is determining if the speech samples satisfy one or more quality control requirements. The computer system may be configured to store the results of the determining on a database204. The computer system may be configured to provide instruction to the user related to the speech samples via the user interface device205. The instructions may comprise auditory instructions (e.g., spoken instructions), text instructions (e.g., text displayed on a screen for the user), or the like, or any combination thereof. Examples of user interface devices include, but are not limited to, telephones (e.g., smartphones, landline phones, etc.), computers (e.g., a laptop comprising a microphone, a desktop computer, etc.), electronic devices (e.g., a tablet, a personal digital assistant, an electronic kiosk, etc.), a microphone coupled to a recorder, or the like, or any combination thereof. For example, the computer system can provide instructions to the user via a speaker within the user's telephone. In another example, the computer system can display the instructions on a screen in front of the user.

Remote Risk Assessment System

Referring toFIG.6, a Remote Risk Assessment (“RRA”) system in accordance with some embodiments of the present disclosure is shown. In some applications, the RRA system100may include a homogeneous, distributed computing and communication network that may include, for example, a processing center (“PC”)10and one or more full-function local server (“LS”) nodes15. When more than one LS node15is included in the system100, the nodes15may be organized in a hierarchical network. The elements of the system100may be hardwired in a network or may include mobile (communication) components that can operate autonomously, communicating and transferring data intermittently, e.g., wirelessly.

AlthoughFIG.6shows a single PC10, the number and type of LS nodes15may vary depending on the number and physical (geographic) location of risk assessment or other interviews being simultaneously conducted, as well as other administrative requirements. The PC10and each LS node15may have its own IP address and may be structured and arranged to manage its own data, as well as to handle data from related systems and other LS nodes15.

In some variations, one or more interview terminals20a,20bmay be in communication with one or more LS nodes15, directly, via a communication network (e.g., the Internet25), and/or via a public switched telephone network (PSTN)30. Interview terminals may include (landline) telephones20a, modular phones, cellphones, iPhones, softphones, computers20b, as well as other mobile voice communication and processing devices. AlthoughFIG.6shows that the PSTN30communicates with the LS nodes15through the Internet25, in some implementations, the PSTN30may communicate directly with the LS nodes15, outside of the Internet25.

Given that the quality of communication across the globe may be distributed unevenly, substantially all implementations of the RRA system100may be characterized by an average distance between an interview terminal20a,20bto an LS node15. Based on distance, all or some portion of the RRA system100may be classified as: a mobile system (distances up to about 100 m, which corresponds to the maximum effective range of a CAT 7 Ethernet), a local corporate system (distances up to about 10 km), a local government system (distance up to about 1000 km), and a global system (distance up to about 20,000 km). Local and global systems may be stationary networks that work, predominantly, in an on-line mode. During emergency conditions, local and global systems may operate temporarily on an off-line basis until the cause of the emergency has been rectified. In contrast, mobile systems may operate in an off-line mode, which may require on-line access and communication with an LS node15sporadically, for example, when data is to be transmitted and/or to receive the results of an interview. Advantageously, even in an extreme case in which there is no electrical power and/or no telecommunication, the RRA system100may operate effectively via mobile, self-powered devices.

In order to join an interview with a service interview, communication between an interview terminal20a,20band an LS node15can be established, e.g., using or via the node's “Clients” module. For example, a (landline) telephone20amay establish communication with an LS node15via a PSTN30using a telephone number either to a call center or directly to the LS node15itself iPhones and/or cellphones may establish communication with an LS node15via a WAN or a LAN network. In some variations, previous registration and entry of an access parameter may be required to facilitate establishing the communication. Access parameters may include, for the purpose of illustration and not limitation, one or more of: a password, a port number, the IP address of the node, and/or a conditional service number. SKYPE and other on-line methods of communicating using a computer or processing device20bmay establish communication with an LS node15via a connection through one or more WAN and a PSTN30, e.g., using a telephone number either to a call center or directly to the LS node15itself.

The LS nodes15may be configured according to modular principles. Between the lowest level and the top of the network, any LS node15may be connected to a host node and may itself be a host node at the same time. Thus, the RRA system100may be hierarchical, such that all LS nodes15in a particular level are associated with adjacent LS nodes15in the same level; but, that each LS node15is associated with a discrete LS node15at a top or upper level.

In some implementations, RRA system topology may display a territorial and/or administrative division of a (geographic) region for collecting interview data. Dimension and topology of the RRA system100may also correspond to an actual workload, as well as to geographic requirements.

Although each LS node15may operate independently from other LS nodes15, LS nodes15may be in communication with other LS nodes15as well as with the PC10, taking into account the peculiarities of organization procedures for, for example, conducting interviews and so forth.

Each LS node15or, collectively, some plurality of the LS nodes15may be adapted to perform one or more of: accept incoming calls and provide information about the system100; support a dialogue with persons/clients in an unattended mode (e.g., without a human operator) in a plurality of foreign languages; reproduce interview questions and record answers; transfer recorded responses to interview questions to other LS nodes15(e.g., in an automatic or manual mode) and receive the processing results; generate reports on interview results; and conduct multiple interviews simultaneously and independently.

Security

Security of the RRA system100may be ensured, for example, by a choice of systemic remedies (e.g., secure data transfer protocols) and/or by developing applied methods of protection (e.g., encryption of sensitive data).

The primary source of information in the RRA system100may be a recorded interview that may include, for example, an audio and/or video record of the response(s) of an interviewed person or client to a plurality of questions posed to the person/client during the interview. In order to ensure that the recorded information is not available to others outside of the system100, the information may be encrypted, which may include both the files containing the person/client's response(s) as well as the results of processing (e.g., of interview results).

Interviews and Service Interviews

An interview may refer to a set of questions that are intended to be asked to a person or client, as well as to an event or the event during which those questions are posed to the person/client (e.g., by an operator or supervisor). In contrast, a service interview may refer to a risk assessment conducted or evaluated, e.g., by a supervisor, based on data collected (e.g., the person/client's responses) during an interview. The interviewed person/client/user may be located remotely from the supervisor performing the service interview. Although the interview and the service interview may be conducted simultaneously or substantially simultaneously, in many implementations, data collected during the interview may be recorded and transmitted remotely from the sites of the interview for later review during the service interview.

Advantageously, the RRA system100may implement the preliminary interview manually, automatically, or by a combination of the two. For example, in the manual mode, a trained operator or supervisor may ask the person/client/user a number of questions from a list of interview qualification questions, the answers of which may be used to assess the person/client/user's status and likelihood of successfully completing a standard interview. In an automatic mode, the system100may perform the same functions as the human actor in the manual mode. In a combined mode, the system100may perform the same functions; however, the supervisor may make an ultimate decision as to the readiness of the person/client for a standard interview. The automatic and combined modes may produce an automatic analysis of the person/client's response in the preliminary interview in real time. For example, the person/client's responses may be recognized, and the verbal responses evaluated. If any of the responses are flagged as posing a risk, further interviewing may be considered inappropriate.

In some embodiments, the RRA system100may implement the standard interview automatically. For example, in automatic mode, the RRA system100may ask the person/client questions using TTS (text-to-speech technology) or the voice of an announcer. Responses may be analyzed immediately in real time or responses, often, may be recorded and recorded data provided to the supervisor on-line, e.g., via the communication network, off-line, e.g., via a PSTN30, or manually at a later date and time.

One of the many advantages of the present system may be that special sensors (e.g., biometric devices for detecting bodily reactions of the person/client) are not necessary. Indeed, some implementations, recording and/or transmission of interviews may be accomplished using a public switched telephone network (PSTN)30or directly over the Internet25, e.g., using a computer20b, iPhone, and the like. In some embodiments, the interview may be conducted using mobile and non-mobile communication devices, e.g., directly from a local area network (LAN) of the system100, e.g., using computer terminals20b, iPhones, and the like, and/or remotely via a global computer network (e.g., a wide area network (WAN)) of a public switched telephone network (PSTN)30.

Risk Appraisal

During a preliminary or a standard interview, a first party may ask questions. In response to the posed question, a second party may answer the question, may choose not to answer the question (e.g., either by remaining silent or by positively expressing that he/she will not answer the question), or may be evasive in his/her answer to the question. The first party, or interviewer, may define the subject matter of the interview and may formulate the questions. The interviewer may be interested in not only receiving clear answers to the interview questions but also receiving answers that the interviewer has a high level of confidence are accurate. The second party, or interviewee, in many instances may be subject to some external force(s) that encourages him/her to answer the questions posed, hence, he/she may try to respond to the questions in a manner most beneficial to his/her personal interests. As a result, there may be some level of uncertainty in accepting the individual responses of the interviewee or any response of the interviewee.

For example, in some embodiments, a series of questions for which a short (yes or no) answer may be formed. The response duration for these short questions may range between about 0.1 and 0.5 seconds. As a result, the response may be associated directly with the content of the question asked.

In another aspect, the present disclosure provides one or more non-transitory computer storage media storing instructions that are operable, when executed by one or more computers, to cause the one or more computers to perform operations. The operations may comprise obtaining a first speech sample from a user. The speech sample may be determined to not satisfy at least one quality control requirement of a plurality of quality control requirements. The plurality of quality control requirements may comprise a missing response, a vocal volume, a vocal speed, and a presence or absence of background noise. Each quality control requirement of the plurality of quality control requirements may be associated with instructions. The instructions may indicate that the quality control requirement was not satisfied. The instructions may indicate how to satisfy the quality control requirement in a second speech sample. The instruction associated with the quality control requirement may be provided to the user. The user may be prompted to provide the second speech sample.

The following examples are illustrative of certain systems and methods described herein and are not intended to be limiting.

Example 1—Performing Quality Control of Audio Interviews

FIG.3is a flow chart of a process for conducting an automated interview of a user, according to an embodiment. The process can be performed by a system of one or more computers in one or more locations (e.g., the system203ofFIG.2). The user can begin the interview by dialing a phone number or visiting a website. The act of dialing the phone number or visiting the website may cause the system to initiate an automated conversation with the user. In a first operation, the system can prompt the user to provide one or more credentials. Examples of credentials include reference numbers, identification numbers, passwords, or the like. In a second operation, the system can check that the credentials are valid (e.g., that the provided password is the correct password for the provided username). If the credentials are found to not be valid, the system may prompt the user again to enter credentials. Failing to provide correct credentials a number of times may result in a user being barred from further attempts to do so. Each user may have a different interview. For example, different users can receive different questions as a part of their interview. Each user may have different credentials for accessing their interview. For example, a first credential can correspond to a first user, while a second credential can be used by a second user to access their interview. The parameters of the interview may be different for different users. Examples of parameters include the number of questions NQ, the question set used for playback Q-SET, the number of repetitions allowed for the questions NQSet_max, or the like.

Once the user is permitted to participate in the interview, the user may begin the process QUEST-SET-Proc as shown inFIG.4. Upon completion of QUEST-SET-Proc, the system can process the obtained speech samples to determine if they do not satisfy at least one quality control requirement. If so, the system can repeat QUEST-SET-Proc until a predetermined number of speech samples that satisfy all of the at least one quality control requirement are obtained, at which point the user may be notified that the interview was completed successfully and the interview may be terminated.

If the speech sample is not determined to satisfy all of the quality control requirements, the system can play the instruction associated with the highest priority quality control requirement in an attempt to improve the quality of subsequent speech samples. If the number of times QUEST-SET-Proc has been performed is less than the maximum number NQSet_max, the QUEST-SET-Proc process can be repeated in order to attempt to receive speech samples that satisfy all of the quality control requirements. If instead, the maximum number has been reached, the user can be informed that the interview was not satisfactorily completed, and the interview may be terminated.

FIG.4is a flow chart of a process QUEST-SET-Proc as shown inFIG.3. The system can begin the process by playing a question (e.g., QUESTION-1) for the user and recording their response to the question. The process may be repeated for each question in the interview. The answers provided by the users can be used as speech samples as described elsewhere herein. For example, the speech samples can be subjected to operation120ofFIG.1to determine if the speech samples satisfy the at least one quality control requirement. The status of the speech samples can be transferred to the process ofFIG.3, and the results can be used to determine how the interview proceeds (e.g., are the questions repeated, is the interview terminated, etc.).

Computer Systems

The present disclosure provides computer systems that are programmed to implement methods of the disclosure.FIG.5shows a computer system501that is programmed or otherwise configured to implement methods as described elsewhere herein. The computer system501can regulate various aspects of the present disclosure, such as, for example, the process100ofFIG.1. The computer system501can be an electronic device of a user or a computer system that is remotely located with respect to the electronic device. The electronic device can be a mobile electronic device.

The computer system501includes a central processing unit (CPU, also “processor” and “computer processor” herein)505, which can be a single core or multi core processor, or a plurality of processors for parallel processing. The computer system501also includes memory or memory location510(e.g., random-access memory, read-only memory, flash memory), electronic storage unit515(e.g., hard disk), communication interface520(e.g., network adapter) for communicating with one or more other systems, and peripheral devices525, such as cache, other memory, data storage and/or electronic display adapters. The memory510, storage unit515, interface520and peripheral devices525are in communication with the CPU505through a communication bus (solid lines), such as a motherboard. The storage unit515can be a data storage unit (or data repository) for storing data. The computer system501can be operatively coupled to a computer network (“network”)530with the aid of the communication interface520. The network530can be the Internet, an internet and/or extranet, or an intranet and/or extranet that is in communication with the Internet. The network530in some cases is a telecommunication and/or data network. The network530can include one or more computer servers, which can enable distributed computing, such as cloud computing. The network530, in some cases with the aid of the computer system501, can implement a peer-to-peer network, which may enable devices coupled to the computer system501to behave as a client or a server.

The CPU505can execute a sequence of machine-readable instructions, which can be embodied in a program or software. The instructions may be stored in a memory location, such as the memory510. The instructions can be directed to the CPU505, which can subsequently program or otherwise configure the CPU505to implement methods of the present disclosure. Examples of operations performed by the CPU505can include fetch, decode, execute, and writeback.

The CPU505can be part of a circuit, such as an integrated circuit. One or more other components of the system501can be included in the circuit. In some cases, the circuit is an application specific integrated circuit (ASIC).

The storage unit515can store files, such as drivers, libraries, and saved programs. The storage unit515can store user data, e.g., user preferences and user programs. The computer system501in some cases can include one or more additional data storage units that are external to the computer system501, such as located on a remote server that is in communication with the computer system501through an intranet or the Internet.

The computer system501can communicate with one or more remote computer systems through the network530. For instance, the computer system501can communicate with a remote computer system of a user. Examples of remote computer systems include personal computers (e.g., portable PC), slate or tablet PC's (e.g., Apple® iPad, Samsung® Galaxy Tab), telephones, Smart phones (e.g., Apple® iPhone, Android-enabled device, Blackberry®), or personal digital assistants. The user can access the computer system501via the network530.

Methods as described herein can be implemented by way of machine (e.g., computer processor) executable code stored on an electronic storage location of the computer system501, such as, for example, on the memory510or electronic storage unit515. The machine executable or machine-readable code can be provided in the form of software. During use, the code can be executed by the processor505. In some cases, the code can be retrieved from the storage unit515and stored on the memory510for ready access by the processor505. In some situations, the electronic storage unit515can be precluded, and machine-executable instructions are stored on memory510.

The code can be pre-compiled and configured for use with a machine having a processer adapted to execute the code, or can be compiled during runtime. The code can be supplied in a programming language that can be selected to enable the code to execute in a pre-compiled or as-compiled fashion.

Aspects of the systems and methods provided herein, such as the computer system501, can be embodied in programming. Various aspects of the technology may be thought of as “products” or “articles of manufacture” typically in the form of machine (or processor) executable code and/or associated data that is carried on or embodied in a type of machine readable medium. Machine-executable code can be stored on an electronic storage unit, such as memory (e.g., read-only memory, random-access memory, flash memory) or a hard disk. “Storage” type media can include any or all of the tangible memory of the computers, processors or the like, or associated modules thereof, such as various semiconductor memories, tape drives, disk drives and the like, which may provide non-transitory storage at any time for the software programming. All or portions of the software may at times be communicated through the Internet or various other telecommunication networks. Such communications, for example, may enable loading of the software from one computer or processor into another, for example, from a management server or host computer into the computer platform of an application server. Thus, another type of media that may bear the software elements includes optical, electrical, and electromagnetic waves, such as used across physical interfaces between local devices, through wired and optical landline networks and over various air-links. The physical elements that carry such waves, such as wired or wireless links, optical links, or the like, also may be considered as media bearing the software. As used herein, unless restricted to non-transitory, tangible “storage” media, terms such as computer or machine “readable medium” refer to any medium that participates in providing instructions to a processor for execution.

Hence, a machine readable medium, such as computer-executable code, may take many forms, including but not limited to, a tangible storage medium, a carrier wave medium or physical transmission medium. Non-volatile storage media include, for example, optical or magnetic disks, such as any of the storage devices in any computer(s) or the like, such as may be used to implement the databases, etc. shown in the drawings. Volatile storage media include dynamic memory, such as main memory of such a computer platform. Tangible transmission media include coaxial cables; copper wire and fiber optics, including the wires that comprise a bus within a computer system. Carrier-wave transmission media may take the form of electric or electromagnetic signals, or acoustic or light waves such as those generated during radio frequency (RF) and infrared (IR) data communications. Common forms of computer-readable media therefore include for example: a floppy disk, a flexible disk, hard disk, magnetic tape, any other magnetic medium, a CD-ROM, DVD or DVD-ROM, any other optical medium, punch cards paper tape, any other physical storage medium with patterns of holes, a RAM, a ROM, a PROM and EPROM, a FLASH-EPROM, any other memory chip or cartridge, a carrier wave transporting data or instructions, cables or links transporting such a carrier wave, or any other medium from which a computer may read programming code and/or data. Many of these forms of computer readable media may be involved in carrying one or more sequences of one or more instructions to a processor for execution.

The computer system501can include or be in communication with an electronic display535that comprises a user interface (UI)540for providing, for example, indications associated with a quality control requirement. Examples of UI's include, without limitation, a graphical user interface (GUI) and web-based user interface.

Methods and systems of the present disclosure can be implemented by way of one or more algorithms. An algorithm can be implemented by way of software upon execution by the central processing unit505. The algorithm can, for example, perform determinations on speech samples as described elsewhere herein.

While preferred embodiments of the present invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only. It is not intended that the invention be limited by the specific examples provided within the specification. While the invention has been described with reference to the aforementioned specification, the descriptions and illustrations of the embodiments herein are not meant to be construed in a limiting sense. Numerous variations, changes, and substitutions will now occur to those skilled in the art without departing from the invention. Furthermore, it shall be understood that all aspects of the invention are not limited to the specific depictions, configurations or relative proportions set forth herein which depend upon a variety of conditions and variables. It should be understood that various alternatives to the embodiments of the invention described herein may be employed in practicing the invention. It is therefore contemplated that the invention shall also cover any such alternatives, modifications, variations, or equivalents. It is intended that the following claims define the scope of the invention and that methods and structures within the scope of these claims and their equivalents be covered thereby.

",James A. Kane,"Finance, Logistics, Marketing & Advertising, Technology,Technology, Media, Healthcare, Artificial Intelligence & Machine Learning,Telecommunications, Technology, Consumer Goods","Administrative data processing, financial transactions, resource planning, inventory management,Speech recognition, audio coding, voice processing, speech synthesis, text-to-speech systems,Telephonic communication systems, mobile and landline substation equipment, subscriber services, VoIP",The present disclosure provides methods and systems that may be used for providing quality control for audio samples. The audio samples may be speech samples of a user. The user may be participating in an audio interview.
12020725,"BACKGROUND OF THE INVENTION

1. Field of the Invention

The present invention generally relates to voice activity detection (VAD), and more particularly to an acoustic feature extraction (AFE) circuit thereof.

2. Description of Related Art

Voice activity detection (VAD) is a technology capable of detecting or recognizing presence or absence of human speech. VAD can be used to activate speech-based applications such as Apple Inc.'s virtual assistant Siri. VAD may be commonly adopted as a front-end device, which is generally an always-on and low-power system.

VAD has been conventionally implemented by digital architecture, which, however, requires considerable circuit area and consumes substantive power. Moreover, conventional VAD suffers process-voltage-temperature (PVT) variation.

A need has thus arisen to propose a novel VAD system to overcome drawbacks of conventional VAD systems.

SUMMARY OF THE INVENTION

In view of the foregoing, it is an object of the embodiment of the present invention to provide a voice activity detection (VAD) system with an acoustic feature extraction (AFE) circuit capable of substantially reducing power consumption with high performance.

According to one embodiment, an acoustic feature extraction (AFE) circuit includes a plurality of band-pass filters (BPFs) and a rectifier. The band-pass filters (BPFs) are adaptable to a plurality of channels with different band-pass frequency ranges respectively for switchably receiving an amplified signal, thereby generating corresponding filtered signals, an operational amplifier being shared among the plurality of channels. The rectifier is switchably coupled to receive the filtered signals, thereby generating a rectified signal. The amplified signal is time-division demultiplexed onto the BPFs in different phases, and the filtered signals are time-division multiplexed onto the rectifier in different phases.

",utility,2024-06-25,Voice activity detection system and acoustic feature extraction circuit thereof,B2,8.0,,"NCKU Research and Development Foundation, Himax Technologies Limited","H03F 3/45, G10L 15/2, G10L 25/84","G10L25/84,G10L15/02,G10L25/03,H03F3/005,H03F3/45475,H03F2200/156,H03F2200/252,H03F2203/45514","inventional,inventional,inventional,inventional,inventional,additional,additional,additional","Speech recognition -Feature extraction for speech recognition; Selection of recognition unit, Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 -characterised by the type of extracted parameters, Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 -Detection of presence or absence of voice signals -for discriminating voice from noise, Indexing scheme relating to amplifiers-One or more switches are realised in the feedback circuit of the amplifier stage, Indexing scheme relating to amplifiers-Multiple switches coupled in the input circuit of an amplifier are controlled by a circuit, e.g. feedback circuitry being controlling the switch, Indexing scheme relating to amplifiers with only discharge tubes or only semiconductor devices as amplifying elements covered by H03F3/00-Indexing scheme relating to differential amplifiers-the FBC comprising one or more switched capacitors, and being coupled between the LC and the IC, Amplifiers with only discharge tubes or only semiconductor devices as amplifying elements-using switched capacitors, e.g. dynamic amplifiers; using switched capacitors as resistors in differential amplifiers , Amplifiers with only discharge tubes or only semiconductor devices as amplifying elements-Differential amplifiers -with semiconductor devices only-characterised by the way of implementation of the active amplifying circuit in the differential amplifier-using IC blocks as the active amplifying circuit","MUSICAL INSTRUMENTS; ACOUSTICS, ELECTRONIC CIRCUITRY","SPEECH ANALYSIS TECHNIQUES OR SPEECH SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING TECHNIQUES; SPEECH OR AUDIO CODING OR DECODING, AMPLIFIERS",0.0,4.0,"DETAILED DESCRIPTION OF THE INVENTION

FIG.1shows a block diagram illustrating a voice activity detection (VAD) system100according to one embodiment of the present invention. In the embodiment, the VAD system100may be an analog system that operates on analog signals.

The VAD system100of the embodiment may include an amplifier11coupled to receive a sound signal converted from sound by a converter such as a microphone10, and configured to generate an amplified signal according to the sound signal. In the embodiment, the amplifier11may be a low-noise amplifier (LNA) capable of amplifying a low-power signal (e.g., the sound signal) without substantially degrading a signal-to-noise ratio (SNR).

The VAD system100of the embodiment may include an acoustic feature extraction (AFE) circuit12coupled to receive the amplified signal, and configured to generate a feature signal representing a feature extracted from the amplified signal.

The VAD system100of the embodiment may include a classifier13configured to identify the feature signal as a voice or a noise. In one embodiment, the classifier13may include an (analog) neural network circuit.

In one embodiment, the VAD system100may further include a buffer14, such as a unit-gain buffer, disposed between the amplifier11and the AFE circuit12, and configured to provide electrical impedance transformation from the amplifier11to the AFE circuit12, such that the amplified signal may not be affected by the AFE circuit12(i.e., the load).

FIG.2shows a detailed block diagram illustrating the acoustic feature extraction (AFE) circuit12ofFIG.1. In the embodiment, the AFE circuit12may include a plurality of band-pass filters (BPFs)121adaptable to a plurality of channels (three channels are exemplified here) with different band-pass frequency ranges respectively (e.g., 95-195 Hz, 150-310 Hz and 250-500 Hz), thereby generating corresponding filtered signals. Specifically, the band-pass filters121are switchably coupled to receive the amplified signal (from the amplifier11). According to one aspect of the embodiment, the amplified signal is time-division demultiplexed onto the BPFs121in different phases (e.g., phase1through phase3via phase switches P1-P3respectively).

The AFE circuit12of the embodiment may include a rectifier122switchably coupled to receive the filtered signals, which are time-division multiplexed onto the rectifier122in different phases (e.g., via phase switches P1-P3respectively), thereby generating a rectified signal. Therefore, a single rectifier122is needed for all the channels.

The AFE circuit12of the embodiment may include a plurality of low-pass filters (LPFs)123adaptable to the plurality of channels with the same low-pass frequency range (e.g., having a cut-off frequency of 30 Hz), thereby generating corresponding feature signals (e.g., 1st feature signal through 3rd feature signal as exemplified inFIG.2). Specifically, the low-pass filters123are switchably coupled to receive the rectified signal (from the rectifier122). The rectified signal is time-division demultiplexed onto the LPFs123in different phases (e.g., phase1through phase3via phase switches P1-P3respectively).

FIG.3Ashows a circuit diagram illustrating a BPF121ofFIG.2for one channel (e.g., channel1), andFIG.3Bshows a timing diagram illustrating time periods of corresponding phase. In the embodiment, each phase is divided into a first period (e.g., ϕ1) and a second period (e.g., ϕ2). For example, the first period ϕ1lies in the first half of the phase signal P1, and the second period ϕ2follows the first period first period ϕ1until the beginning of the next phase signal P1.

In the embodiment, the BPF121may include a switched-capacitor (SC) circuit. Specifically, the BPF121may include an operational amplifier1211with a negative input node, a positive input node (connected to earth) and an output node. According to one aspect of the embodiment, a single operational amplifier1211may be shared among the channels in a time-division demultiplexing manner, thereby substantially reducing power consumption and circuit area.

The BPF121may include a first charge capacitor CR1with a first end switchably coupled to receive the amplified signal via a first period switch ϕ1that is closed in the first period, and with a second end connected to earth. The BPF121may include a first filter capacitor CC1with a first end switchably connected to (the first end of) the first charge capacitor CR1via a second period switch ϕ2that is closed in the second period, and with a second end switchably connected to the negative input node (of the operational amplifier1211) via a phase switch P1that is closed in a corresponding phase. The BPF121may include a second filter capacitor CC2with a first end connected to (the first end of) the first filter capacitor CC1, and with a second end switchably connected to the output node of the operational amplifier1211via another phase switch P1. The BPF121may include a second charge capacitor CR2with a first end switchably connected to the second end of the first filter capacitor CC1via another first period switch ϕ1and switchably connected to the second end of the second filter capacitor CC2via another second period switch ϕ2, and with a second end connected to earth.

According to another aspect of the embodiment, the BPF121may include a first stabilizing capacitor CL1with a first end switchably connected to the negative input node of the operational amplifier1211via the phase switch P1, and with a second end connected to earth. The first stabilizing capacitor CL1is configured to solve floating voltage issue at the interconnect node between the first filter capacitor CC1and the first stabilizing capacitor CL1in hold state. The BPF121may further include a second stabilizing capacitor CL2with a first end switchably connected to the output node of the operational amplifier1211via said another phase switch P1, and with a second end connected to earth. The second stabilizing capacitor CL2is configured to stabilize the voltage at the output node of the operational amplifier1211in charge state.

FIG.4Ashows a circuit diagram illustrating BPFs121for three channels, andFIG.4Bshows a timing diagram illustrating time periods of corresponding phases. It is noted that a single operational amplifier1211may be shared among the channels in a time-division demultiplexing manner.

FIG.5Ashows equivalent circuits of the BPFs121in the first period ϕ1of the first phase P1, in which the BPF121of channel1is in charge state, the BPF121of channel2is in hold state, and the BPF121of channel3is in hold state.FIG.5Bshows equivalent circuits of the BPFs121in the second period ϕ2of the first phase P1, in which the BPF121of channel1is in filter state, the BPF121of channel2is in hold state, and the BPF121of channel3is in hold state.FIG.5Cshows equivalent circuits of the BPFs121in the first period ϕ3of the second phase P2, in which the BPF121of channel1is in rectifier state, the BPF121of channel2is in charge state, and the BPF121of channel3is in hold state.FIG.5Dshows a table illustrating states of the BPFs121in corresponding periods and phases. It is noted that the operational amplifier1211is utilized in charge and filter states, while the rectifier122is utilized in rectifier state.

FIG.6shows a circuit diagram illustrating the rectifier122ofFIG.2. In the embodiment, the rectifier122may include an operational amplifier1221with a positive input node coupled to receive the filtered signal, and with a negative input node coupled to receive a common-mode voltage VCM, thereby generating a switch signal ϕ C. The filtered signal is switchably transferred via a first switch that is controlled by the switch signal ϕ C, and the common-mode voltage VCM is switchably transferred via a second switch that is controlled by an inverted switch signal ϕ C \. Accordingly, one of the filtered signal and the common-mode voltage VCM is transferred as the rectified signal, thereby resulting in a half-wave rectifier. In an alternative embodiment, the rectifier122may be implemented by a full-wave rectifier instead.

FIG.7shows a circuit diagram illustrating a LPF123for one channel. In the embodiment, the LPF123may include a switched-capacitor (SC) circuit. Specifically, the LPF123may include an operational amplifier1231with a negative input node, a positive input node (connected to earth) and an output node. The operational amplifier1231may act as a buffer or driving stage, and may be omitted when buffering or driving is not required. The LPF123may include a first charge capacitor CR1with a first end switchably coupled to receive the rectified signal via a first period switch ϕ1that is closed in the first period, and with a second end connected to earth. The LPF123may include a filter capacitor CC1with a first end switchably connected to (the first end of) the first charge capacitor CR1via a second period switch ϕ2that is closed in the second period, and with a second end connected to the output node (of the operational amplifier1231). The BPF121may include a second charge capacitor CR2with a first end switchably connected to the negative input node of the operational amplifier1231via another first period switch ϕ1and switchably connected to the second end of the filter capacitor CC1via another second period switch ϕ2, and with a second end connected to earth.

Although specific embodiments have been illustrated and described, it will be appreciated by those skilled in the art that various modifications may be made without departing from the scope of the present invention, which is intended to be limited solely by the appended claims.

","Meng-Ju Chiang, Soon-Jyh Chang","Electronics, Telecommunications, Audio Technology,Technology, Media, Healthcare, Artificial Intelligence & Machine Learning","Amplifiers, signal amplification, noise reduction, temperature management, power management,Speech recognition, audio coding, voice processing, speech synthesis, text-to-speech systems","An acoustic feature extraction (AFE) circuit includes a plurality of band-pass filters (BPFs) adaptable to a plurality of channels with different band-pass frequency ranges respectively for switchably receiving an amplified signal, thereby generating corresponding filtered signals, the plurality of BPFs including an operational amplifier that is shared among the plurality of channels; and a rectifier switchably coupled to receive the filtered signals, thereby generating a rectified signal. The amplified signal is time-division demultiplexed onto the BPFs in different phases, and the filtered signals are time-division multiplexed onto the rectifier in different phases."
12020726,"CROSS-REFERENCE TO RELATED APPLICATIONS

This application is based upon and claims the benefit of priority from Japanese Patent Application No. 2022-138505, filed on Aug. 31, 2022; the entire contents of which are incorporated herein by reference.

FIELD

Embodiments described herein generally relate to a magnetic reproduction processing device, a magnetic recording/reproducing device, and a magnetic reproducing method.

BACKGROUND

For example, fewer errors are desired in magnetic replay processors.

",utility,2024-06-25,"Magnetic reproduction processing device, magnetic recording/reproducing device, and magnetic reproducing method",B2,19.0,,Kabushiki Kaisha Toshiba,"G11B 5/0, G11B 5/9, G11B 20/10, G11B 20/12, G11B 20/18","G11B20/1833,G11B20/10046,G11B20/1217,G11B2020/10787,G11B2020/1222,G11B2020/185,G11B2020/1863","inventional,inventional,inventional,additional,additional,additional,additional","Signal processing not specific to the method of recording or reproducing; Circuits therefor-Digital recording or reproducing-Improvement or modification of read or write signals-filtering or equalising, e.g. setting the tap weights of an FIR filter, Signal processing not specific to the method of recording or reproducing; Circuits therefor-Digital recording or reproducing-Formatting, e.g. arrangement of data block or words on the record carriers -on discs, Signal processing not specific to the method of recording or reproducing; Circuits therefor-Digital recording or reproducing-Error detection or correction; Testing ; , e.g. of drop-outs-by adding special lists or symbols to the coded information , Signal processing not specific to the method of recording or reproducing; Circuits therefor-Digital recording or reproducing-Audio or video recording; Data buffering arrangements -Data buffering arrangements, e.g. recording or playback buffers-the usage of the buffer being restricted to a specific kind of data-parameters, e.g. for decoding or encoding, Signal processing not specific to the method of recording or reproducing; Circuits therefor-Digital recording or reproducing-Formatting, e.g. arrangement of data block or words on the record carriers -on discs-wherein the formatting concerns a specific area of the disc-ECC block, i.e. a block of error correction encoded symbols which includes all parity data needed for decoding , Signal processing not specific to the method of recording or reproducing; Circuits therefor-Digital recording or reproducing-Error detection or correction; Testing ; , e.g. of drop-outs-by adding special lists or symbols to the coded information -using an low density parity check [LDPC] code, Signal processing not specific to the method of recording or reproducing; Circuits therefor-Digital recording or reproducing-Error detection or correction; Testing ; , e.g. of drop-outs-by adding special lists or symbols to the coded information -wherein the Viterbi algorithm is used for decoding the error correcting code",INFORMATION STORAGE,INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER ,0.0,4.0,"DETAILED DESCRIPTION

According to one embodiment, a magnetic reproduction processing device includes a decoder. The decoder includes a convolutional layer including a plurality of filters, and an attention layer configured to derive a degree of contribution related to the filters. The decoder is configured to output a decoded result obtained by integrating results of processing an input signal with the filters according to the degree of contribution.

Various embodiments are described below with reference to the accompanying drawings.

The drawings are schematic and conceptual; and the relationships between the thickness and width of portions, the proportions of sizes among portions, etc., are not necessarily the same as the actual values. The dimensions and proportions may be illustrated differently among drawings, even for identical portions.

In the specification and drawings, components similar to those described previously in an antecedent drawing are marked with like reference numerals, and a detailed description is omitted as appropriate.

First Embodiment

FIG.1is a schematic diagram illustrating a magnetic reproduction processing device and a magnetic recording/reproducing device according to the first embodiment.

As shown inFIG.1, a magnetic recording/reproducing device210according to the embodiment includes a magnetic reproduction processing device110according to the embodiment. The magnetic recording/reproducing device210may further include a magnetic recording/reproducing portion80D.

The magnetic recording/reproducing portion80D includes a magnetic recording medium80. The magnetic recording medium80may include, for example, a magnetic disk (HDD: Hard Disk Drive). The magnetic recording/reproducing portion80D may include, for example, an SSD (Solid State Drive). The magnetic recording/reproducing portion80D may include, for example, the recording/reproducing portion80R. The recording/reproducing portion80R may include, for example, a magnetic head80H. Information is recorded on the magnetic recording medium80by the recording/reproducing portion80R (magnetic head80H). Information recorded on the magnetic recording medium80is reproduced by the recording/reproducing portion80R (magnetic head80H). A reproduced signal Sig-r is obtained from the recording/reproducing portion80R.

The reproduced signal Sig-r obtained by the magnetic recording/reproducing portion80D (recording/reproducing portion80R) is supplied to the magnetic reproduction processing device110. The reproduced signal Sig-r is processed (decoded) by the magnetic reproduction processing device110. A result (decoding process) processed by the magnetic reproduction processing device110is output from the magnetic reproduction processing device110as a decoded signal Sig-c. The decoded signal Sig-c is, for example, a binary signal “1,0”.

The magnetic reproduction processing device110includes a decoder71. The magnetic reproduction processing device110includes an input interface78, for example. The reproduced signal Sig-r or a signal based on the reproduced signal Sig-r is supplied to the decoder71via an input interface78. An input signal Sig-i including the reproduced signal Sig-r is supplied to the decoder71. The input signal Sig-i (for example, the reproduced signal Sig-r) may be a time-series continuous or discontinuous signal.

As shown inFIG.1, the decoder71includes a convolutional layer10and an attention layer15. As described below, the decoder71may include an input layer10I, a connected layer20and an output layer10O.

The convolutional layer10includes a plurality of filters11. The attention layer15is configured to derive a degree of contribution CR for multiple filters11.

The decoder71is configured to output a decoded result RR1obtained by integrating the results of processing the input signal Sig-i by the plurality of filters11according to the degree of contribution CR. Thereby, decoding with higher accuracy can be performed.

For example, a first reference example in which Viterbi decoding is performed to PRML (Partial Response Maximum Likelihood) can be considered. As will be described later, the decoding accuracy may not be sufficient in the first reference example. On the other hand, for example, a second reference example is considered in which the input signal Sig-i is processed by one filter in the decoder71. In the second reference example, one filter is used such that the entire input signal Sig-i is properly processed. In the second reference example, the decoding accuracy may be insufficient depending on the state of the input signal Sig-i.

In contrast, in the embodiment, the decoding is performed using the results of processing the input signal Sig-i with the multiple filters11. Thereby, decoding with high accuracy is possible. According to the embodiment, it is possible to provide a magnetic reproduction processing device capable of suppressing errors.

As shown inFIG.1, the magnetic reproduction processing device110may include a storage10M. The storage10M is configured to store a coefficient related to multiple filters11. The coefficient is, for example, a “weight”. The attention layer is configured to derive the degree of contribution CR using the coefficient stored in the storage10M. The attention layer15is configured to estimate the degree of contribution CR, for example.

As shown inFIG.1, the attention layer15estimates the degree of contribution CR corresponding to the number cF of the multiple filters11based on the coefficient stored in the storage10M.

In the embodiment, the coefficients stored in the storage10M may be determined by machine learning, for example.

The multiple filters11may be optimized by machine learning, for example. The decoder71includes, for example, an NN (neural network) structure. For example, the attention layer15may include a NN (neural network). The convolutional layer10may include, for example, a CNN (Convolutional Neural Network).

As shown inFIG.1, the decoder71may further include an input layer10I. The input signal Sig-i is input to the input layer10I. The input signal Sig-i input to the input layer10I is supplied to the convolutional layer10and the attention layer15.

As shown inFIG.1, the decoder71may further include a connected layer20. The connected layer20connects the processing result of the convolutional layer10and the processing result of the attention layer15. For example, the results of processing the input signal Sig-i with the multiple filters11are integrated according to the degree of contribution CR. For example, the product of the result of processing the input signal Sig-i by the multiple filters11and the degree of contribution CR is calculated. A sum of the calculated products is calculated. The processing result (decoded result RR1) by the connected layer20may be output. The decoded result RR1may include likelihood information.

As shown inFIG.1, the processing result (decoded result RR1) of the connected layer20may be supplied to other layer25. In other layer25, for example, at least some of the multiple processing results may be combined. The processing results of other layer25can be provided to the output layer10O.

As shown inFIG.1, the decoder71may further include an output layer10O. The output layer10O is configured to output an output information LH1based on the decoded result RR1. The output information LH1includes, for example, likelihood proportion.

In the embodiments, the degree of contribution CR estimated by the attention layer15may be changed depending on the state of the input signal Sig-i. The state of the input signal Sig-i is, for example, a signal waveform. The state of the input signal Sig-i may be, for example, the length of the signal.

For example, the input signal Sig-i (reproduced signal Sig-r) includes a reproduced signal of NkT. “T” is the minimum recording unit (minimum recording period) in reproduction (and recording). “k” is an integer of 1 or more. For example, the reproduced signal Sig-r includes signals such as “1T”, “2T”, . . . , “10T”.

For example, the input signal Sig-i includes a reproduced signal of NiT and a reproduced signal of NjT. “i” is an integer of 1 or more. “j” is an integer of 1 or more. “j” is different from “i”. For example, the degree of contribution CR for the reproduced signal of NiT is different from the degree of contribution CR for the reproduced signal of NjT. For example, the coefficients for the reproduced signal in NiT are different from the coefficients for the reproduced signal in NjT.

As shown inFIG.1, the magnetic reproduction processing device110may further include an error correction decoder72. As described above, the decoder71may further include the output layer10O. The output layer10O is configured to output the output information LH1based on the decoded result RR1. The output information LH1may be provided to error correction decoder72. Errors are corrected in the error correction decoder72. The error correction decoder72may include, for example, an LDPC (Low-Density Parity-Check Codes) decoder.

As shown inFIG.1, the output of the error correction decoder72is can be provided to the decoder71as part of input signal Sig-i. Errors are suppressed more by repeating processing.

In the embodiment, the number of input nodes10N in the input layer10I may be, for example, not less than 5 and not more than 300,000.

In the embodiment, the number of multiple filters11may be, for example, not less than 2 and not more than 1000.

As shown inFIG.1, attention layer15may be configured to acquire, in parallel with the convolutional layer10, at least a part of the input signal Sig-i.

FIG.2is a schematic diagram illustrating the magnetic reproduction processing device and the magnetic recording/reproducing device according to the first embodiment.

As shown inFIG.2, in a magnetic recording/reproducing device211according to the embodiment, a magnetic reproduction processing device111further includes a contribution adjuster73. Except for this, the configuration of the magnetic reproduction processing device111may be the same as the configuration of the magnetic reproduction processing device110.

The contribution adjuster73is configured to adjust the contributions CR related to the multiple filters11. The contribution adjuster73adjusts the contributions CR of at least a part of the multiple filters11based on at least a part of the processing results of the error correction decoder72. In this example, at least a part of the processing result of the error correction decoder72is supplied to the contribution adjuster73. In the embodiment, the method of adjusting the degree of contribution CR in the contribution adjuster73may be variously modified. For example, the degree of contribution CR may be adjusted according to the characteristics of the target magnetic recording/reproducing portion80D. Better decoding can be performed.

FIG.3is a schematic diagram illustrating a magnetic reproduction processing device and a magnetic recording/reproducing device according to the first embodiment.

As shown inFIG.3, in a magnetic recording/reproducing device212according to the embodiment, a magnetic reproduction processing device112further includes a waveform controller74(WC: Waveform controller). Except for this, the configuration of the magnetic reproduction processing device112may be the same as the configuration of the magnetic reproduction processing device110.

The decoder71is configured to acquire the input signal Sig-i after being adjusted by the waveform controller74. For example, the waveform controller74acquires the reproduced signal Sig-r, adjusts the waveform of the reproduced signal Sig-r, and outputs the waveform of the reproduced signal Sig-r being adjusted as the input signal Sig-i. The decoder71(e.g., input layer10I) may be configured to acquire the input signal Sig-i adjusted by waveform controller74. For example, the waveform is adjusted in accordance with the characteristics of the target magnetic recording/reproducing portion80D. Better decoding is possible. The waveform controller74may include, for example, an FIR (Finite Impulse Response).

FIG.4is a schematic diagram illustrating a magnetic reproduction processing device and a magnetic recording/reproducing device according to the first embodiment.

As shown inFIG.4, in a magnetic recording/reproducing device213according to the embodiment, a magnetic reproduction processing device113includes a plurality of decoders71. Except for this, the configuration of the magnetic reproduction processing device113may be the same as the configuration of the magnetic reproduction processing device110.

In the magnetic reproduction processing device113, the multiple decoders71are capable of parallel processing. The results of processing by multiple decoders71are combined by a combiner75. The output of combiner75is provided to the error correction decoder72. Thus, for example, the combiner75can combine the decoded result RR1obtained from one of the plurality of decoders71and the decoded result RR1obtained from another one of the plurality of decoders71. For example, the combiner75combines the output information LH1(e.g., likelihood proportion) from one of the plurality of decoders71with the output information LH1(e.g., likelihood proportion) from another one of the plurality of decoders71. The combined result by combiner75is provided to the error correction decoder72. The results of the processing by the error correction decoder72may be supplied to multiple decoders71. Better decoding is possible. For example, faster processing is possible.

For example, a first learning condition of one of the plurality of decoders71is different from a second learning condition of another of the plurality of decoders71. In one example, a first error function in the first learning condition is different from a second error function in the second learning condition. In one example, a sequence of the multiple learning data values in the first learning condition is different from a sequence of the multiple learning data values in the second learning condition. For example, in the first learning condition and the second learning condition, the sequence of the learning data values is in reverse. For example, in the learning, the reproduction signal Sig-r is used as learning data. The reproduced signal Sig-r is expressed as multiple signal strength values at multiple times. The learning data includes the first value to k-th values. “k” is an integer greater than or equal to 2. The “k” corresponds to the time. In the first learning condition, the learning is performed in the direction from the first value to the k-th value. In the second learning condition, the learning is performed in the direction from the k-th value to the first value. More appropriate decoding is possible by processing the learning data by multiple decoders71under different learning conditions.

FIG.5is a graph illustrating characteristics of the magnetic reproduction processing device.

FIG.5illustrates the characteristics of the magnetic reproduction processing devices110-113. Furthermore,FIG.5also illustrates the characteristics of a magnetic reproduction processing device118of a first reference example and the characteristics of a magnetic reproduction processing device119of a second reference example. In the magnetic reproduction processing device118, Viterbi decoding using a PR filter is performed. The decoding in the first reference example corresponds to the number of filters being one. In the magnetic reproduction processing device119of the second reference example, the reproduction signal is processed by one filter. The first reference example corresponds to a configuration in which the decoder71is replaced with a Viterbi decoder. The second reference example corresponds to a configuration in which the decoder71processes with one filter. Except for the above, the configurations of the magnetic reproduction processing device118and the magnetic reproduction processing device119are the same as the configuration of the magnetic reproduction processing device110. The horizontal axis ofFIG.5represents the repetition number nx of the process. The repetition number nx is normalized and described. The vertical axis inFIG.5is BER (Bit Error Rate).

In the example ofFIG.5, the input layer10I has 11 nodes and number of channel is 1. The number of filters11included in the convolutional layer10is 15. In the other layer25(e.g., a fully connected layer), the number of layers is 5 and the number of nodes is 10. The number of output nodes in the output layer10O is 1. In this example, the attention layer15is a fully connected layer, in which the number of the layer is 5, the number of input layers is 11, and the number of the output nodes is 15. The output of attention layer15is a softmax output.

As shown inFIG.5, in the magnetic reproduction processing devices110to113according to the embodiment, a BER lower than the BER in the magnetic reproduction processing device118of the first reference example and the magnetic reproduction processing device119of the second reference example is obtained. The BER in the magnetic reproduction processing device111is lower than the BER in the magnetic reproduction processing device110. The BER in the magnetic reproduction processing device112is lower than the BER in the magnetic reproduction processing device111. The BER in the magnetic reproduction processing device113is lower than the BER in the magnetic reproduction processing device112.

The magnetic reproduction processing device according to the embodiment may include a computer.

FIG.6is a schematic diagram illustrating a magnetic reproduction processing device according to the embodiment.

As shown inFIG.6, the magnetic reproduction processing device (for example, the magnetic reproduction processing device110) according to the embodiment may include a processing circuit76p, a memory circuit76m, an interface circuit76f, and the like. The processing circuit76pis, for example, an electric circuit. The memory circuit76mmay include, for example, at least one of ROM (Read Only Memory) and RAM (Random Access Memory). As the memory circuit76m, for example, a part of the magnetic recording/reproducing portion80D may be used.

The magnetic reproduction processing device (for example, the magnetic reproduction processing device110) according to the embodiment may include a display device76d, an input device76i, and the like. The display device76dmay include various displays. The input device76iincludes, for example, a device having operation functions (e.g., keyboard, mouse, touch input panel, voice recognition input device, etc.).

A plurality of elements included in the magnetic reproduction processing device (for example, the magnetic reproduction processing device110) according to the embodiment can communicate with each other by at least one of wireless or wired methods. The locations where the plurality of elements included in the magnetic reproduction processing device110are provided may be different from each other. For example, a general-purpose computer may be used as the magnetic reproduction processing device110. For example, a plurality of computers connected to each other may be used as the magnetic reproduction processing device110. A dedicated circuit may be used as at least part of the magnetic reproduction processing device110. As the magnetic reproduction processing device110, for example, a plurality of circuits connected to each other may be used.

The embodiments may include a program. The program causes the computer (magnetic reproduction processing device110) to perform the above operations. The embodiments may include a storage medium storing the above program.

Second Embodiment

The second embodiment relates to a magnetic reproducing method. The magnetic reproducing method according to the embodiment is a method using the magnetic reproduction processing device (for example, magnetic reproduction processing device110to113) according to the first embodiment and modifications thereof. It is possible to provide a magnetic reproducing method capable of suppressing errors.

The embodiments may include the following configurations (for example, technical proposals).

Configuration 1

A magnetic reproduction processing device, comprising:a decoder,the decoder includinga convolutional layer including a plurality of filters, andan attention layer configured to derive a degree of contribution related to the filters,the decoder being configured to output a decoded result obtained by integrating results of processing an input signal with the filters according to the degree of contribution.
Configuration 2

The processing device according to Configuration 1, further comprising:a storage configured to store a coefficient related to the filters,the attention layer being configured to derive the degree of contribution using the coefficient.
Configuration 3

The processing device according to Configuration 1 or 2, wherein the decoded result includes likelihood information.

Configuration 4

The processing device according to any one of Configurations 1 to 3, wherein the input signal includes a reproduced signal obtained from a magnetic recording/reproducing portion.

Configuration 5

The processing device according to any one of Configurations 1 to 4, whereinthe input signal includes a reproduced signal of NiT and a reproduced signal of NjT,the “T” is the minimum recording unit in reproducing,the “i” is an integer of 1 or more,the “j” is an integer of 1 or more,the “j” is different from the “i”, andthe degree of contribution for the reproduced signal of the NiT is different from the degree of contribution for the reproduced signal of the NjT.
Configuration 6

The processing device according to any one of Configurations 1 to 5, whereinthe decoder further includes an output layer,the output layer is configured to output an output information based on the decoded result, andthe output information includes likelihood proportion.
Configuration 7

The processing device according to any one of Configurations 1 to 5, further comprising:an error correction decoder,the decoder further including an output layer,the output layer being configured to output an output information based on the decoded result, andthe output information being configured to be supplied to the error correction decoder.
Configuration 8

The processing device according to Configuration 7, wherein an output of the error correction decoder is configured to be provided to the decoder as part of the input signal.

Configuration 9

The processing device according to any one of Configurations 1 to 8, wherein a number of the filters is not less than 2 and not more than 1000.

Configuration 10

The processing device according to any one of Configurations 1 to 9, wherein the attention layer is configured to acquire at least a part of the input signal in parallel with the convolutional layer.

Configuration 11

The processing device according to any one of Configurations 1 to 10, whereinthe decoder further includes an input layer,the input signal is input to the input layer,the input signal input to the input layer is supplied to the convolutional layer and the attention layer, andthe number of input nodes in the input layer is not less than 5 and not more than 300,000.
Configuration 12

The processing device according to any one of Configurations 1 to 10, further comprising a waveform controller, andthe decoder being configured to acquire the input signal after being adjusted by the waveform controller.
Configuration 13

The processing device according to any one of Configurations 1 to 12, further comprising a contribution adjuster,the contribution adjuster being configured to operate to adjust the degree of contribution for the filters.
Configuration 14

The processing device according to any one of Configurations 1 to 13, further comprising a combiner,a plurality of the decoder being provided, andthe combiner being configured to combine a decoded result acquired from one of the plurality of decoders and a decoded result acquired from another one of the plurality of decoders.
Configuration 15

The processing device according to any one of Configurations 1 to 13, whereina plurality of the decoders are provided, anda first learning condition in one of the plurality of decoders is different from a second learning condition in another one of the plurality of decoders.
Configuration 16

The processing device according to Configuration 15, wherein a first error function in the first learning condition is different from a second error function in the second learning condition.

Configuration 17

The processing device according to Configuration 15, wherein a sequence of the plurality of learning data values in the first learning condition is different from a sequence of the plurality of learning data values in the second learning condition.

Configuration 18

The processing device according to any one of Configurations 1 to 15, wherein the decoder includes a neural network structure.

Configuration 19

A magnetic recording/reproducing device, comprising: the magnetic reproduction processing device according to Configuration 4; andthe magnetic recording/reproducing portion.
Configuration 20

A magnetic reproducing method using the magnetic reproduction processing device according to any one of Configurations 1 to 18.

According to the embodiments, it is possible to provide a magnetic reproduction processing device, a magnetic recording/reproducing device, and a magnetic reproducing method that can suppress errors.

Hereinabove, exemplary embodiments of the invention are described with reference to specific examples. However, the embodiments of the invention are not limited to these specific examples. For example, one skilled in the art may similarly practice the invention by appropriately selecting specific configurations of components included in the magnetic reproduction processing devices such as decoders, error correction decoders, waveform controllers, contribution adjusters, etc., from known art. Such practice is included in the scope of the invention to the extent that similar effects thereto are obtained.

Further, any two or more components of the specific examples may be combined within the extent of technical feasibility and are included in the scope of the invention to the extent that the purport of the invention is included.

Moreover, all magnetic reproduction processing devices, all magnetic recording/reproducing devices, and all magnetic reproducing method practicable by an appropriate design modification by one skilled in the art based on the magnetic reproduction processing devices, the magnetic recording/reproducing device, and the magnetic reproducing method described above as embodiments of the invention also are within the scope of the invention to the extent that the purport of the invention is included.

Various other variations and modifications can be conceived by those skilled in the art within the spirit of the invention, and it is understood that such variations and modifications are also encompassed within the scope of the invention.

While certain embodiments have been described, these embodiments have been presented by way of example only, and are not intended to limit the scope of the inventions. Indeed, the novel embodiments described herein may be embodied in a variety of other forms; furthermore, various omissions, substitutions and changes in the form of the embodiments described herein may be made without departing from the spirit of the inventions. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the invention.

",Yousuke Isowaki,"Technology, Media, Retail, Information Storage","Data storage, recording devices, optical and magnetic storage, retrieval systems","According to one embodiment, a magnetic reproduction processing device includes a decoder. The decoder includes a convolutional layer including a plurality of filters, and an attention layer configured to derive a degree of contribution related to the filters. The decoder is configured to output a decoded result obtained by integrating results of processing an input signal with the filters according to the degree of contribution."
12020728,"TECHNICAL FIELD

The systems and methods disclosed herein relate generally to producing recordings of tracked events, such as in the context of indoor and outdoor tracked sporting and adventure activities.

SUMMARY

Example embodiments relate to a computing system that allows for the effective tracking of participants and operators of tracked activities and events, as well as capturing, modifying, and compiling video segments of the tracked activities and events, all of which may be used to make a video and/or other graphical representations of the tracked activities and events. In some embodiments, the computing system receives tracking data associated with a tracked event. In response to receiving the tracking data, the computing system executes an automated video processing procedure that comprises initiating one or more live video feeds of the tracked event, wherein each live video feed is streamed from a separate camera during the tracked event. Then, the computing system receives a first live video stream feed of a first portion of the tracked event and selects one or more segments of the first live video feed for further processing. Further, the computing system receives a second live video stream feed of a second portion of the tracked event and selects one or more segments of the second live video feed for further processing. Then, the computing system modifies each of the one or more segments of the received first and second live video feeds to improve performance of the automated video processing procedure and compiles the modified video segments into a first video of the tracked event.

Furthermore, because this video processing procedure is automated (or at least semi-automated), and occurs in real time (or substantially real time) while a participant is engaged in the tracked event and/or activity, any editing and/or compilation procedures also occur during the activity and are completed at the same time, or at substantially the same time, that the participant completes the activity, so the participant and the user are able to focus on the activity, and not the recording and/or editing processes. This result saves the participant (and the user) time and effort and allows them to focus on the activity exclusively and maximizes the participant's level of satisfaction with the activity and the experience.

In some embodiments, a method is provided that includes receiving, by a computing system, tracking data associated with a tracked event. The method further includes, in response to the receiving the tracking data, executing an automated video processing procedure comprising: (i) initiating one or more live video feeds of the tracked event, wherein each live video feed is streamed from a separate camera during the tracked event, (ii) receiving a first live video feed of a first portion of the tracked event, (iii) selecting one or more segments of the received first live video feed for further processing, wherein the one or more segments of the received first live video feed are selected based on the received tracking data, (iv) receiving a second live video feed of a second portion of the tracked event, (v) selecting one or more segments of the received second live video feed for further processing, wherein the one or more segments of the received first live video feed are selected based on the received tracking data; (vi) for each of the selected one or more segments of the received first live video feed and the second live video feed, modifying the selected one or more segments to improve performance of the automated video processing procedure; and (vii) compiling the modified one or more segments into a first video of the tracked event.

Some embodiments include a system comprising at least one processor and a tangible, non-transitory computer readable medium having stored therein instructions that, when executed by the at least one processor, cause the at least one processor to perform functions, including but not limited to the functions described herein. The system includes one or more tracking devices configured to transmit tracking date, which the system may use to determine a first video of a tracked event and/or activity should be generated, wherein the tracked event and/or activity comprises one or more portions of the tracked event and/or activity. In operation, in response to receiving the tracking data, the system executes an automated video processing procedure comprising: (i) initiating one or more live video feeds of the tracked event, wherein each live video feed is streamed from a separate camera during the tracked event, (ii) receiving a first live video feed of a first portion of the tracked event, (iii) selecting one or more segments of the received first live video feed for further processing, wherein the one or more segments of the received first live video feed are selected based on the received tracking data, (iv) receiving a second live video feed of a second portion of the tracked event, (v) selecting one or more segments of the received second live video feed for further processing, wherein the one or more segments of the received first live video feed are selected based on the received tracking data; (vi) for each of the selected one or more segments of the received first live video feed and the second live video feed, modifying the selected one or more segments to improve performance of the automated video processing procedure; and (vii) compiling the modified one or more segments into a first video of the tracked event.

It should be noted that this summary is for the purposes of illustration only. Specifically, this summary should not be read in any way as limiting to the aspects, examples, and embodiments of the claimed invention disclosed herein.

",utility,2024-06-25,Systems and methods for generating video compilations for tracked activities,B2,22.0,,,"G11B 27/36, G06T 7/80, G11B 27/0","G11B27/036,H04N21/2187,G06T7/80,G11B27/005,G11B27/031,H04N21/21805,H04N21/8549,G06T2207/10016","inventional,inventional,inventional,inventional,inventional,inventional,inventional,additional","Indexing scheme for image analysis or image enhancement-Image acquisition modality-Video; Image sequence, Image analysis-Analysis of captured images to determine intrinsic or extrinsic camera parameters, i.e. camera calibration, Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel-Reproducing at a different information rate from the information rate of recording , Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel-Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers-Electronic editing of digitised analogue information signals, e.g. audio or video signals, Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel-Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers-Electronic editing of digitised analogue information signals, e.g. audio or video signals-Insert-editing, Selective content distribution, e.g. interactive television or video on demand [VOD] -Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof-Server components or server architectures-Source of audio or video content ; , e.g. local disk arrays -enabling multiple viewpoints, e.g. using a plurality of cameras, Selective content distribution, e.g. interactive television or video on demand [VOD] -Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof-Server components or server architectures-Source of audio or video content ; , e.g. local disk arrays -Live feed, Selective content distribution, e.g. interactive television or video on demand [VOD] -Generation or processing of content or additional data by content creator independently of the distribution process; Content per se -Assembly of content; Generation of multimedia applications-Content authoring-Creating video summaries, e.g. movie trailer ","COMPUTING; CALCULATING OR COUNTING, INFORMATION STORAGE, ELECTRIC COMMUNICATION TECHNIQUE","IMAGE DATA PROCESSING OR GENERATION, IN GENERAL, INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER , PICTORIAL COMMUNICATION, e.g. TELEVISION",0.0,3.0,"DETAILED DESCRIPTION

Example methods and computing systems are described herein. Any aspect, example, or embodiment described herein should not be construed as preferred or advantageous over other potential embodiments or features. Instead, it should be easily and readily understood that certain aspects of the disclosed computing systems and methods can be arranged and combined in a host of different ways.

Additionally, the arrangements reflected in the Figures should not be regarded as limiting, but instead should be viewed as examples only. Further, these examples and others could contain more or fewer of any given element, a different arrangement of those elements, and/or, the deletion of one or more of those elements and/or the addition of one or more other elements.

I. Overview

In the context of recording indoor and outdoor tracked events and/or activities, cameras are often used to capture video while attached to one or more parts of the equipment used to facilitate the activity or activities. For example, if the activity is driving and/or racing recreational vehicles (e.g., go-karts, all-terrain vehicles, motorcycles, etc.) around a track (e.g., go-kart tracks, all-terrain vehicle tracks, etc.), one or more cameras may be attached to recreational vehicle and/or the participant. These configurations, however, may be focused, statically, on a particular angle or direction of the recreational vehicles and/or the participant's face.

When used in this fashion, the video captured by such cameras are often non-dynamic and of a static viewpoint, which may lead to uninteresting, unnecessarily long, and/or corrupted video segments—from the perspective of the participant, spectators, or others.

During the recording of such a tracked event, the it may be beneficial to record different portions of the tracked event from different cameras and, once the recording is completed, how to arrange the recorded footage to make a correctly ordered video and/or determine if the final video recording is compliant with one or more predetermined parameters.

Disclosed herein are systems configured to control multiple cameras and produce an auto-edited compilation of images and/or videos of a tracked activity, as well as methods for operating the disclosed systems. Specifically, the example embodiments disclosed herein allow a computing system, once it received tracking data associated with a tracked event, to accurately and efficiently carry out the tasks associated with capturing, altering, and compiling of video segments generated from live video feeds received by the computing system into a video or videos of the tracked event or activity.

In operation, the systems and methods disclosed herein allow the computing system, without intervention from the participant, spectator, and/or manager of the tracked event, to automatically detect a tracked event is happening, record and edit different portions of the tracked event, and compile a finished video of the tracked event. By doing so, the participant, spectators, and/or managers of the tracked event may focus their attention and efforts on participating in, overseeing, and/or spectating the tracked event instead of recording it. In a further aspect, the systems and methods disclosed herein are also applicable to any tracked activity, tracked event, tracked adventure sport, tracked amusement ride, tracked excursion tour, or similar circumstances which involves participants and where multi-captured and multi-angle footage is of benefit for tracked events and/or activities.

For example, in one scenario, the computing system is configured to receiving tracking data associated with a tracked event, and may do so in a number of ways. In one example, the computing system may receive data from a tracking sensor on recreational vehicles (e.g., a GPS sensor on a go-kart), at a track (e.g., a motion sensor that detects a go-kart going by on the tracks), and/or both, among other possibilities.

In any event, once the computing system receives this tracking data, and in some instances in response to receiving the tracking data, the computing system may execute an automated video processing procedure. In some embodiments, the automated video processing procedure comprises, among other elements: initiating one or more live video feeds of the tracked event, receiving a first live video stream feed of a first portion of the tracked event, and selecting one or more segments of the first live video feed for further processing. Furthermore, in some embodiments, the computing system may also receive a second live video stream feed of a second portion of the tracked event, selects one or more segments of the second live video feed for further processing, and then modify each of the one or more segments of the received first and second live video feeds to improve performance of the automated video processing procedure, all before and compiling the modified video segments into a first video of the tracked event.

In some embodiments, compiling the one or more modified video segments comprises combining the one or more modified video segments in a particular order. In some embodiments, the compilation may also include generating a graphical representation of one or more events that occurred during the tracked event (e.g., lap times for one or more go-karts that participated in a go-kart race around the track, the fastest speed for one or more go-karts that participated in a go-kart race around the track). In a further aspect, this generated graphical representation may be based on the tracking data received during the tracked event, and the computing system may interleave and/or overlay the graphical representation with a least a portion of the modified one or more segments, thereby causing the first video to depict an ordered sequence of events representative of the tracked event.

Additionally or alternatively, the computing system may combine the one or more modified video segments with previously recorded stock video segments (which may include intro/outro sequences, and/or video segments of similar activities or environmental surroundings, among other possibilities) and/or stock images (e.g., a company logo and/or intro slide associated with the tracked events), which may also be interleaved, potentially seamlessly, with the generated video segments and/or graphical representations, before being compiled into a video of the activity. In a further aspect, any or all of these segments (both those that are generated and those that were previously recorded) may be altered to ensure consistency across all the segments used in the compiled video of the activity. For example, the generated graphical representation may overlay the graphical representation over a video segment that includes the corresponding recreational vehicle, participant, or both (e.g., the fastest lap time graphic could overlay a video of the corresponding recreational vehicle and participant crossing the finish line).

Additionally or alternatively, the computing system may combine the one or more modified video segments and/or stock video segments/images with previously recorded stock audio music, soundtracks, and/or sound effects (which may include intro/outro music, and/or sound effects for similar activities or environmental surroundings, among other possibilities), and which may also be interleaved, potentially seamlessly, with the generated video segments and/or graphical representations, before being compiled into a video of the activity. In a further aspect, any or all of these audio segments (both those that are recorded during the tracked event and those that were previously recorded) may be altered to ensure consistency across all the audio segments used in the compiled video of the activity.

In other embodiments, at the completion of the activity, a copy of the video produced by the system is (or at least can be) provided to the participant, participants, or others via a variety of mediums. For example the video may be provided to one or more of these parties on a mobile storage device (e.g., a USB flash drive, a cloud-based storage system, etc.), by uploading the video to a website or physical kiosk for review and selection, uploading the video to a social media site/service associated with one of these parties, and/or transferred to a device associated with one of these parties (e.g., a camera, smart phone, tablet or other computing device), among other scenarios.

In some embodiments, the computing system automatically (or at least semi-automatically) adjusts its video capture parameters/settings, as well as the functionality of associated hardware and devices by examining one or more parameters associated with these components through the execution of one or more calibration sequences. For example, in some embodiments, the computing system obtains a calibration video segment from each camera associated with the computing system, compares the calibration video segment to a corresponding reference video segment to evaluate the level of consistency between the video segments, and then configures each camera so that video received from each camera has a consistent look and feel when initially received for processing from its respective camera, thereby reducing the amount of post-video-capture processing required for the compiled video to have a consistent look and feel across the different video segments making up the compiled video. And, in still further embodiments, the computing system automatically (or at least semi-automatically) adjusts the field of view for each (or at least some) of the associated cameras, by comparing the field of view for each camera to a corresponding reference field of view, and adjusting each camera's field of view to improve the likelihood that each camera will capture footage that includes the participants.

In other examples, the computing system, while executing an automated video processing procedure related to tracking data for a first portion of the tracked event (e.g., corresponding to a first portion of a go-kart track), may receive to tracking data for a second portion of the tracked event (e.g., corresponding to a second portion of a go-kart track). In response, the computing system may executes a second automated video processing procedure and may manage the received video streams and/or the steps of the first and second automated video processing steps to ensure no critical data is lost during the execution of one or both of the automated video processes. In some examples, the computing system facilitates this management by executing the first and second automated video processing procedures at the same time (e.g., by parallel processing or other similar processing techniques), and may potentially execute a third, fourth, fifth, or more automated video processing procedures in a similar fashion.

The embodiments described herein that are configured to select and process live video feeds from individual cameras without participant intervention have multiple advantages over alternative procedures that may include collecting and processing video footage captured and operated by one or more participants.

First, because the cameras transmit live video feeds to the computing system in response to tracking data that occurs during the tracked activity, there is no input required from a system operator or event participant to begin, facilitate, or end the live feeds, editing, and/or video compilation of the tracked event. In this regard, the computing system is able to be powered on and ready to operate at any moment and provide a robust video compilation of the tracked activity that will not be corrupted by operator mistakes (e.g., a go-kart track operator forgetting to turn the track cameras on during the activity).

Second, because the computing system can begin and/or end the live streaming feeds from the tracked cameras in response to tracking data corresponding to events occurring during the tracked event, the cameras are not required to record and/or stream at all times. Instead, the computing system may track one or more participants in the tracked event, and only record when the one or more participants are within the field of view of the particular camera. By doing so, the computing system may save computational resources (e.g., storage space associated with videos recorded while the participants are not in the camera's field of view), computing system components (e.g., the system cameras may last longer because of non-continuous use), and/or other costs associated with operating the computing system.

Third, because individual cameras transmit only enough of a live video feed to capture when the participant is in the camera's field of view, the video segments associated with that portion of the live video feed may be transmitted for further processing faster and in more real time. Furthermore, as described above, this process may begin and end without user intervention or input and lead to faster video segment editing and/or modification, as well as faster video compilation because the process automatically occurs in response to the participant being actively engaged in the tracked activity. In a further aspect, because this process is occurring while the participant is engaged in the tracked activity, the computing system may be undertaking the video processing procedure and then, based on the participant finishing the activity, cease recording any video segments and finish the video compilation at the same (or substantially the same) time.

Fourth, because the computing system can access a live video feed from each camera, the computing system can also calibrate and adjust an individual camera's video parameters based on the tracking data associated with tracked event. In some embodiments, the calibration and adjustment procedure is performed before each new activity starts, but the calibration and adjustment could be performed with nearly any desired frequency (e.g., every 5 minutes, 10 minutes, 15 minutes, hour, 2 hours, daily, and so on).

In a further aspect, if for sale, the participant may be more inclined to purchase a finalized video that is immediately available after completing the activity because the participant is still is still engaged in the moments of excitement immediately following the activity, which may diminish quickly. Additionally, such an inclination for purchasing the finalized video benefits the user (and any associated parties) as well as the participant (e.g., if excited about the finalized video, the participant may be more likely to buy other related products from the user, submit positive reviews concerning their impressions of the activity and/or the user, upload/post the finalized video on social media sites (thereby promoting the activity and/or the user, among other such possibilities).

Other advantages of the systems and methods disclosed herein will become apparent to those of skill in the art upon review of the figures and following detailed description.

II. System Architecture

In the following sections, references will be made, in some detail, to various embodiments of the systems and methods disclosed herein. Although the following detailed descriptions provide many details to provide a context to ensure a full understanding of the present disclosure for the reader, these descriptions should not be viewed as limitations. Further, the disclosed systems and methods may be accomplished without all of these specific details.

FIG.1shows a system100, in accordance with example embodiments. More specifically, system100includes one or more cameras104in communication with a computing system102via a source network106and one or more destination devices120in communication with the computing system102via a distribution network118. In a further aspect, in this example, computing system102includes one or more video processing engines108, a stock media library110, a media database112, tracking engine114, and a controller116. Furthermore, in some embodiments, one or more of these components may be encased in one or more weatherproof units.

In some embodiments, tracking engine114comprises one or more processors programmed with the software that, when executed by the one or more processors, causes or otherwise enables the tracking engine114to communicate with and/or control one or more other components in computing system102and/or system100. For example, in some embodiments, tracking engine114communicates with one or more tracking devices and, in response to receiving tracking data from the one or more tracking devices, communicates with cameras104via a source network106.

In some embodiments, controller116comprises one or more processors programmed with the software that, when executed by the one or more processors, causes or otherwise enables the controller116to communicate with and/or control one or more other components in computing system102and/or system100. For example, in some embodiments, controller116communicates with one or more cameras104via a source network106.

In some embodiments, in response to receiving tracking data from tracking engine114, controller116selects one or more live video feeds received from the one or more cameras104. In some embodiments, each live video feed is obtained by an individual camera. In operation, tracking engine and/or controller116can communicate with the one or more cameras104via the source network106in one or more of a variety of ways. For example, this communication may be accomplished via wired connections (e.g., High-Definition Multimedia Interface (HDMI) cables, coaxial cables, Ethernet cables, or any other suitable wireline communication technology now known or later developed) and/or wireless connections (e.g., WIFI, VHF or UHF radio frequencies, or other suitable wireless technologies now known or later developed), among other possibilities.

Additionally, in some embodiments, one or more cameras104may be in direct communication with the computing system102(e.g., via a source network106). In a further aspect, one or more cameras104may be controlled by one or more components of the computing system102(e.g., via a source network106), including to record and/or otherwise capture video clips (e.g., from live feeds) and send them directly to computing system102. In other embodiments, cameras source network106may be connected to one or more switching devices controlled by tracking engine114and/or the controller116, facilitating which live video feeds are passed from the one or more cameras104to the computing system102at any given time.

For example, in embodiments where the one or more cameras104are connected to the computing system102via HDMI cables, the source network106includes an HDMI switch (not shown), where the HDMI switch has one input for each of the one or more cameras104, and at least one output to the computing system102. In operation, the controller116controls which of the live video feeds on each of the HDMI inputs from each camera are passed to the HDMI output(s) to the computing system102for processing according to the methods and procedures disclosed and described herein. In some embodiments, the above-described HDMI switch may be a separate component of the source network106or an integrated component of the computing system102.

In another example, rather than controlling an HDMI switch (or other type of switch) configured to implement a switching scheme for passing one or more live video feeds to the computing system for processing, the tracking engine114and/or controller116may instead directly control the one or more cameras104to cause each camera to transmit a live video feed to the computing system at particular times. In yet another example, each camera is configured to transmit a live video feed via a wireless or wireline network to the computing system102on a separate physical or virtual channel (e.g., a separate RF channel in a frequency division multiplexed (FDM) scheme, a separate time slot channel in a time division multiplexed (TDM) scheme, a separate source IP address, e.g., the camera's IP address, in a video over IP scheme) and the controller is configured to tune or otherwise configure a physical or virtual receiver (not shown) of the computing system102to receive and process one or more of the live video feeds at a particular time.

In a further aspect, in order to determine which one or more live video feeds captured by one or more cameras104are selected for further processing, the controller116in some embodiments relies on information received from the tracking engine114, a set of predetermined instructions (e.g., a script that the processors of the controller execute), and/or other input data.

For example, in some embodiments, in response to receiving tracking data from one tracking device associated with tracking engine114(e.g., a tracking sensor at the start line of a race), the computing system executes an automated (or at least semi-automated) video processing procedure. In other embodiments, however, the computing system may execute, continue, and/or cease an automated (or at least semi-automated) video processing procedure in response to receiving tracking data from more than tracking device associated with tracking engine114(e.g., a tracking sensor at the start line of a race, GPS sensors in one or more recreational vehicles in the race, one or more tracking sensors at various portions of the race track, and/or a tracking sensor at the finish line of a race). In other embodiments, the computing system may receive this tracking data wirelessly (e.g., broadcasting over WiFi), from one or more wired devices, and/or other devices and/or technologies.

In still other embodiments, once the one or more segments of the received live video stream feeds are selected for processing, those segments are received by a video processing engine108that processes the received segments in one or more ways. In one example, the video processing engine modifies the selected segment(s) to improve the performance of the computing system in connection with performing the automated video processing procedure. In some embodiments, this modification may include, among other possibilities, compressing the selected segment(s) to reduce the processing load for generating the compiled video, among others.

In a further aspect, once a selected segment has been modified, the video processing engine108generates one or more video segments based on one or more of the modified video feeds, which may include a variety of video processing techniques (e.g., recording the video segment in real time and saving it in one or more formats).

In a further aspect, video processing engine108may use tracking data from tracking engine114to analyze the video segments and/or further influence how the video segments are modified and/or compiled. For example, video processing engine108may use tracking data to select and/or modify video segments so that the one or more particular event participants (e.g., a particular go-kart operator) are in the video frame across all cameras and/or camera angles for the entire tracked event. In another example, video processing engine108may use tracking data to select and/or modify video segments that pertain to the one or more particular event participants (e.g., the particular go-kart operator) and disregard any video segments that do not include the one or more particular event participants. Other examples are possible.

In yet another aspect, video processing engine108compiles the one or more generated video segments into a video of the tracked event in a variety of ways.

For example, in some embodiments, video processing engine108compiles the one or more generated video segments into the video of the activity by combining the one or more generated video segments into the video of the activity with no further editing or input.

In other embodiments, however, video processing engine108generates a graphical indication of one or more events that occurred during the tracked event. In some embodiments this graphical representation may include and/or be based on tracking data received during the tracked event. In a further aspect, video processing engine108may overlay and/or interleave the generated graphical representation with the modified video segments, and then compiles both the generated graphical representations and generated video segments into the video of the tracked activity.

In other embodiments, the video processing engine108selects one or more stock video segments from stock media library110, interleaves the selected stock video segments with the generated video segments, and then compiles both the stock and generated video segments into the video of the activity.

In some embodiments, the computing system102(or one or more components thereof) selects the stock video segments based on data detected concerning the operational parameters and/or environmental factors detected by the computing system during the tracked event. For example, the computing system may detect information regarding one or more relevant factors concerning system100(e.g., current weather conditions or other environmental information, including lighting, time of day, etc.), which may influence the video processing engine's selection of the stock video segments (e.g., if it is cloudy/overcast, the video processing engine may select only the stock video segments that reflect cloudy/overcast conditions).

In further examples, video processing engine108may also alter the generated graphical representations, selected stock video segments, the modified video segments, or both, to achieve a consistent look across the compiled video. For example, if it is rainy and overcast during the tracked event, the computing system may modify the selected live and/or stock video segments, generate graphical representations of the tracked event reflecting the conditions (e.g., cartoon rainclouds and/or thunder next to the fastest lap time), or both, among other possibilities.

In a further aspect, once the video of the tracked activity has been completed and the video is ready to be transferred, the video may be sent (via the video processing engine108or the controller116) to the media database112. In one example, media data base112may serve as both a repository for finished videos (e.g., a storage device for such videos), output interface (e.g., network interface), or both. In a further aspect, the media database112may also facilitate communication between one or more destination devices120and the computing system102via a distribution network118.

Under either scenario, in example embodiments, similar to communications via source network106, communications with the one or more destination devices120(e.g., mobile devices, computing devices, kiosks, and/or other devices associated with the user, the participant, or another party) via the destination network118may be accomplished in a variety of ways (e.g., via wired, wireless, and/or other transmission technologies). In some embodiments, the video may transferred to a SD/USB medium or other tangible memory media. In other embodiments, the video may be uploaded to a website for later download by a party, such as the activity participant. In other embodiments, the video may also be transferred via distribution network118to a participant's portable electronic device (e.g., camera, smart phone, tablet computer, or other portable electronic device).

III. Computing Device

FIG.2illustrates computing device200in accordance with example embodiments, which may be configured to perform at least one function of computing system102, as previously described.

Computing device200includes one or more user interfaces201, a network-communication interface202, one or more processors204, and a data storage device206with computer-readable instructions208stored thereon, all of which may be linked together by a connector203, such as a system bus or similar mechanism.

In operation, the user interface201may be configured to receive and/or convey information and/or data to/from a user. In one example, the user interface201is a touch screen that displays one or more graphical user interfaces to receive and convey information from and to the user. In other examples, the user interface201receives information and/or data from a user through the use of a computer mouse, a keyboard, one or more cameras, and/or one or more microphones (which may also work in connection with voice recognition software), among other such devices. In other examples, the user interface201is configured to convey information and/or data to a user through the use of display devices (which may include LCD, LED, and/or plasma, among other, screens, and/or other similar devices, either now known or later developed) and/or speakers (or an audio output port configured to send an audio signal to an external device), among other similar devices.

Computing device200also includes one or more network-communications interfaces202, which may comprise one or more wireless interfaces212and/or one or more wired interfaces214, both or either of which may be configured to communicate via a network. For example, wireless interfaces212may include wireless receivers, wireless transmitters, and/or wireless transceivers, one or more of which may employee technologies such as Bluetooth, WIFI, and/or other similar types of similar technologies. In other examples, wired interfaces214may include one or more wired receivers, wired transmitters, and/or wired transceivers, one or more of which may employee technologies such as coaxial, Ethernet, USB, and/or other similar types of similar technologies.

In some example embodiments, the wired interfaces214include one or more HDMI interfaces, where each HDMI interface is configured to receive a live video feed from one of the one or more cameras104shown and described inFIG.1.

Computing device200also includes one or more processors204, which are configured to execute computer-readable program instructions208that are contained in data storage206and/or other instructions as described herein. In operation, the one or more processors204are configured to execute the software for controlling the other components of computing device200.

Computing device200also includes one or more data storage devices206, one or more of which may include one or more computer-readable storage media that can be read and/or accessed by the at least one or more processors204. In a further aspect, one or more data storage devices206can also contain, among other data, computer-readable program instructions208. And, in some embodiments, data storage206can additionally include instructions required to perform at least part of the methods and method steps described herein. In some embodiments, the one or more data storage devices206are also configured to store (at least temporarily) one, more, or all of the received live video feeds from the one or more cameras104, the video segments derived from the live feeds, the generated graphical representations, the stock media library, altered stock and/or modified video segments, and compiled videos generated by the computing system102.

Computing device200also includes one or more tracking modules210(e.g., a GPS module), one or more of which may send tracking data the at least one or more processors204. In some embodiments, tracking module210can additionally include functionality required to perform at least part of the methods and method steps described herein. In some embodiments, the one or more tracking modules210are also configured to generate and/or store (at least temporarily) some or all of the tracking data utilized by the computing system102.

IV. Tracking and Editing Computing System and Architecture

In the following sections, references will be made, in some detail, to various embodiments of the systems and methods disclosed herein. Although the following detailed descriptions provide many details to provide a context to ensure a full understanding of the present disclosure for the reader, these descriptions should not be viewed as limitations. Further, the disclosed systems and methods may be accomplished without all of these specific details.

FIG.3shows a system300, in accordance with example embodiments. More specifically, system300includes several tracking devices (some of which are shown inFIG.3as vehicle tracking device304, start position tracking device306, first course position tracking device308, second course position tracking device312, third course position tracking device314, fourth course position tracking device318, near finish course position tracking device320, and finish course position tracking device324), at least some of which are in communication with tracking and editing computing system302. In a further aspect,FIG.3shows several cameras (some of which are shown inFIG.3as first camera310, second camera316, and third camera322), at least some of which are also in communication with tracking and editing computing system302. In yet a further aspect, tracking and editing computing system302is also in communication with distribution devices326. In a further aspect, in this example, computing system302includes one or more of the components illustrated inFIGS.1and2(e.g., video processing engines108, a stock media library110, media database112, tracking engine114, and/or controller116, as shown inFIG.1). Furthermore, in some embodiments, one or more of these components may be encased in one or more weatherproof units.

In some embodiments, tracking and editing computing system302receives tracking data associated with a tracked event. As shown inFIG.3, tracking and editing computing system302receives tracking data associated with a tracked event, a go-kart race on a go-kart track. In one aspect, tracking and editing computing system302receives tracking data from vehicle tracking device304, which is positioned on a recreational vehicle (e.g., a go-kart shown inFIG.3in the “pit,” an area where racing vehicles are often kept before and after a race). Once a participant engages the vehicle with the vehicle tracking device304, tracking data associated with that vehicle (e.g., position, speed, etc.) may be transmitted to tracking and editing computing system302from vehicle tracking device304. Based on this tracking data from vehicle tracking device304, the tracking and editing computing system302may take one or more responsive actions. For example, based on receiving tracking data from the vehicle tracking device304, tracking and editing computing system302may create one or more profiles associated with the participant and/or vehicle containing the vehicle tracking device304. Additionally or alternatively, based on receiving tracking data from the vehicle tracking device304, tracking and editing computing system302may initiate the automated video processing procedure described, at least, inFIG.1. Other examples are possible.

For example, in another example embodiment tracking and editing computing system302may receive tracking data from start position tracking device306, which is positioned on or at the starting line of the track (shown inFIG.3as the “START” line, an area where racing vehicles go after leaving the pit, and where the tracked event/race begins). Once the participant/recreational vehicle crosses the “START” line, start position tracking device may transmit tracking data associated with that vehicle (e.g., data indicating the vehicle has crossed the “START” line, vehicle position and speed, etc.) to tracking and editing computing system302. In some examples, start position tracking device306may be made up of one or more devices that indicate that the vehicle has crossed the “START” line, including one or more electronic gates, photo sensors, cameras, motion sensors, infrared beam sensor, and/or electronic sensors, including RFID tags and sensors, any or all of which may be imbedded into the track itself. In a further aspect, based on receiving this tracking data from start position tracking device306, the tracking and editing computing system302may take one or more responsive actions. For example, based on receiving tracking data from the start position tracking device306, tracking and editing computing system302may create one or more profiles associated with the participant and/or vehicle crossing the “START” line and/or otherwise interacting start position tracking device306. Additionally or alternatively, based on receiving tracking data from the start position tracking device306, tracking and editing computing system302may initiate the automated video processing procedure described, at least, inFIG.1. Other examples are possible.

In a further aspect, once the tracking and editing computing system302receives tracking data from the vehicle tracking device304and/or the start position tracking device306and initiates the automated video processing procedure, the computing system may take one or more additional responsive actions using the components illustrated inFIG.3. For example, tracking and editing computing system302may initiate one or more live feeds from one or more cameras placed around the track. For example, as shown inFIG.3, once the tracking and editing computing system302receives tracking data from the vehicle tracking device304and/or the start position tracking device306, the computing system might initiate a live video feed from first camera310. In an example embodiment, once the vehicle crosses the “START” line and turns the corner towards first course position tracking device308, the vehicle will be in the field of view of first camera310, and, thus, a subject of the footage streamed and/or recorded by first camera310and sent to computing system302.

In a further aspect, tracking and edit computing system302may receive updated tracking data from additional tracking devices on the track, the vehicle, or both, and take one or more responsive actions based on this updated tracking data. For example, as the vehicle passes first course position tracking device308, the computing system302may receive the updated tracking data and cause first camera310to take one or more additional actions. For example, based on the tracking data from first course position tracking device308, computing system302may cause first camera310to capture one or more images from the live video stream. Additionally or alternatively, based on the tracking data from first course position tracking device308, computing system302may cause first camera310to switch to a higher quality live video stream (e.g., HD 720 to HD 1080). Additionally or alternatively, based on the tracking data from first course position tracking device308, computing system302may select on or more segments of the live video stream from first camera310for further processing. Other examples are possible.

In any event, in these examples, computing system302may take the one or more additional actions based on the updated tracking data from first course position tracking device308because, when crossing first course position tracking device308, the vehicle will directly in the field of view of first camera310. In this regard, a subject of the footage streamed and/or recorded by first camera310at a track position corresponding to first course position tracking device308may be more desirable.

In a further aspect, as the vehicle continues around the track, tracking and edit computing system302may receive updated tracking data from second course position tracking device312, and take one or more responsive actions based on this updated tracking data. For example, as the vehicle passes second course position tracking device312, the computing system302may cause first camera310to disable and/or otherwise end the live video stream (e.g., because the vehicle is out of the field of view of the first camera310) and also cause second camera316to initiate a live video stream (e.g., in order to begin capturing the vehicle, which will be coming into the field of view of second camera316).

Furthermore, in an example embodiment, based on the tracking data from third course position tracking device314, computing system302may cause second camera316to capture one or more images from the live video stream. Additionally or alternatively, based on the tracking data from third course position tracking device314, computing system302may cause second camera316to switch to a higher quality live video stream (e.g., HD 720 to HD 1080). Additionally or alternatively, based on the tracking data from third course position tracking device314, computing system302may select on or more segments of the live video stream from second camera316for further processing, potentially with one or more segments selected by computing system302from the live stream from first camera310. Other examples are possible.

In a further aspect, in an example embodiment, as the vehicle continues around the track, tracking and edit computing system302may receive updated tracking data from fourth course position tracking device318, and take one or more responsive actions based on this updated tracking data. For example, as the vehicle passes fourth course position tracking device318, the computing system302may cause second camera316to disable and/or otherwise end the live video stream (e.g., because the vehicle is out of the field of view of the second camera316). Other examples are possible.

For example, as the vehicle continues around the track and nears the finish line (shown inFIG.3as “FINISH” line), tracking and edit computing system302may receive updated tracking data from near finish course position tracking device320and/or finish course position tracking device324. For example, once the recreational vehicle/participant crosses the “FINISH” line, finish course position tracking device324may transmit tracking data associated with that vehicle (e.g., data indicating the vehicle has crossed the “FINISH” line, vehicle position and speed, etc.) to tracking and editing computing system302. In some examples, finish course position tracking device324may be made up of one or more devices that indicate that the vehicle has crossed the “FINISH” line, including one or more electronic gates, photo sensors, cameras, motion sensors, infrared beam sensor, and/or electronic sensors, including RFID tags and sensors, any or all of which may be imbedded into the track itself. In a further aspect, based on receiving this tracking data from finish course position tracking device324, the tracking and editing computing system302may take one or more responsive actions.

For example, as the vehicle passes near finish course position tracking device320, the computing system302may cause all other cameras to disable and/or otherwise end the live video streams (e.g., because the vehicle is about to cross the “FINISH” line and finish the race) and also cause third camera322to initiate a live video stream (e.g., in order to begin capturing the vehicle, which will be crossing the “FINISH” line directly in the field of view of third camera322).

Furthermore, in an example embodiment, based on the tracking data from updated tracking data from near finish course position tracking device320and/or finish course position tracking device324, computing system302may cause third camera322to capture one or more images from the live video stream, some of which may include images and/or higher resolution video segments of vehicle crossing the finish line. Additionally or alternatively, based on the tracking data from near finish course position tracking device320, computing system302may cause third camera322to switch to a higher quality live video stream (e.g., HD 720 to HD 1080) and/or select on or more segments of the live video stream from third camera322for further processing, potentially with one or more segments selected by computing system302from the live stream from third camera322. Other examples are possible.

For example, once the vehicle passes the “FINISH” line, finish course position tracking device324may transmit tracking data to computing system302that causes the computing system to modify the selected one or more video segments and begin to compile a video of the tracked event using video segments from first camera310, second camera316, and third camera322, among others. Based on this tracking data from finish course position tracking device324, the computing system302may also generate a graphical indication of one or more events that occurred during the tracked event, some or all of which are based on tracking data received during the tracked event and combine these generated graphical representations with the video segments and/or images selected from the live video streams of the tracked event. Additionally or alternatively, on this tracking data from finish course position tracking device324, the computing system302may also select one or more stock video segments from a library of previously recorded stock video segments based on one or more events that occurred during the track. Like the generated graphical representations, some or all of the stock video segments may be combined with the video segments and/or images selected from the live video streams of the tracked event to create a video of the tracked event.

In still other examples, the tracking data received by computing system302may be used to facilitate compiling the selected video segments, graphical representations, and/or stock video segments in a manner that allows the compiled video of the tracked event to reflect an ordered sequence of events (e.g., showing the vehicle racing around the track and crossing the “FINISH” line). The computing system302may do so in a number of ways. For example, the computing system may use the tracking data received from the tracking devices inFIG.3to determine the time at which each video stream, video segment, and/or image was generated and create a time stamp associated with each video stream, video segment, and/or image. Furthermore, in an example, the computing system302may use these timestamps to order the video stream, video segments, and/or images when compiling the video that is made of these video streams, video segments, and/or images. The computing system302may also use these timestamps to select corresponding stock video segments and/or images that share tracking data, timestamps, and/or other characteristics with each video stream, video segment, and/or image to use in compiling the video of the tracked event. In this regard, the computing system302may compile all of the video segments (stock or recorded during the event), graphical representations, and/or images (stock or captured during the event) in a manner that appears to be footage of the tracked event.

In a further aspect, the computing system may also alter these the video segments (stock or recorded during the event), graphical representations, and/or images (stock or captured during the event) in a manner that appears to be footage of the tracked event. Specifically, one or both of these video segment types may be altered by adjusting the playback speed of the video segments, and/or one or more of the following parameters associated with the video segments: brightness, contrast, grading, saturation, color balance, or image filter parameters. Under any of these scenarios, however, by adjusting these parameters, the computing system may achieve a more consistent look/visual appearance across all of the video segments, graphical representations, and/or images.

In any event, once the video of the tracked activity has been completed and the video is ready to be transferred, the video may be stored in one or more media databases of tracking and editing computing system302, which may serve as both a repository for finished videos (e.g., a storage device for such videos), output interface (e.g., network interface), or both. In a further aspect, computing system may then transmit the finished video to one or more destination devices326(e.g., mobile devices, computing devices, kiosks, and/or other devices associated with the user, the participant, or another party) for previewing by (or transfer/sale to) the participant after the race. In other embodiments, the finished video may be uploaded to a website for later download by a party, such as the participant. In other embodiments, based on creating an associated user profile at the beginning of the race, the video may also be transferred directly to the participant's portable electronic device and/or an electronic device associated with the user profile (e.g., camera, smart phone, tablet computer, or other portable electronic device).

In still other example embodiments, with multiple vehicles participating in a tracked event, the computing system302may use the tracking data associated with each of the vehicles in the tracked event to create videos that are individualized for each vehicle and participant, as well as videos, graphical representations, and/or images that reflect the participation of all participants in the tracked event. For example, based on the tracking data for each vehicle in the tracked event, the computing system may generate a graphical representation of a ranked leaderboard showing the results of the tracked event and use data associated with each participant to supplement the graphical representation (e.g., one participant's photo next a “FIRST PLACE: CHAMPION” graphic and/or total time to finish the race and another participant's photo next a “SECOND PLACE: RUNNER UP” graphic and/or total time to finish the race). In this regard, the participants may all view the results of the tracked and elect to buy and/or otherwise transfer videos and/or images that are finished and ready for transfer by the time the participants leave the vehicles and interact with the destination devices. Other examples are possible.

V. Method Flowchart and Example Embodiments

FIG.4illustrates a flowchart showing an example method400executed by the computing system to capture, alter, and compile video segments into a first video of a tracked even. Method400may be carried out by one or more computing devices, such as computing system102, computing device200, and/or computing system300, as illustrated and described with respect toFIGS.1,2, and3. In additional examples, method400may be carried out by computing system102, computing device200, and/or computing system300, operating as part of a cloud-based computing system. Additionally, method400may be performed by one or more other types of computing systems and devices other than those illustrated inFIGS.1,2, and3.

Further, the functionality described in connection with the flowcharts described herein can be implemented via one or more parts or components of the computing systems and computing devices illustrated inFIG.1(e.g., video processing engine108, tracking engine114, and/or controller116),FIG.2(e.g., processors204, data storage device206with computer-readable instructions208, and/or tracking module210), and/orFIG.3(e.g., tracking and editing computing system302, vehicle tracking device304, start position tracking device306, first course position tracking device308, second course position tracking device312, third course position tracking device314, fourth course position tracking device318, near finish course position tracking device320, and finish course position tracking device324, first camera310, second camera316, and third camera322, and/or distribution devices326).

Moreover, for each block of the flowchart illustrated inFIG.4, the circuitry and/or components shown in and described with reference toFIGS.1,2, and3allows each block to be performed under the functionality of the specific component, as well as part of the functionality of the device on which the specific component is implemented. Further, the blocks of the flowchart illustrated inFIG.4may be executed in or out of the order shown, as long as the overall functionality of the described method is maintained.

At block410, method400includes receiving tracking data associated with a tracked event. In some embodiments, the computing system receives the tracking data as illustrated inFIGS.1,2, and3.

In some examples, in response to receiving the tracking data, the computing system performs one or more aspects of an automated (or at least semi-automated) video processing procedure. In some embodiments, after the computing system receives the tracking data, the computing system may not undertake more than one automated (or at least semi-automated) video processing procedure until the automated video processing procedure is completed (e.g., for one vehicle/participant of the tracked event). In other examples, however, after the computing system receives tracking data corresponding to one vehicle/participant, the computing system may receive tracking data corresponding to one or more additional vehicles/participants in the tracked event, some or all of which may require their own individual automated (or at least semi-automated) video processing procedure, in spite of the non-completion of the first automated video processing procedure already initiated by the computing system.

For example, during an automated video processing procedure concerning a first video of a vehicles/participants, the computing system may receive tracking data corresponding to one or more additional vehicles/participants in the tracked event and coordinate all system architecture (e.g., the cameras illustrated inFIG.3) based on tracking data corresponding to each vehicle/participant. In one example, the computing system may undertake several automated (or at least semi-automated) video processing procedures in parallel, each of which may correspond to each vehicle/participant. In other examples, the computing system may prioritize certain automated (or at least semi-automated) video processing procedures corresponding to different vehicles/participants. For example, from the outset of the tracked event, some participants may pay a premium to have their corresponding video of the tracked event prioritized for faster completion. In other examples, based on receiving tracking data for a vehicle/participant that ends the tracked event early (e.g., by wrecking the vehicle), the computing system may prioritize or discontinue the automated (or at least semi-automated) video processing procedure corresponding to that participant (e.g., because no further video segments are required).

At block420, method400further includes, in response to receiving the tracking data, executing a first automated video processing procedure. In operation, the video processing procedure engages the computing system to perform a variety tasks that cause the computing system to capture, alter, and compile video segments into a video of a tracked activity.

For example, at block420a, method400includes initiating one or more live video feeds of the tracked event. In some but not all embodiments, each live video feed is obtained from a separate camera. In operation, the computing system may receive one or more live video feeds using devices and systems such as those illustrated inFIGS.1,2, and3. In some embodiments, the computing system does not need to request the separate cameras to begin recording or transmitting video. Instead, the computing system obtains the one or more live video feeds by receiving live video feeds for one or more separate cameras that are each transmitting live video feeds to the source network106(FIG.1).

At block420b, method400includes receiving a first live video feed of a first portion of the tracked event.

At block420c, method400includes selecting one or more segments of the received first live video feed for further processing. In particular, the computing system may select one or more live video feeds received from one or more of the separate cameras based on the received tracking data, by referencing a predetermined script, or a combination of the two.

At block420d, method400includes receiving a second live video feed of a second portion of the tracked event.

At block420e, method400includes selecting one or more segments of the received second live video feed for further processing. In particular, the computing system may select one or more live video feeds received from one or more of the separate cameras based on the received tracking data, by referencing a predetermined script, or a combination of the two.

In other embodiments, the computing system may select the one or more live video feeds based on a combination of tracking data and a set of time-based intervals. More specifically, once the computing system receives the tracking data, the computing system may initiate a live stream for one or more cameras corresponding that tracking data for a predetermined duration of time (e.g., initiating a live video stream from a first camera for 20 seconds, then initiating a live video stream from a second camera for 20 seconds).

At block420f, method400further includes, for each of the selected one or more segments of the received first live video feed and the second live video feed, modifying the selected one or more segments to improve performance of the automated video processing procedure. In particular, the computing system may modify the selected one or more segments in one or more ways that improve the computing system's ability to accurately and efficiently execute the automated video procedure.

For example, in some embodiments, the computing system modifies the one or more segments by one or more data processing techniques, filters, and/or similar technologies. For example, to improve performance of the automated video processing procedure, the computing system may compress the one or more selected segments individually, as a group, or by some other categorization. More particularly, the computing system may compress these selected segments by reducing the overall file size. In other examples, the file size of the segments may be compressed by converting portions of the segments to different file or video formats. In other embodiments, the computing system may modify the selected segments in other ways (e.g., re-encoding, motion blur reduction, color/contrast adjustments, culling mechanisms, etc.).

In a further aspect, these segments may be generated and/or stored in a variety of formats, including: Video MPUs (.MPU), Audio Video Interleave (.AVI) format, Flash Video (.FLV) format, Windows Media Video (.WMV) format, Apple QuickTime Movie (.MOV) format, Moving Pictures Expert Group 4 (.MP4) format, and/or other similar video formats, depending on the specific scenario for which the computing system will ultimately utilize the one or more segments.

At block420g, method400further includes compiling the modified one or more segments into a first video of the tracked event. In particular, the computing system may compile the one or segments into a first video of the tracked event by combining the one or more segments directly (e.g., stitching the segments together in a particular order) and/or in combining them with other segments (e.g., previously recorded stock video segments).

In other embodiments, the computing system may compile the one or more segments with stock video and/or audio segments, generated graphical representations of the tracked event, images captured during the tracked event, and/or stock images, all or some of which may be selected and/or modified based on tracking data received by the computing system during the tracked event. Under any of these scenarios, utilizing the tracking data results in a compiled video that depicts an ordered sequence of events that is representative of the captured activity.

In other embodiments, the computing system may adjust an operational parameter of the automated video processing procedure based on the tracking data. For example, based on the tacking data, the computing system may adjust at least one of the following operational parameter of the automated video processing procedure: (i) a duration of the first live video feed of the first portion of the tracked event; (ii) a duration of the second live video feed of the second portion of the tracked event; (iii) a modification of the selected one or more segments of the received first live video feed and the second live video feed to improve performance of the automated video processing procedure; and (iv) an order of compilation for the modified one or more segments into the first video of the tracked event.

IV. Further Example Embodiments

In other embodiments, the methods described herein may further include disabling the received first and second live video feeds of the tracked event, wherein the live video feeds are disabled based on tracking data indicating that the tracked event has ended.

In a further aspect, because each displayed live video feed is obtained from a separate camera, this information may be helpful for the user to review.

In other embodiments, the methods described herein may further include the computing system selecting a set of images from one or more of the selected segments. In a further aspect, the selection of these images may be based on: a predetermined script the computing system may reference (e.g., selecting an image at the top of every second over a 20-second video segment), input from the user (e.g., user input indicating the selection of one or images displayed on the computing system as part of a displayed live video feed), and/or some other related technology or system input. In a further aspect, the computing system may also save the selected images in one or more places (e.g., a virtual folder).

In other examples, the methods described herein may further include the computing system storing a copy of the finalized video (e.g., the first video of the tracked activity) in a tangible, non-transitory readable memory of the computing system. In a further aspect, the storage of this video may be duplicative (e.g., storing a first copy of the video in a first tangible, non-transitory computer-readable memory of the computing system and storing a second copy of the video in a second tangible, non-transitory computer-readable memory of a separate computing system).

In a further aspect, a copy of this video may be provided to a participant of the captured video in a variety of ways, including: stored on a mobile storage device (e.g., a USB flash drive, a cloud-based storage system, etc.), uploading the video to a website or physical kiosk for the participant's review and selection, uploading the video to a social media site/service associated with the user, the participant, or some other party, or transferred to a device associated with the user, the participant, or some other party (e.g., a camera, smart phone, tablet or other computing device), among other scenarios. Additionally, although such duplicative transfer and/or storage may improve convenience for the user, the participant, or some other party, these copies may also be used as backup redundant copies in the event of a failure or destruction of one of the aforementioned devices. Further, although these example embodiments generally relate to one activity participant, they apply to groups of activity participants as well (e.g., for a group of ten participants, there may be ten videos and/or related media created and transmitted via WIFI to a kiosk at visitation center, where all ten participants may review and/or purchase placed one or more of the aforementioned finalized product).

In a further aspect, the computing system may also manage the steps of the automated video processing steps to reduce the likelihood that critical data might be lost during the execution of one or both of the automated video processes. For example, the computing system may facilitate this management by executing the first and second automated video processing procedures at the same time (e.g., by parallel processing or other similar processing techniques), and may potentially execute a third, fourth, fifth, or more automated video processing procedures in a similar fashion.

In other examples, the computing system may prioritize one or more of the second automated video procedure's steps over the completion of one or more of the steps in the first automated video procedure. In these examples, in general, the highest priority steps of the first and second automated video processing procedures relate to capture, with lower priority given to the steps relating to processing and/or editing captured video. For example, in some embodiments, the computing system prioritizes execution of a second automated video procedure's steps of (i) initiating one or more live video feeds of the tracked event and (ii) receiving a first live video feed of a first portion of the tracked event, over one or more of the first automated video processing procedure's steps of (i) selecting one or more segments of the received first live video feed for further processing, (ii) modifying the selected one or more segments to improve performance of the automated video processing procedure, and (iii) compiling the modified one or more segments into a first video of the tracked event.

In other embodiments, the methods described herein may include the computing system calibrating its functionality, as well as the functionality of associated hardware and devices, before, during, or after engaging in the steps of other methods described herein by undertaking a variety of testing sequences.

For example, to ensure the consistency and functionality of the live video feeds received and/or the video segments selected therefrom, the computing system may engage in a calibration sequence. Specifically, the computing system may obtain a first calibration video segment from each camera associated with the computing system and then compare the first calibration video segment to a corresponding reference video segment. In a further aspect, the computing system may also determine one or more camera configuration parameters (e.g., brightness, contrast, saturation, color balance, filters, and/or other parameters) to achieve consistency between the video segments and then configure each camera with one or more of the determined parameters. In yet a further aspect, to ensure this consistency, the computing system may repeat this procedure (potentially more than once) over the term of use of the system, and may also display one or more of the camera's corresponding first calibration video segment, a second calibration video segment, the reference video segments, and/or one or more notifications identifying problems (e.g., defective components).

In other embodiments, this calibration sequence may also be performed by referencing one or more stock video segments. Specifically, in response to selecting one or more stock video segments from a library of previously recorded stock video segments, the computing system may obtain a first calibration video segment from each camera associated with the computing system and then compare the first calibration video segment to a corresponding reference stock video segment. In a further aspect, the computing system may also determine one or more camera configuration parameters (e.g., brightness, contrast, saturation, color balance, filters, and/or other parameters) to achieve consistency between the video segments and then configure each camera with one or more of the determined parameters. In yet a further aspect, to ensure this consistency, the computing system may repeat this procedure (potentially more than once) over the term of use of the system, and may also display one or more of the camera's corresponding first calibration video segment, a second calibration video segment, the reference stock video segments, and/or one or more notifications identifying problems (e.g., defective components) for the user's review.

In other embodiments, the computing system may ensure its functionality, as well as the functionality of the associated cameras by comparing, for each separate camera, that camera's field of view against a corresponding reference field of view. In a further aspect, in response to undertaking the comparison, the computing system may adjust one or more individual camera's field of view to ensure consistency between that camera's field of view and its corresponding reference field of view. In yet a further aspect, after adjusting each separate camera's field of view, for each camera, the computing system may display (via the user interface of the computing system or otherwise) one or more camera's adjusted field of view and/or its corresponding reference field of view for the user's review, allowing the user to visually confirm capture timings and other parameters of the system and adjust the camera angles to optimize the resulting quality of the videos produced therefrom.

Finally, while it is true that a number of the aspects, examples, and embodiments of the claimed invention have been disclosed herein, it is also true that other aspects, examples, and embodiments of the claimed invention will be readily apparent to those skilled in the art. Accordingly, the aspects, examples, and embodiments of the claimed invention disclosed herein are provided for the purpose of explanation only, and should not be interpreted to be limiting. Instead, the most accurate and true scope of the claimed invention are captured in the following claims.

",Kyle Quinton Beatch,"Art & Design, Entertainment, Cybersecurity, Technology,Media, Digital Media, Entertainment, Telecommunications,Technology, Media, Retail, Information Storage","Data storage, recording devices, optical and magnetic storage, retrieval systems,Image processing, enhancement, transformation, 3D modeling, virtual reality, pattern recognition,Television transmission, pictorial communication, facsimile systems, image data processing","Systems and methods disclosed herein include a method implemented by a computing system, the method comprising: receiving tracking data associated with a tracked event and executing an automated video processing procedure including: (i) initiating one or more live video feeds of the tracked events; (ii) receiving a first live video feed of a first portion of the tracked event; (iii) selecting one or more segments of the received first live video feed for further processing; (iv) receiving a second live video feed of a second portion of the tracked event; (v) selecting one or more segments of the received second live video feed for further processing; (vi) modifying the selected one or more segments to improve performance of the automated video processing procedure; and (vii) compiling the modified one or more segments into a first video of the tracked event."
12020729,"BACKGROUND

The presentation of information to users by means of video content on electronic devices has become increasingly prevalent. Such video content includes not only video clips, movies, or the like explicitly selected by the user for viewing on the electronic device, but also includes promotional content, such as autoplay video advertisements.

An informational or promotional payload of such video content is often not readily available when the video content is presented, while some video content does not deliver its informational or promotional payload immediately but does so only some way into playback of the content. Users often, however, dismiss the video playback before its promotional payload has been completed or before the video playback has progressed sufficiently to present particular targeted information to the user.

",utility,2024-06-25,Summary information display during video sequence playback,B2,15.0,,Snap Inc.,"G11B 27/36, G06F 3/4845, G06F 3/488, G06Q 30/241, G11B 27/28, G11B 27/34, H04N 5/91, H04N 5/92, G06F 3/483","G11B27/036,G06F3/04845,G06F3/0488,G06Q30/0277,G11B27/28,G11B27/34,H04N5/91,H04N5/92,G06F3/0483","inventional,inventional,inventional,inventional,inventional,inventional,inventional,inventional,additional","Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements-Input arrangements or combined input and output arrangements for interaction between user and computer -Interaction techniques based on graphical user interfaces [GUI]-based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance-Interaction with page-structured environments, e.g. book metaphor, Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements-Input arrangements or combined input and output arrangements for interaction between user and computer -Interaction techniques based on graphical user interfaces [GUI]-for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range-for image manipulation, e.g. dragging, rotation, expansion or change of colour, Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements-Input arrangements or combined input and output arrangements for interaction between user and computer -Interaction techniques based on graphical user interfaces [GUI]-using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser-using a touch-screen or digitiser, e.g. input of commands through traced gestures, Commerce-Marketing; Price estimation or determination; Fundraising-Advertisements-Online advertisement, Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel-Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers-Electronic editing of digitised analogue information signals, e.g. audio or video signals-Insert-editing, Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel-Indexing; Addressing; Timing or synchronising; Measuring tape travel-by using information detectable on the record carrier-by using information signals recorded by the same method as the main recording , Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel-Indexing; Addressing; Timing or synchronising; Measuring tape travel-Indicating arrangements , Details of television systems -Television signal recording-Television signal processing therefor, Details of television systems -Television signal recording-Television signal processing therefor-Transformation of the television signal for recording, e.g. modulation, frequency changing; Inverse transformation for playback","COMPUTING; CALCULATING OR COUNTING, INFORMATION STORAGE, ELECTRIC COMMUNICATION TECHNIQUE","ELECTRIC DIGITAL DATA PROCESSING , INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR, INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER , PICTORIAL COMMUNICATION, e.g. TELEVISION",1.0,40.0,"The headings provided herein are merely for convenience and do not necessarily affect the scope or meaning of the terms used.

DETAILED DESCRIPTION

The description that follows includes systems, methods, techniques, instruction sequences, and computing machine program products that embody illustrative embodiments of the disclosure. In the following description, for the purposes of explanation, numerous specific details are set forth in order to provide an understanding of various embodiments of the inventive subject matter. It will be evident, however, to those skilled in the art, that embodiments of the inventive subject matter may be practiced without these specific details. In general, well-known instruction instances, protocols, structures, and techniques are not necessarily shown in detail.

One aspect of the disclosure provides for a method and system for presenting video content via an electronic device. The method comprises, in response to detecting user input to dismiss the video content, displaying in replacement of the video content non-video summary information or contextual material that pertains semantically to subject matter of the replaced video content.

In some embodiments, the video content comprises a video advertisement. In such cases, the summary information may comprise graphic display of a brand, product, event, and/or offer to which the subject matter of the media content pertains. In some embodiments, the video advertisement and the summary information may comprise a video version and a print version of a particular advertisement. Differently defined, the video content and the corresponding summary information may be a video advertisement and a print advertisement (or other non-video advertisement) forming part of a common advertising campaign. Note that the terms “print advertisement” or “print version” as used herein mean a still image that is suitable for playback in a print medium, and these terms include non-print playback of such an image on an electronic display screen. The non-video advertisement or print advertisement can comprise a composite visual advertisement comprising text information together with photographic, graphics, or other pictorial images.

In some embodiments, the video content comprises entertainment and/or informational material that is non-promotional in nature (i.e. not being a video advertisement). In such embodiments, the corresponding summary information may display title and/or bibliographic information about the video content. In one example embodiment, the video content comprises educational video material or video entertainment material such as a movie or an episode of a television series, with the summary information comprising a title card or title image. Such a title card or title information may comprise, for example, the relevant movie title or series and episode title and/or number, as the case may be.

In some embodiments, the user input to dismiss the video content may comprise a dismissal gesture performed via a user input mechanism provided by the user device. In some embodiments, the dismissal gesture comprises a swiping gesture in which the media content is grabbed (e.g., by a finger press on a touchscreen or a cursor click and hold via mouse input) and dragged or swiped in a specific direction, to dismiss the video content. In such embodiments, the video content may automatically be replaced with a summary image displaying the corresponding summary information on initiation of the swiping or dragging motion.

One instance of such an embodiment can be seen with reference to the examples described with reference toFIGS.1and5A. A user viewing an autoplay video advertisement on a mobile electronic device (seeFIG.1A) may, for example, decide to dismiss the video advertisement prematurely—e.g., before an advertising payload has been communicated to the user—to scroll on to a next item in a series of media items. When the user taps on the touchscreen display of the device and starts dragging the video advertisement sideways in order to dismiss it (FIG.1B), the video advertisement is instantaneously replaced by summary information in the form of a print version of the video advertisement. In this manner, the user is apprised or informed of at least some aspects of the informational payload of the video advertisement, even though viewing of the advertisement has ceased prematurely.

In some embodiments, on-screen behavior of a substitutional summary image that provides the summary information is consistent with a way in which the substituted video content would have behaved responsive to user input, had it not been replaced by the substitutional summary image. For example, in the example embodiment ofFIG.1C, a summary image or title card displayed in place of the video advertisement during a dismissal swipe moves sideways together with the user's finger or cursor as it swipes sideways.

In some embodiments, abortion or cessation of the dismissal gesture before completing dismissal of the video advertisement automatically results in resumption of playback of the video content. Returning again to the example embodiment ofFIGS.1A-1E, it will be seen that the above feature is manifested in these embodiments by providing for the summary image (e.g., the print version of the video advertisement) in turn to be replaced—if the dismissal swipe is not completed—by the video advertisement, which continues playback from its last viewed location (FIG.1E). When the user, for example, starts swiping the video advertisement sideways in order to dismiss it, but then takes note of the summary information of the print version by which the video advertisement has been replaced, the user may in some instances decide not to dismiss the video advertisement. To do so, the user in this example merely releases the on-screen object being swiped (thus ceasing the dismissal gesture), in response to which the swiped title card is bounced back to full-screen configuration and is replaced by the interrupted video content, which resumes playback in full-screen mode.

In some embodiments, the dismissal gesture may comprise a tapping or clicking gesture rather than a swiping motion as discussed above. In some such embodiments, display of the summary information comprises display of a substitutional summary image for the duration of the tap or click input. In some embodiments, e.g., receipt of a tap-and-hold input (or a click-and-hold) automatically results in display of the summary image for at least the duration of the tap-and-hold input.

In some embodiments, display of the summary image is ceased immediately upon release of the tap- or click input. In some such embodiments, release of the tap- or click input causes substantially immediate dismissal of the video content and initiation of presentation of another media item. In other embodiments, summary information may be displayed automatically in response to input other than a dismissal commands. In some embodiments, for example, a dismissal command may comprise a swipe input, with a tap-and-hold or click-and-hold input automatically resulting in display of the substitutional summary information while the tap or click input is held, and release of the tap or click input resulting in resumed playback of the original video content.

In some embodiments, receipt of a tap input or click input automatically results in automated display of the summary image for a predetermined set interval. One example of such an embodiment is described below with reference toFIGS.2A-2EandFIG.5B. In some such embodiments, the summary image automatically fades out or transitions automatically after expiry of the set interval (e.g.,FIG.2C). In some such auto-transition embodiments, the user can select to resume playback of the video content instead of dismissing it by providing another tap or click input during the set interval while the summary image is displayed on-screen instead of the video content (e.g.,FIG.2D). The set interval may be 0.5-2 seconds. In some embodiments the interval may be approximately one second.

Automated display of summary information in the form of, for example, a synoptic image, title card, or an abbreviated advertisement upon the user-initiated dismissal of video content on the display of an electronic device, as described, may provide various benefits when employed with many different types of electronic devices, including both mobile and non-mobile devices. The method is particularly beneficial when used in conjunction with full screen displays, and/or in instances where available screen space is limited. Mobile phones and other mobile electronic devices, for example, often have relatively small screens on which the provision of a text overlay over video content during or before dismissal may be insufficiently efficacious in drawing user attention and/or in conveying information to the user. These difficulties are ameliorated by the described methods and systems, in that the replacement summary image or replacement abbreviated advertisement can be designed to have sufficient visual impact to facilitate virtually instantaneous user recognition and consumption.

Another advantage of the described methods and systems is that they allow for increased presentation of information to the user with minimal increase in user irritation or increased user input. If, for example, the user does not wish to continue viewing video content, the dismissal gesture or input is simply completed, in which case the information of the summary image is conveyed to the user without requiring from the user any input additional to that which would have been provided without display of the summary image during dismissal of the video content.

Yet a further benefit of use of these methods and/or systems in conjunction with video advertisement is that it allows for effective delivery of the information payload of an advertisement even if the user does not want to watch video advertisement for long enough to receive this information from the video advertisement. Yet further, user attention is typically fixed on the device display during the provision of input to dismiss the relevant video content, thus providing for increased likelihood of effectively conveying the relevant information to the user.

More detailed example embodiments will now be described briefly with respect to the attached drawings. InFIGS.1A-1E and2A-2Eof the drawings, reference numeral100generally indicates a mobile electronic device in the form of a mobile smartphone that is specially configured (e.g., by having one or more permanently configured video playback controllers or by having one or more temporarily configured video playback controllers provided by software executed by a dynamically reconfigurable computer processor) to provide a transitional display of summary information about the video content during dismissal of the video content by a user.FIG.5AandFIG.5Bshow respective flow charts500and501of methods corresponding to two different example embodiments. The example method ofFIG.5Awill first be described with reference toFIGS.1A-1E, after which the example method ofFIG.5Bwill be described with reference toFIGS.2A-2E.510

At operation510in the flowchart500ofFIG.5A, video content in the example form of a video advertisement114is played on a screen107of the mobile phone100in full-screen format (seeFIG.1A). In this example embodiment, the video advertisement114is a movie trailer. In other embodiments, the video advertisement114can be directed to promoting a particular product, service, offer, opportunity, or the like.

If a user viewing the video advertisement114wishes to stop the playing of the video and dismiss it, the user can in this example embodiment provide a dismissal gesture indicating that the video advertisement114is to be dismissed. In this example embodiment, the screen107is a touchscreen receptive to haptic input directly on the screen107on which the video is displayed. In other embodiments, such as in instances where the video advertisement114is displayed on screen that does not have touchscreen functionality, the dismissal input may be provided in a manner different from the touch input described below with reference to the example embodiment ofFIG.1A.

In this example embodiment, the dismissal gesture provided by touch input on the screen107comprises dragging or swiping the video advertisement114to a side edge of the screen107. Here, the video advertisement114is dismissed by swiping towards a right side edge124of the screen107. It will be appreciated that such dismissal of the video advertisement114may in many instances occur shortly after commencement of the video playback, before the promotional content or payload of the video advertisement114has been conveyed to the user. Thus, for example, the particular video advertisement114may be one and a half minutes in total length and may provide information about the name of the particular movie, the names of the lead actors and/or director, and the release date of the movie only 30 seconds into playback of the video advertisement114. This information provides, in this example, the promotional or informational payload of the video advertisement114. If the video advertisement114is skipped before the video playback has progressed for 30 seconds, the payload of the advertisement is not delivered, and the user is not exposed to the information targeted for delivery by the video advertisement114.

When, at operation520(FIG.5A), touchscreen input that dismisses the video advertisement114(or, in this example embodiment, starts dismissal of the video advertisement) is received, non-video summary information about the video advertisement114is displayed on the screen107in full-screen format instead of the video advertisement114(operation525,FIG.5A). In this example embodiment, commencement of the dismissal gesture is detected when a rightward swiping motion via the user's finger121in contact with the screen107is started (as indicated schematically by arrow132inFIG.1B). As can be seen inFIG.1B, the summary information or summary image in this example embodiment comprises a non-video advertisement128about the same movie promoted by video advertisement114which it replaces. The non-video advertisement128consists of text information and pictures/graphics that provide an informational payload of the interrupted video advertisement114. In this example, the informational or promotional payload comprises a movie title, the name of the headline actor, and the relevant opening date. A photographic image and/or tagline can also be included in this example summary information.

At operation545(FIG.5A), the dismissal swipe is continued, with the user's finger121moving closer to the lateral right side edge124of the screen107(seeFIG.1C). During this swiping motion, full-screen display of the summary information may be replaced by display of the summary information in a user interface element that is movable on-screen responsive to continued user input. In the example embodiment ofFIG.1C, the movable user interface element is a title card within which the non-video advertisement128is displayed and which is moved sideways across the screen107together with movement of the user's finger121. During swiping of the non-video advertisement128, the full-screen display is thus in this example embodiment replaced by a split screen display having a vertical split line131that separates the screen between the dragged title card of the non-video advertisement128and a next media item135to be displaced after dismissal of the video advertisement114.

If the user drags the non-video advertisement128all the way to the side edge124of the screen, the dismissal gesture is completed. The right side edge124is thus in this example a terminal position for the drag input. In this instance, this results in automatic playing of the next media item135.

At operation555, a video playback controller forming part of the mobile phone100automatically detects whether or not the dismissal swipe is completed. If so, the successive or next media item135in a series of media items is displayed on the screen107in full-screen mode (operation570inFIG.5A;FIG.1D). If the dismissal gesture is not completed, playback of the video advertisement114is recommenced at its last playback position (operation580;FIG.1E).

If the user thus, for example, starts to dismiss an autoplay video advertisement114, but then notices the promotional summary information provided by the non-video advertisement128and decides to view the video advertisement114further, the user need only stop the swiping gesture short of its completion, in response to which the video advertisement jumps back to fullscreen mode and seamlessly resumes playback (FIG.1E).

FIG.5Bshows a flowchart that schematically illustrates another example method501that is analogous to the above-described method500ofFIG.5A, with one distinction being that user input to dismiss video content (again shown inFIG.2Ain the example embodiment of a video advertisement114) can be provided in a form different from the swiping gesture described with reference toFIG.5AandFIGS.1A-1E. In the following description with reference toFIG.5BandFIGS.2A-2Euser input to dismiss currently played video content is in the form of a touchscreen tap instead of a dragging or swiping touchscreen gesture.

In response to tapping on the full-screen playback of the video advertisement114(operation521inFIG.5B; corresponding toFIG.2A), the non-video advertisement128in the form of a title card for the video advertisement114is automatically displayed on the screen107in full-screen mode in replacement of the video advertisement114(operation526;FIG.2B). Note that although the presently described example embodiments are with respect to display via a video playback application executing on a touchscreen-enabled mobile phone100, the disclosed methods may in other example embodiments be performed using different platforms and user interfaces. The described functionalities may, for example, be provided on a desktop personal computer in which video content is presented to a user by being played in, for example, a browser interface on a display screen that is not touch-enabled. In such case, user input can be provided using a peripheral such as a mouse or a keyboard, with the dismissal command (and the subsequent optional resumption command) being a mouseclick on the played video, a mouseclick on an on-screen NEXT or SKIP soft button, a spacebar press, or the like.

Display of the summary information in the form of the non-video advertisement128corresponding to the video advertisement114that is to be dismissed comprises, in this example embodiment, displaying the non-video advertisement128for at least a predetermined set interval, in this example being one second. If no further input is received during the set interval while the summary information is displayed (as determined in operation556ofFIG.5B), then the next media item135is automatically displayed in full-screen mode (operation571;FIG.2C).

In this example embodiment, display of the non-video advertisement128(at operation526) immediately follows reception of the dismissal command (at operation521). In other embodiments, however, one or more additional contextual images can be displayed before or after display of the non-video advertisement128(or, in other embodiments, display of corresponding informational material related to the replace video content). In one example, an isolated brand image (such as a brand logo and/or brand name) may be displayed as a transitional image between the video content (e.g., the video advertisement114) and the associated informational material (e.g., the non-video advertisement128). In another embodiment, such a brand or product page can instead or in addition be displayed after display of the non-video advertisement128. Note that, in some embodiments, the summary information or informational material associated with a video may consist in totality of a brand or product page, as described. For example, if a video advertisement for, say, Walmart™ is dismissed by the user, an image with the company's logo on a solid blue background may be displayed for a brief period (e.g., a second or two), before the display transition to the next media item135.

It will be seen that the summary information or informational payload provided by the non-video advertisement128is thus displayed as an ephemeral transitional image subsequent to premature user-dismissal of the video advertisement114. When the user watches the video advertisement114to completion, however, no transitional image such as the non-video advertisement128is provided between the video advertisement114and the next media item135.

If, in the example embodiment ofFIG.5B, the user wishes to resume viewing of the movie trailer114upon taking note of the summary information provided by the non-video advertisement128, a further tap or click input is to be provided before expiry of the set interval during which the summary information128is displayed. The user may for example dismiss the autoplay movie trailer video advertisement114almost immediately or at least before the details of the promoted movie is given by playback of the video advertisement114. When, however, the corresponding non-video advertisement128is thereafter displayed, the user may identify the promoted movie to be of interest, and may wish to view the video advertisement114in full. A resumption command indicating that playback of the video advertisement114is to be resumed is in this example embodiment provided by a touchscreen tap or a cursor click on the non-video advertisement128that is displayed in replacement of the video advertisement114.

The method501thus includes, at operation556, determining whether or not a further tap/click input is received during the set interval. If so (FIG.2D), playback of the video advertisement114is resumed from the last viewed location (operation581;FIG.2E). If not, the image fades or transitions to the next media item135(operation571;FIG.2C).

In some embodiments, a threshold location or position in the video content can be predefined by an operator for a particular video advertisement or other video item to serve as a worship for automatically determining whether or not the non-video summary information is to be displayed. In such cases, dismissal of the video advertisement114before the threshold moment results in automatic display of the transitional summary image128, while dismissal of the video advertisement114after the watershed moment results in transition to the next media item135without display of the summary information.

One such example embodiment is illustrated inFIG.5C, which shows a flowchart of one example method503. The method503corresponds largely to the method502ofFIG.5B. In the method503ofFIG.5C, however, receipt of a dismissal command (at operation521) immediately triggers automatic determination, at operation522, whether or not the dismissal command was received before or after the predefined threshold moment particular to the video advertisement114that is to be dismissed. If the dismissal command (in this example a touchscreen tap) was indeed received before the threshold moment, then the method proceeds similarly to that described inFIG.5B, comprising display of the substitutional non-video advertisement128for a set interval during which playback of the video may be resumed (at operation581) responsive to a resumption command (at operation556) in the example form of a further touchscreen tap.

If, however, it is determined at operation522that the dismissal command was received after the threshold moment, the video advertisement114is dismissed directly by fading or transitioning to the next media item135(operation571), without displaying any intermediate summary information.

Some embodiments provide for display of one or more additional images or screens during automated transitions as described. For example, a solid color fullscreen and/or a relevant brand image may be displayed during transitions. One example provides, for example, for automated transition responsive to a dismissal input of a video advertisement first to a solid color screen associated with a particular brand, and then to the relevant summary information. In some embodiments, the solid color screen may include prominent isolated display of a relevant product brand.

Example System(s)

FIG.4, shows an example embodiment of high-level client-server-based network architecture200that provides for advanced interactive video content presentation functionalities as disclosed herein. A networked system202, in the example form of a social media platform system, provides server-side functionality via a network204(e.g., the Internet or wide area network (WAN)) to multiple client devices210. For clarity of illustration, a single mobile client device210is shown inFIG.4, but many similar or analogous client devices210are typically connected to the system202at any given time. It will be appreciated that non-mobile client devices210may subscribe to services provided by the system202, and that dynamic media format presentation may, in some instances, be employed with respect to content delivery to such non-mobile client devices210(e.g., desktop computers). The system202is, in this example, configured to provide a social media service that includes media content-rich functionalities, such as video messaging and/or online video sharing.

The client device210can execute software for providing various functionalities associated with social media services and media content consumption.FIG.4illustrates, for example, a web client212(e.g., a browser, such as the Internet Explorer® browser developed by Microsoft® Corporation of Redmond, Washington State), and an on-device client application214executing on client device210.

Different types of client devices210on which social media functionalities are available via the system202may comprise, but are not limited to, mobile phones, desktop computers, laptops, portable digital assistants (PDAs), smart phones, tablets, ultra-books, netbooks, multi-processor systems, microprocessor-based or programmable consumer electronics, game consoles, set-top boxes, or any other communication device that a user may utilize to access the networked system202. In some embodiments, the client device210may comprise a display module (not shown) to display information (e.g., in the form of user interfaces). In further embodiments, the client device210may comprise one or more of a touch screen107, accelerometers, gyroscopes, cameras, microphones, global positioning system (GPS) devices, and so forth. The client device210may be a device of a user that is used to perform a transaction involving digital items within the networked system202. In one embodiment, the networked system202is configured to provide a video content presentation service that responds to requests for media content from remote mobile client devices210.

The users206associated with respective client devices210may be people, machines, or other means of interacting with client devices210. In some embodiments, the user206is not part of the network architecture200, but may interact with the network architecture200via client device210or another means. For example, one or more portions of network204may be an ad hoc network, an intranet, an extranet, a virtual private network (VPN), a local area network (LAN), a wireless LAN (WLAN), a wide area network (WAN), a wireless WAN (WWAN), a metropolitan area network (MAN), a portion of the Internet, a portion of the public switched telephone network (PSTN), a cellular telephone network, a wireless network, a WiFi network, a WiMax network, another type of network, or a combination of two or more such networks.

Each of the client devices210may include one or more applications (also referred to as “apps) such as, but not limited to, a web browser, messaging application, electronic mail (email) application, and the like. The client applications214can, in the example embodiment ofFIG.1A-1E, include social media apps (e.g., a Snapchat™, Facebook™, or the like) video content viewing applications (e.g., Youtube™, Netflix™, or the like) that can execute on the client device210and cooperate with the system202to submit media content requests and to provide for enhanced interactive video content presentation as disclosed herein. In some embodiments, a client application214on the client device210performs the described automated operations for enhanced video content presentation as described. In other embodiments, such video content presentation enhancement are performed by a content delivery server, such as application server240inFIG.4.

In one example embodiment, for example, a database226forming part of the system202has stored thereon multiple video advertisements together with respectively linked summary information files (e.g., respective non-video advertisements). In the previously described example embodiments, the video advertisement114and the associated non-video advertisement128provides one example of such a pair of linked video and non-video files.

When a user selects a particular video file to watch on the client device210, the video content presentation system244automatically selects a particular autoplay video advertisement to play on a display screen of the client device210before playback of the selected video file commences. The selected video advertisement is then downloaded to the client device210together in association with a file for the linked non-video material. Thereafter, one of the methods ofFIGS.5A-5Cmay, for example, be performed using the downloaded video file and linked non-video file. Before the video clip selected by the user is played back, the selected video advertisement is automatically presented as a skippable video ad preceding the user-selected video clip. If the user206skips the video advertisement before completion (or, in some embodiments, before playback to a predefined threshold point), a transitional image in the form of the linked non-video advertisement is displayed before the selected video clip (corresponding in this instance to the next media item135in the embodiment ofFIGS.1A-1E) is played back on the client device206.

In some embodiments, if the social media application or video playback application is executed on a given one of the client devices210, then this application is configured to locally provide the user interface and at least some of the functionalities with the application configured to communicate with the networked system202, on an as needed basis, for data and/or processing capabilities not locally available (e.g., access to a social media platform to upload and/or download media content, etc.). Conversely, if the social media application is not included in the client device210, the client device210may use its web browser to access the relevant social media site (or a variant thereof) hosted on the networked system202.

An application program interface (API) server220and a web server222are coupled to, and provide programmatic and web interfaces respectively to, one or more application servers240. The application servers240may host one or more systems for providing various functionalities, for example including a social media platform management system(s)242and a video content presentation system244, each of which may comprise one or more modules or applications and each of which may be embodied as permanently configured hardware, hardware, executing software to dynamically configure one or more processor devices to perform various automated operations, firmware, or any combination thereof. The application servers240are, in turn, shown to be coupled to one or more database servers224that facilitate access to one or more information storage repositories or database(s)226. In an example embodiment, the databases226are storage devices that store information to be posted on the social media platform, message data, and/or media content (e.g., digital photos, videos, and audio files). The databases226may also store digital item information in accordance with example embodiments.

Further, while the client-server-based network architecture200shown inFIG.4employs a client-server architecture, the present disclosure is not limited to such an architecture, and could equally well find application in a distributed, or peer-to-peer, architecture system, for example. The various platform management system(s)242and video content presentation system(s)244could also be implemented as standalone software programs, which do not necessarily have networking capabilities.

The web client212may access the various platform management and video content presentation systems242and244via the web interface supported by the web server222. At least some of the client application(s)214may comprise a programmatic client to cooperate with the system202to facilitate enhanced video content presentation. Additionally, a third party application232, executing on a third party server(s)230, is shown as having programmatic access to the networked system202via the programmatic interface provided by the API server220. For example, the third party application232, utilizing information retrieved from the networked system202, may support one or more features or functions on a website hosted by the third party.

FIG.3is a schematic block diagram of a video content presentation system244, in accordance with an example embodiment. The system244comprises a number of different hardware-implemented modules, units, or other means configured for automated performance of associated operations, as described in greater detail elsewhere herein. The various components may, in some embodiments, be provided by permanently configured circuit arrangements, and may, in other embodiments, be provided by software executing on one or more dynamically reconfigurable processor devices, thereby to configure the processor devices sequentially, or in parallel, to provide the respective hardware-implemented modules. In some embodiments, the system244may be provided by server-side components, such as in the example embodiment ofFIG.4. In other embodiments, at least part of the system244may be provided by the mobile client device210executing custom software. In some embodiments, the system244may be provided entirely by an electronic device such as a personal computer or a mobile electronic device. In embodiments such as that described with reference toFIGS.1A-2E, the system244is provided by the mobile electronic device such as the phone100.

The system244includes a request module310configured to receive a request for delivery of media content to a mobile client device210. In cases where the request module310is a server-side component, the request module310may be configured to receive an electronic communication originating from the relevant mobile client device210that indicates the particular media content which is to be delivered. In other embodiments, where request module310forms part of the mobile client device210, the request module310may be configured to receive and interpret user input on the client device210, and to communicate an electronic request message to the relevant video content presentation server244.

An input interpreter320is provided to receive and interpret user input provide with respect to displayed video content, e.g., to identify dismissal commands and/or resumption commands during video playback, as described earlier. The system244further includes a semantic mapper330configured to map in device memory multiple transitional images or material with respective units of video content. In some example embodiments, the transitional material comprises a non-video advertisements to be displayed as a transitional image after dismissal of particular associated video advertisements. In this manner, each of multiple video advertisements114may be linked to a respective non-video advertisements128. In other embodiments, the transitional material may instead, or in addition, include a solid color screen/image to be displayed in association with a respective video advertisement, a brand name and/or logo image to be displayed in association with a respective video advertisement, or the like.

The example system244further includes a video playback controller340configured to dynamically interact with the other described system components to provide the enhanced video playback functionalities described with reference toFIGS.1A-2E and5A-5C.

Modules, Components, and Logic

Certain embodiments are described herein as including logic or a number of components, modules, or mechanisms. Examples of such components include the request module310, the input interpreter320, the semantic mapper330, and a video playback controller340described with reference toFIG.3. Such components or modules may constitute either software modules (e.g., code embodied on a machine-readable medium) or hardware modules/components. A “hardware module/component is a tangible unit capable of performing certain operations and may be configured or arranged in a certain physical manner. In various example embodiments, one or more computer systems (e.g., a standalone computer system, a client computer system, or a server computer system) or one or more hardware modules of a computer system (e.g., a processor or a group of processors) may be configured by software (e.g., an application or application portion) as a hardware module that operates to perform certain operations as described herein. In such cases, the various described hardware modules of a system or machine to perform the disclosed operations may not at any time have all of the modules described as forming part of the system or machine. Instead, a reconfigurable computer processor (e.g., a CPU) may, at various times, be configured by execution of specific software to form different corresponding modules.

In some embodiments, a hardware module may be implemented mechanically, electronically, or any suitable combination thereof. For example, a hardware module may include dedicated circuitry or logic that is permanently configured to perform certain operations. For example, a hardware module may be a special-purpose processor, such as a field-programmable gate array (FPGA) or an application specific integrated circuit (ASIC). A hardware module may also include programmable logic or reconfigurable circuitry that is temporarily configured by software to perform certain operations. For example, a hardware module may include software executed by a computer processor or other programmable processor. Once configured by such software, hardware modules become specific machines (or specific components of a machine) uniquely tailored to perform the configured functions and are no longer general-purpose processors. It will be appreciated that the decision to implement a hardware module mechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.

Accordingly, the phrase “hardware module or component, or reference to a processor configured to perform specified operations should be understood to encompass a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to operate in a certain manner or to perform certain operations described herein. As used herein, “hardware-implemented module” refers to a hardware module. As mentioned earlier in respect to embodiments in which hardware modules are temporarily configured (e.g., programmed), each of the hardware modules need not be configured or instantiated at any one instance in time. For example, where a hardware module comprises a general-purpose processor configured by software to become a special-purpose processor, the general-purpose processor may be configured as respectively different special-purpose processors (e.g., comprising different hardware modules) at different times. Software accordingly configures a particular processor or processors, for example, to constitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.

Hardware modules can provide information to, and receive information from, other hardware modules. Accordingly, the described hardware modules may be regarded as being communicatively coupled. Where multiple hardware modules exist contemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses) between or among two or more of the hardware modules. In embodiments in which multiple hardware modules are configured or instantiated at different times, communications between such hardware modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware modules have access. For example, one hardware module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further hardware module may then, at a later time, access the memory device to retrieve and process the stored output. Hardware modules may also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of information).

The various operations of example methods described herein may be performed, at least partially, by one or more processors that are temporarily configured (e.g., by software) or permanently configured to perform the relevant operations. Whether temporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions described herein. As used herein, “processor-implemented module” refers to a hardware module implemented using one or more processors.

Similarly, the methods described herein may be at least partially processor-implemented, with a particular processor or processors being an example of hardware. For example, at least some of the operations of a method may be performed by one or more processors or processor-implemented modules. Moreover, the one or more processors may also operate to support performance of the relevant operations in a “cloud computing” environment or as a “software as a service” (SaaS). For example, at least some of the operations may be performed by a group of computers (as examples of machines including processors), with these operations being accessible via a network204(e.g., the Internet) and via one or more appropriate interfaces (e.g., an application program interface (API)).

The performance of certain of the operations may be distributed among the processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the processors or processor-implemented modules may be located in a single geographic location (e.g., within a home environment, an office environment, or a server farm). In other example embodiments, the processors or processor-implemented modules may be distributed across a number of geographic locations.

Machine and Software Architecture

The modules, methods, applications and so forth described in conjunction withFIGS.1A-5Care implemented in some embodiments in the context of a machine and an associated software architecture. The sections below describe representative software architecture(s) and machine (e.g., hardware) architecture that are suitable for use with the disclosed embodiments.

Software architectures are used in conjunction with hardware architectures to create devices and machines tailored to particular purposes. For example, a particular hardware architecture coupled with a particular software architecture will create a mobile device, such as a mobile phone, tablet device, or so forth. A slightly different hardware and software architecture may yield a smart device for use in the “internet of things”, while yet another combination produces a server computer for use within a cloud computing architecture. Not all combinations of such software and hardware architectures are presented here as those of skill in the art can readily understand how to implement various embodiments consistent with this disclosure in different contexts from the disclosure contained herein.

Software Architecture

FIG.6is a block diagram600illustrating a representative software architecture602, which may be used in conjunction with various hardware architectures herein described.FIG.6is merely a non-limiting example of a software architecture and it will be appreciated that many other architectures may be implemented to facilitate the functionality described herein. The software architecture602may be executing on hardware such as machine700ofFIG.7that includes, among other things, processors710, memory730, and I/O components750. A representative hardware layer604is illustrated and can represent, for example, the machine700ofFIG.7. The representative hardware layer604comprises one or more processing units606having associated executable instructions608. Executable instructions608represent the executable instructions of the software architecture602, including implementation of the methods, modules and so forth ofFIGS.1A-5C. Hardware layer604also includes memory and/or storage modules610, which also have executable instructions608. Hardware layer604may also comprise other hardware as indicated by612which represents any other hardware of the hardware layer604, such as the other hardware illustrated as part of machine700.

In the example architecture ofFIG.6, the software602may be conceptualized as a stack of layers where each layer provides particular functionality. For example, the software602may include layers such as an operating system614, libraries616, frameworks/middleware618, applications660and presentation layer644. Operationally, the applications660and/or other components within the layers may invoke application programming interface (API) calls624through the software stack and receive a response, returned values, and so forth illustrated as messages626in response to the API calls624. The layers illustrated are representative in nature and not all software architectures have all layers. For example, some mobile or special purpose operating systems614may not provide a frameworks/middleware layer618, while others may provide such a layer. Other software architectures may include additional or different layers.

The operating system614may manage hardware resources and provide common services. The operating system614may include, for example, a kernel628, services630, and drivers632. The kernel628may act as an abstraction layer between the hardware and the other software layers. For example, the kernel628may be responsible for memory management, processor management (e.g., scheduling), component management, networking, security settings, and so on. The services630may provide other common services for the other software layers. The drivers632may be responsible for controlling or interfacing with the underlying hardware. For instance, the drivers632may include display drivers, camera drivers, Bluetooth® drivers, flash memory drivers, serial communication drivers (e.g., Universal Serial Bus (USB) drivers), Wi-Fi® drivers, audio drivers, power management drivers, and so forth depending on the hardware configuration.

The libraries616may provide a common infrastructure that may be utilized by the applications660and/or other components and/or layers. The libraries616typically provide functionality that allows other software modules to perform tasks in an easier fashion than to interface directly with the underlying operating system614functionality (e.g., kernel628, services630and/or drivers632). The libraries616may include system634libraries (e.g., C standard library) that may provide functions such as memory allocation functions, string manipulation functions, mathematic functions, and the like. In addition, the libraries616may include API libraries636such as media libraries (e.g., libraries to support presentation and manipulation of various media format such as MPEG4, H.264, MP3, AAC, AMR, JPG, PNG), graphics libraries (e.g., an OpenGL framework that may be used to render 2D and 3D in a graphic content on a display), database libraries (e.g., SQLite that may provide various relational database functions), web libraries (e.g., WebKit that may provide web browsing functionality), and the like. The libraries616may also include a wide variety of other libraries638to provide many other APIs to the applications660and other software components/modules.

The frameworks618(also sometimes referred to as middleware) may provide a higher-level common infrastructure that may be utilized by the applications660and/or other software components/modules. For example, the frameworks618may provide various graphic user interface (GUI) functions, high-level resource management, high-level location services, and so forth. The frameworks618may provide a broad spectrum of other APIs that may be utilized by the applications660and/or other software components/modules, some of which may be specific to a particular operating system614or platform.

The applications660includes built-in applications640and/or third party applications642. Examples of representative built-in applications640may include, but are not limited to, a contacts application, a browser application, a book reader application, a location application, a media application, a messaging application, and/or a game application. Third party applications642may include any of the built in applications640as well as a broad assortment of other applications. In a specific example, the third party application642(e.g., an application developed using the Android™ or iOS™ software development kit (SDK) by an entity other than the vendor of the particular platform) may be mobile software running on a mobile operating system such as iOS™, Android™, Windows® Phone, or other mobile operating systems614. In this example, the third party application642may invoke the API calls624provided by the mobile operating system such as operating system614to facilitate functionality described herein.

The applications660may utilize built in operating system functions (e.g., kernel628, services630and/or drivers632), libraries616(e.g., system634, APIs636, and other libraries638), and frameworks/middleware618to create user interfaces to interact with users206of the system202. Alternatively, or additionally, in some systems, interactions with a user206may occur through a presentation layer, such as presentation layer644. In these systems, the application/module “logic” can be separated from the aspects of the application/module that interact with a user206.

Some software architectures utilize virtual machines. In the example ofFIG.6, this is illustrated by virtual machine648. A virtual machine648creates a software environment where applications/modules can execute as if they were executing on a hardware machine (such as the machine700ofFIG.7, for example). A virtual machine648is hosted by a host operating system (operating system614inFIG.6) and typically, although not always, has a virtual machine monitor646, which manages the operation of the virtual machine648as well as the interface with the host operating system (i.e., operating system614). A software architecture executes within the virtual machine648such as an operating system650, libraries652, frameworks/middleware654, applications656and/or presentation layer658. These layers of software architecture executing within the virtual machine648can be the same as corresponding layers previously described or may be different.

Example Machine Architecture and Machine-Readable Medium

FIG.7is a block diagram illustrating components of a machine700, according to some example embodiments, able to read instructions608from a machine-readable medium (e.g., a machine-readable storage medium) and perform any one or more of the methodologies discussed herein. Specifically,FIG.7shows a diagrammatic representation of the machine700in the example form of a computer system, within which instructions716(e.g., software, a program, an application, an applet, an app, or other executable code) for causing the machine700to perform any one or more of the methodologies discussed herein may be executed. For example the instructions716may cause the machine700to execute the flow diagrams ofFIGS.4and5. Additionally, or alternatively, the instructions716may implement the respective modules ofFIG.3and so forth. The instructions716transform the general, non-programmed machine700into a particular machine700programmed to carry out the described and illustrated functions in the manner described. In alternative embodiments, the machine700operates as a standalone device or may be coupled (e.g., networked) to other machines. In a networked deployment, the machine700may operate in the capacity of a server machine or a client machine in a server-client network environment, or as a peer machine in a peer-to-peer (or distributed) network environment. The machine700may comprise, but not be limited to, a server computer, a client computer, a personal computer (PC), a tablet computer, a laptop computer, a netbook, a set-top box (STB), a personal digital assistant (PDA), an entertainment media system, a cellular telephone, a smart phone, a mobile device, a wearable device (e.g., a smart watch), a smart home device (e.g., a smart appliance), other smart devices, a web appliance, a network router, a network switch, a network bridge, or any machine capable of executing the instructions716, sequentially or otherwise, that specify actions to be taken by machine700. Further, while only a single machine700is illustrated, the term “machine” shall also be taken to include a collection of machines700that individually or jointly execute the instructions716to perform any one or more of the methodologies discussed herein.

The machine700may include processors710, memory730, and I/O components750, which may be configured to communicate with each other such as via a bus702. In an example embodiment, the processors710(e.g., a central processing unit (CPU), a reduced instruction set computing (RISC) processor, a complex instruction set computing (CISC) processor, a graphics processing unit (GPU), a digital signal processor (DSP), an application specific integrated circuit (ASIC), a radio-frequency integrated circuit (RFIC), another processor, or any suitable combination thereof) may include, for example, processor712and processor714that may execute instructions716. The term “processor” is intended to include a multi-core processor710that may comprise two or more independent processors712,714(sometimes referred to as “cores”) that may execute instructions716contemporaneously. AlthoughFIG.7shows multiple processors712,714, the machine700may include a single processor710with a single core, a single processor710with multiple cores (e.g., a multi-core process), multiple processors710with a single core, multiple processors710with multiples cores, or any combination thereof.

The memory/storage730may include a memory732, such as a main memory, or other memory storage, and a storage unit736, both accessible to the processors710such as via the bus702. The storage unit736and memory732store the instructions716, embodying any one or more of the methodologies or functions described herein. The instructions716may also reside, completely or partially, within the memory732, within the storage unit736, within at least one of the processors710(e.g., within the processor710's cache memory), or any suitable combination thereof, during execution thereof by the machine700. Accordingly, the memory732, the storage unit736, and the memory of processors710are examples of machine-readable media.

As used herein, “machine-readable medium” means a device able to store instructions716and data temporarily or permanently and may include, but is not be limited to, random-access memory (RAM), read-only memory (ROM), buffer memory, flash memory, optical media, magnetic media, cache memory, other types of storage (e.g., erasable programmable read-only memory (EEPROM)) and/or any suitable combination thereof. The term “machine-readable medium” should be taken to include a single medium or multiple media (e.g., a centralized or distributed database226, or associated caches and servers) able to store instructions716. The term “machine-readable medium” shall also be taken to include any medium, or combination of multiple media, that is capable of storing instructions (e.g., instructions716) for execution by a machine (e.g., machine700), such that the instructions716, when executed by one or more processors of the machine700(e.g., processors710), cause the machine700to perform any one or more of the methodologies described herein. Accordingly, a “machine-readable medium” refers to a single storage apparatus or device, as well as “cloud-based” storage systems or storage networks that include multiple storage apparatus or devices. The term “machine-readable medium” excludes signals per se.

The I/O components750may include a wide variety of components to receive input, provide output, produce output, transmit information, exchange information, capture measurements, and so on. The specific I/O components750that are included in a particular machine700will depend on the type of machine700. For example, portable machines such as mobile phones will likely include a touch input device or other such input mechanisms, while a headless server machine will likely not include such a touch input device. It will be appreciated that the I/O components750may include many other components that are not shown inFIG.7. The I/O components750are grouped according to functionality merely for simplifying the following discussion and the grouping is in no way limiting. In various example embodiments, the I/O components750may include output components752and input components754. The output components752may include visual components (e.g., a display such as a plasma display panel (PDP), a light emitting diode (LED) display, a liquid crystal display (LCD), a projector, or a cathode ray tube (CRT)), acoustic components (e.g., speakers), haptic components (e.g., a vibratory motor, resistance mechanisms), other signal generators, and so forth. The input components754may include alphanumeric input components (e.g., a keyboard, a touch screen configured to receive alphanumeric input, a photo-optical keyboard, or other alphanumeric input components), point based input components (e.g., a mouse, a touchpad, a trackball, a joystick, a motion sensor, or other pointing instrument), tactile input components (e.g., a physical button, a touch screen that provides location and/or force of touches or touch gestures, or other tactile input components), audio input components (e.g., a microphone), and the like.

In further example embodiments, the I/O components750may include biometric components756, motion components758, environmental components760, or position components762among a wide array of other components. For example, the biometric components756may include components to detect expressions (e.g., hand expressions, facial expressions, vocal expressions, body gestures, or eye tracking), measure biosignals (e.g., blood pressure, heart rate, body temperature, perspiration, or brain waves), identify a person (e.g., voice identification, retinal identification, facial identification, fingerprint identification, or electroencephalogram based identification), and the like. The motion components758may include acceleration sensor components (e.g., accelerometer), gravitation sensor components, rotation sensor components (e.g., gyroscope), and so forth. The environmental components760may include, for example, illumination sensor components (e.g., photometer), temperature sensor components (e.g., one or more thermometer that detect ambient temperature), humidity sensor components, pressure sensor components (e.g., barometer), acoustic sensor components (e.g., one or more microphones that detect background noise), proximity sensor components (e.g., infrared sensors that detect nearby objects), gas sensors (e.g., gas detection sensors to detection concentrations of hazardous gases for safety or to measure pollutants in the atmosphere), or other components that may provide indications, measurements, or signals corresponding to a surrounding physical environment. The position components762may include location sensor components (e.g., a Global Position System (GPS) receiver component), altitude sensor components (e.g., altimeters or barometers that detect air pressure from which altitude may be derived), orientation sensor components (e.g., magnetometers), and the like.

Communication may be implemented using a wide variety of technologies. The I/O components750may include communication components764operable to couple the machine700to a network780or devices770via coupling782and coupling772respectively. For example, the communication components764may include a network interface component or other suitable device to interface with the network780. In further examples, communication components764may include wired communication components, wireless communication components, cellular communication components, near field communication (NFC) components, Bluetooth® components (e.g., Bluetooth® Low Energy), Wi-Fi® components, and other communication components to provide communication via other modalities. The devices770may be another machine or any of a wide variety of peripheral devices (e.g., a peripheral device coupled via a Universal Serial Bus (USB)).

Moreover, the communication components764may detect identifiers or include components operable to detect identifiers. For example, the communication components764may include radio frequency identification (RFID) tag reader components, NFC smart tag detection components, optical reader components (e.g., an optical sensor to detect one-dimensional bar codes such as Universal Product Code (UPC) bar code, multi-dimensional bar codes such as Quick Response (QR) code, Aztec code, Data Matrix, Dataglyph, MaxiCode, PDF417, Ultra Code, UCC RSS-2D bar code, and other optical codes), or acoustic detection components (e.g., microphones to identify tagged audio signals). In addition, a variety of information may be derived via the communication components764, such as, location via Internet Protocol (IP) geo-location, location via Wi-Fi® signal triangulation, location via detecting a NFC beacon signal that may indicate a particular location, and so forth.

Transmission Medium

In various example embodiments, one or more portions of the network780may be an ad hoc network, an intranet, an extranet, a virtual private network (VPN), a local area network (LAN), a wireless LAN (WLAN), a wide area network (WAN), a wireless WAN (WWAN), a metropolitan area network (MAN), the Internet, a portion of the Internet, a portion of the public switched telephone network (PSTN), a plain old telephone service (POTS) network, a cellular telephone network, a wireless network, a Wi-Fi® network, another type of network, or a combination of two or more such networks. For example, the network780or a portion of the network780may include a wireless or cellular network and the coupling782may be a Code Division Multiple Access (CDMA) connection, a Global System for Mobile communications (GSM) connection, or other type of cellular or wireless coupling. In this example, the coupling782may implement any of a variety of types of data transfer technology, such as Single Carrier Radio Transmission Technology (1×RTT), Evolution-Data Optimized (EVDO) technology, General Packet Radio Service (GPRS) technology, Enhanced Data rates for GSM Evolution (EDGE) technology, third Generation Partnership Project (3GPP) including 3G, fourth generation wireless (4G) networks, Universal Mobile Telecommunications System (UMTS), High Speed Packet Access (HSPA), Worldwide Interoperability for Microwave Access (WiMAX), Long Term Evolution (LTE) standard, others defined by various standard setting organizations, other long range protocols, or other data transfer technology.

The instructions716may be transmitted or received over the network780using a transmission medium via a network interface device (e.g., a network interface component included in the communication components764) and utilizing any one of a number of well-known transfer protocols (e.g., hypertext transfer protocol (HTTP)). Similarly, the instructions716may be transmitted or received using a transmission medium via the coupling772(e.g., a peer-to-peer coupling) to devices770. The term “transmission medium” shall be taken to include any intangible medium that is capable of storing, encoding, or carrying instructions716for execution by the machine700, and includes digital or analog communications signals or other intangible medium to facilitate communication of such software.

Language

Throughout this specification, plural instances may implement components, operations, or structures described as a single instance. Although individual operations of one or more methods are illustrated and described as separate operations, one or more of the individual operations may be performed concurrently, and nothing requires that the operations be performed in the order illustrated. Structures and functionality presented as separate components in example configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements fall within the scope of the subject matter herein.

Although an overview of the disclosed subject matter has been described with reference to specific example embodiments, various modifications and changes may be made to these embodiments without departing from the broader scope of embodiments of the present disclosure. Such embodiments of the inventive subject matter may be referred to herein, individually or collectively, by the term “invention” merely for convenience and without intending to voluntarily limit the scope of this application to any single disclosure or inventive concept if more than one is, in fact, disclosed.

The embodiments illustrated herein are described in sufficient detail to enable those skilled in the art to practice the teachings disclosed. Other embodiments may be used and derived therefrom, such that structural and logical substitutions and changes may be made without departing from the scope of this disclosure. The Detailed Description, therefore, is not to be taken in a limiting sense, and the scope of various embodiments is defined only by the appended claims, along with the full range of equivalents to which such claims are entitled.

As used herein, the term “or” may be construed in either an inclusive or exclusive sense. Moreover, plural instances may be provided for resources, operations, or structures described herein as a single instance. Additionally, boundaries between various resources, operations, modules, engines, and data stores are somewhat arbitrary, and particular operations are illustrated in a context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within a scope of various embodiments of the present disclosure. In general, structures and functionality presented as separate resources in the example configurations may be implemented as a combined structure or resource. Similarly, structures and functionality presented as a single resource may be implemented as separate resources. These and other variations, modifications, additions, and improvements fall within a scope of embodiments of the present disclosure as represented by the appended claims. The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense.

",Brent Davis,"Finance, Logistics, Marketing & Advertising, Technology,Media, Digital Media, Entertainment, Telecommunications,Technology, Media, Retail, Information Storage,Technology, Telecommunications, Consumer Goods, Cybersecurity","Administrative data processing, financial transactions, resource planning, inventory management,Data storage, recording devices, optical and magnetic storage, retrieval systems,Digital data processing, computer architecture, error detection, data transfer protocols, secure processing systems,Television transmission, pictorial communication, facsimile systems, image data processing","A method is disclosed for displaying a sequence of video items on an electronic device. During playback of a current video item, a dismissal input such as a swipe or tap/click is received via a user input mechanism, indicating that display of the current video item is to be ceased and that display of the next video item in the sequence is to be commenced. In response to the dismissal input, summary information is displayed on the display screen, providing non-video communication of an informational payload of the current video item. A timer is started at the commencement of display of the summary information, at the expiry of which display of the next video item is to be commenced. In response to a resumption input such as a touchscreen tap or cursor click on the summary information before expiry of the timer, playback of the current video item is resumed via the display screen."
